---
title: ä½¿ç”¨Vllm-AscendæŽ¨ç†Qwen3
date: '2025-11-13T06:10:00.000Z'
lastmod: '2025-11-13T06:27:00.000Z'
draft: false
tags:
- LLMs
categories:
- AI
---

> ðŸ’¡ ä½¿ç”¨vllm-ascendè¿›è¡ŒæŽ¨ç†ï¼Œå¢žåŠ é€Ÿåº¦ä¼˜åŒ–ï¼Œç¦ç”¨æ€è€ƒã€‚

---

### 1. æ‹‰å–é•œåƒ

```powershell
docker pull quay.io/ascend/vllm-ascend:v0.11.0rc1
```

---

### 2. å¯åŠ¨å®¹å™¨

```powershell
# Update the vllm-ascend image
export IMAGE=quay.io/ascend/vllm-ascend:v0.11.0rc1
docker run --rm \
-d \
--name vllm-ascend \
--shm-size=1g \
--device /dev/davinci0 \
--device /dev/davinci1 \
--device /dev/davinci2 \
--device /dev/davinci3 \
--device /dev/davinci4 \
--device /dev/davinci5 \
--device /dev/davinci6 \
--device /dev/davinci7 \
--device /dev/davinci_manager \
--device /dev/devmm_svm \
--device /dev/hisi_hdc \
-v /usr/local/dcmi:/usr/local/dcmi \
-v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \
-v /usr/local/Ascend/driver/lib64/:/usr/local/Ascend/driver/lib64/ \
-v /usr/local/Ascend/driver/version.info:/usr/local/Ascend/driver/version.info \
-v /etc/ascend_install.info:/etc/ascend_install.info \
-v /root/.cache:/root/.cache \
-p 8000:8000 \
-it $IMAGE bash
```

---

### 3. é…ç½®çŽ¯å¢ƒ

```powershell
export VLLM_USE_MODELSCOPE=True
export PYTORCH_NPU_ALLOC_CONF=max_split_size_mb:256
export VLLM_ASCEND_ENABLE_DENSE_OPTIMIZE=1
export VLLM_ASCEND_ENABLE_FLASHCOMM=1
export VLLM_ASCEND_ENABLE_PREFETCH_MLP=1
export PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
export CPU_AFFINITY_CONF=1

# export TASK_QUEUE_ENABLE=2 
unset TASK_QUEUE_ENABLE
```

- ç¦ç”¨æ€è€ƒ
---

### 4. å¯åŠ¨æœåŠ¡

```powershell
vllm serve Qwen/Qwen3-32B --tensor-parallel-size 8 --async-scheduling --chat-template ./qwen3_nonthinking.jinja
```

---

### 5. æµ‹è¯•

```powershell
#!/usr/bin/env python3
"""
vLLM æ€§èƒ½æµ‹è¯•è„šæœ¬ - æ— éœ€ jq ä¾èµ–
"""
import subprocess
import json
import time
import sys

def call_vllm(prompt: str, max_tokens: int = 4096):
    """è°ƒç”¨ vLLM API å¹¶è®¡æ—¶"""
    
    url = "http://localhost:8000/v1/chat/completions"
    
    payload = {
        "model": "Qwen/Qwen3-32B",
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.6,
        "max_tokens": max_tokens
    }
    
    print(f"ðŸ“¤ å‘é€è¯·æ±‚: {prompt[:50]}...")
    start_time = time.time()
    
    try:
        cmd = [
            'curl', '-s',
            '-H', 'Content-Type: application/json',
            '-d', json.dumps(payload),
            url
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
        
        if result.returncode != 0:
            print(f"âŒ curl æ‰§è¡Œå¤±è´¥: {result.stderr}")
            return None
        
        response = json.loads(result.stdout)
        duration = (time.time() - start_time) * 1000
        
        if 'error' in response:
            print(f"âŒ API é”™è¯¯: {response['error']}")
            return None
        
        # æå–ä¿¡æ¯
        usage = response.get('usage', {})
        content = response['choices'][0]['message']['content']
        
        prompt_tokens = usage.get('prompt_tokens', 0)
        completion_tokens = usage.get('completion_tokens', 0)
        
        # è®¡ç®—æŒ‡æ ‡
        throughput = (completion_tokens / duration * 1000) if duration > 0 else 0
        latency_per_token = duration / completion_tokens if completion_tokens > 0 else 0
        
        # æ‰“å°æ€§èƒ½æŠ¥å‘Š
        print("\n" + "="*60)
        print("ðŸ“Š æ€§èƒ½æŒ‡æ ‡æŠ¥å‘Š")
        print("="*60)
        print(f"â±ï¸  æ€»è€—æ—¶:       {duration:.2f}ms ({duration/1000:.2f}s)")
        print(f"ðŸ“¥ è¾“å…¥Tokens:    {prompt_tokens}")
        print(f"ðŸ“¤ è¾“å‡ºTokens:    {completion_tokens}")
        print(f"âš¡ åžåé‡:        {throughput:.2f} tokens/s")
        print(f"ðŸŽ¯ å»¶è¿Ÿ/Token:    {latency_per_token:.2f} ms/token")
        print("="*60)
        
        print("\nðŸ“ å“åº”å†…å®¹ï¼ˆå‰ 200 å­—ç¬¦ï¼‰:")
        print("-" * 60)
        print(content[:200] + "..." if len(content) > 200 else content)
        print("-" * 60 + "\n")
        
        return {
            'duration': duration,
            'prompt_tokens': prompt_tokens,
            'completion_tokens': completion_tokens,
            'throughput': throughput,
            'content': content
        }
        
    except subprocess.TimeoutExpired:
        print("âŒ è¯·æ±‚è¶…æ—¶ï¼ˆ300ç§’ï¼‰")
        return None
    except json.JSONDecodeError as e:
        print(f"âŒ JSON è§£æžå¤±è´¥: {e}")
        print(f"å“åº”å†…å®¹: {result.stdout[:200]}")
        return None
    except Exception as e:
        print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
        return None

def benchmark(num_requests: int = 3):
    """è¿è¡Œå¤šæ¬¡è¯·æ±‚è¿›è¡ŒåŸºå‡†æµ‹è¯•"""
    
    print(f"\nðŸš€ å¼€å§‹åŸºå‡†æµ‹è¯• ({num_requests} ä¸ªè¯·æ±‚)\n")
    
    prompts = [
        "Give me a short introduction to large language models.",
        "What are the main applications of artificial intelligence?",
        "Explain quantum computing in simple terms."
    ]
    
    results = []
    
    for i, prompt in enumerate(prompts[:num_requests], 1):
        print(f"\n[è¯·æ±‚ {i}/{num_requests}]")
        result = call_vllm(prompt)
        if result:
            results.append(result)
        time.sleep(1)  # è¯·æ±‚é—´éš”
    
    # æ±‡æ€»ç»Ÿè®¡
    if results:
        print("\n" + "="*60)
        print("ðŸ“ˆ æ±‡æ€»ç»Ÿè®¡")
        print("="*60)
        
        durations = [r['duration'] for r in results]
        throughputs = [r['throughput'] for r in results]
        
        print(f"å¹³å‡è€—æ—¶:       {sum(durations)/len(durations):.2f}ms")
        print(f"æœ€å°è€—æ—¶:       {min(durations):.2f}ms")
        print(f"æœ€å¤§è€—æ—¶:       {max(durations):.2f}ms")
        
        print(f"\nå¹³å‡åžåé‡:     {sum(throughputs)/len(throughputs):.2f} tokens/s")
        print(f"æœ€ä½Žåžåé‡:     {min(throughputs):.2f} tokens/s")
        print(f"æœ€é«˜åžåé‡:     {max(throughputs):.2f} tokens/s")
        print("="*60 + "\n")

if __name__ == "__main__":
    # å•æ¬¡è°ƒç”¨
    call_vllm("Give me a short introduction to large language models.")
    
    # æˆ–è€…è¿è¡ŒåŸºå‡†æµ‹è¯•ï¼ˆå–æ¶ˆæ³¨é‡Šä¸‹ä¸€è¡Œï¼‰
    # benchmark(num_requests=3)


```

---

> References

