---
title: Learning vLL (0.7.2) with Cursor
date: '2025-04-14T00:38:00.000Z'
lastmod: '2025-04-14T09:00:00.000Z'
draft: false
tags:
- VLLM
categories:
- AI
---

> ğŸ’¡ ä¹‹å‰ä¸€ç›´ç”¨cursorè¾…åŠ©å†™ä»£ç ï¼Œçªç„¶è¯•äº†ä¸€ä¸‹å»åˆ©ç”¨cursoræ¥å­¦ä»£ç ï¼Œå‘ç°æ•ˆæœå˜å˜å¥½ã€‚è¿™é‡Œè®°å½•ä¸€ä¸‹æ¯”è¾ƒç«çš„vllmä¸­æå‡æ¨ç†é€Ÿåº¦çš„ä¸€äº›å…³é”®ä»£ç ã€‚

# 1. What is vLLMï¼Ÿ

![](https://prod-files-secure.s3.us-west-2.amazonaws.com/fc187c04-cf34-444f-b5f2-bdcdfad76660/ccc90792-56ef-4142-8b21-8ecb32787141/image.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB4665MMM3XNS%2F20260128%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20260128T030508Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEJj%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJIMEYCIQDBN2Yjf7rQ%2BEJIsarEBXZMFuSs2vsczJtpY5SaaUA%2F8wIhAKPzNYVY0POkZ0Zx%2B68LM1ez6gNLTDW3qQ0DihZN0sgZKv8DCGEQABoMNjM3NDIzMTgzODA1IgxtFKJdHzAnfVOEk5Iq3ANcAG6Ch9wRq7LBykeckiCAPZN8FWqMj3t1OgPFsLilxvwAboZ%2F%2FFKbvWsTpxdsy72KLHToWFs6kVRUwkvFMPY0LJykvBiqnbahfUeabPOSv3V24WzDYfJ28qY1Wb2HFZm6diyO%2Bqce4aXUQ8DiWiHNO2iNuMjgqGj%2BHoZPRUSG3C8iO6HOiWFMJPb7wg%2Fi2PIvounTY6aBmX8WBMEOJosNZqUv3l71dzFxP8iaXOdUCfeUGEYA4aly7w170cAXZsmG1qoC8IbG%2FsX86eyzwrdqf8LJHap7l6I%2FTcOTm8Bunxhu1Dgl6Ll9uE0wBC%2FplsQSe85BiTvIBTiZuImTgZJF8b%2FBbgjSCOYy7m2iqZhqJAZbQVrSt%2F7HsbPY6zqGb3PLjA0yjny%2FgpHvX56G%2FoyL2TuAAC7w0kQLMaoHfZejKodQVcihVE354AIgOYxuk%2B5N2D%2FqJXtV%2Bmtlo%2BIvq1Fgb2azo12FZ8ER32tKxqCTXrIrX9A1wUMDAwD9Aw9OldYMR%2FfnXLKB8G3fD%2FvIWYqQoGIljNOGiCiCBz05DIAHdb1mPmcslOtuZ3Bjn8CMg2NL1LLKhQzsn%2FW6QFUTvsEPieU965lVr3KeKBGTnKcST62TTOta%2FNLN1VaRrDCSluXLBjqkAeWWV3VmzbDjIAs%2BEYOO27vnbta78l8CNJX%2FSz%2BY%2F1u%2Fgux%2FcIfY3HHIbh3D2JSAa5WMs7h3MyMNO2aJGh8RqEjrOKeJtjRcdyk490XSgCy6dGnI3YjCsKT%2FDQRQdSadBtwFWwBRNodlINHyururarygEt9WtkIVqt7JIFzgufjjbFpDQi3ErZkVmHWZBA7V00bv4t32NusGnePkDfFYhtrbJpOx&X-Amz-Signature=3b94f72936ddd57f84b74933c49ab35465bbb763f4e27d33f4c835551100a3c8&X-Amz-SignedHeaders=host&x-amz-checksum-mode=ENABLED&x-id=GetObject)



vLLMæ˜¯å¤§æ¨¡å‹æ¨ç†åŠ é€Ÿæ¡†æ¶ã€‚ä¸»è¦åœ¨KVcacheç®¡ç†ä¸­é‡‡ç”¨äº†PageAttentionæŠ€æœ¯ï¼Œæ”¯æŒå¸‚é¢ä¸Šä¸»æµçš„å¼€æºå¤§æ¨¡å‹ï¼Œæ”¯æŒé‡åŒ–ç±»å‹ä¸ºï¼šGPTQ,Â AWQ, INT4, INT8, and FP8

---

# 2. KVcacheç›¸å…³çš„è®¡ç®—ä»£ç 

ä½¿ç”¨claude-3.7-sonnet thinkingï¼Œå¯¹vllmæ•´ä¸ªé¡¹ç›®è¿›è¡Œæé—®ã€‚

![](https://prod-files-secure.s3.us-west-2.amazonaws.com/fc187c04-cf34-444f-b5f2-bdcdfad76660/cb54e376-a945-4f40-a5b9-ff4fc10e9fc2/image.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB4665MMM3XNS%2F20260128%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20260128T030508Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEJj%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJIMEYCIQDBN2Yjf7rQ%2BEJIsarEBXZMFuSs2vsczJtpY5SaaUA%2F8wIhAKPzNYVY0POkZ0Zx%2B68LM1ez6gNLTDW3qQ0DihZN0sgZKv8DCGEQABoMNjM3NDIzMTgzODA1IgxtFKJdHzAnfVOEk5Iq3ANcAG6Ch9wRq7LBykeckiCAPZN8FWqMj3t1OgPFsLilxvwAboZ%2F%2FFKbvWsTpxdsy72KLHToWFs6kVRUwkvFMPY0LJykvBiqnbahfUeabPOSv3V24WzDYfJ28qY1Wb2HFZm6diyO%2Bqce4aXUQ8DiWiHNO2iNuMjgqGj%2BHoZPRUSG3C8iO6HOiWFMJPb7wg%2Fi2PIvounTY6aBmX8WBMEOJosNZqUv3l71dzFxP8iaXOdUCfeUGEYA4aly7w170cAXZsmG1qoC8IbG%2FsX86eyzwrdqf8LJHap7l6I%2FTcOTm8Bunxhu1Dgl6Ll9uE0wBC%2FplsQSe85BiTvIBTiZuImTgZJF8b%2FBbgjSCOYy7m2iqZhqJAZbQVrSt%2F7HsbPY6zqGb3PLjA0yjny%2FgpHvX56G%2FoyL2TuAAC7w0kQLMaoHfZejKodQVcihVE354AIgOYxuk%2B5N2D%2FqJXtV%2Bmtlo%2BIvq1Fgb2azo12FZ8ER32tKxqCTXrIrX9A1wUMDAwD9Aw9OldYMR%2FfnXLKB8G3fD%2FvIWYqQoGIljNOGiCiCBz05DIAHdb1mPmcslOtuZ3Bjn8CMg2NL1LLKhQzsn%2FW6QFUTvsEPieU965lVr3KeKBGTnKcST62TTOta%2FNLN1VaRrDCSluXLBjqkAeWWV3VmzbDjIAs%2BEYOO27vnbta78l8CNJX%2FSz%2BY%2F1u%2Fgux%2FcIfY3HHIbh3D2JSAa5WMs7h3MyMNO2aJGh8RqEjrOKeJtjRcdyk490XSgCy6dGnI3YjCsKT%2FDQRQdSadBtwFWwBRNodlINHyururarygEt9WtkIVqt7JIFzgufjjbFpDQi3ErZkVmHWZBA7V00bv4t32NusGnePkDfFYhtrbJpOx&X-Amz-Signature=724ffda283eeb6093b19e0d1fd941acc6acf096f8c704c9112307cd4b1d71dad&X-Amz-SignedHeaders=host&x-amz-checksum-mode=ENABLED&x-id=GetObject)

ä¸ºäº†æ›´å¥½ä¸¾ä¾‹ï¼Œä¸‹è½½äº†deepseek-v3-0324ç‰ˆæœ¬çš„config.jsonï¼Œä»£å…¥é¡¹ç›®è¿›è¡Œæé—®ã€‚

```json
{
  "architectures": [
    "DeepseekV3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "configuration_deepseek.DeepseekV3Config",
    "AutoModel": "modeling_deepseek.DeepseekV3Model",
    "AutoModelForCausalLM": "modeling_deepseek.DeepseekV3ForCausalLM"
  },
  "aux_loss_alpha": 0.001,
  "bos_token_id": 0,
  "eos_token_id": 1,
  "ep_size": 1,
  "first_k_dense_replace": 3,
  "hidden_act": "silu",
  "hidden_size": 7168,
  "initializer_range": 0.02,
  "intermediate_size": 18432,
  "kv_lora_rank": 512,
  "max_position_embeddings": 163840,
  "model_type": "deepseek_v3",
  "moe_intermediate_size": 2048,
  "moe_layer_freq": 1,
  "n_group": 8,
  "n_routed_experts": 256,
  "n_shared_experts": 1,
  "norm_topk_prob": true,
  "num_attention_heads": 128,
  "num_experts_per_tok": 8,
  "num_hidden_layers": 61,
  "num_key_value_heads": 128,
  "num_nextn_predict_layers": 1,
  "pretraining_tp": 1,æ¨¡å‹ä¸­Transformerå±‚çš„æ•°é‡ã€‚æ›´å¤šçš„å±‚æ•°ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æ›´æ·±å±‚æ¬¡çš„ç‰¹å¾å’Œå…³ç³»ï¼Œä½†ä¹Ÿå¢åŠ äº†æ¨¡å‹çš„æ·±åº¦å’Œè®¡ç®—æˆæœ¬ã€‚

  "q_lora_rank": 1536,
  "qk_nope_head_dim": 128,
  "qk_rope_head_dim": 64,
  "quantization_config": {
    "activation_scheme": "dynamic",
    "fmt": "e4m3",
    "quant_method": "fp8",
    "weight_block_size": [
      128,
      128
    ]
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "beta_fast": 32,
    "beta_slow": 1,
    "factor": 40,
    "mscale": 1.0,
    "mscale_all_dim": 1.0,
    "original_max_position_embeddings": 4096,
    "type": "yarn"
  },
  "rope_theta": 10000,
  "routed_scaling_factor": 2.5,
  "scoring_func": "sigmoid",
  "seq_aux": true,
  "tie_word_embeddings": false,
  "topk_group": 4,
  "topk_method": "noaux_tc",
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "v_head_dim": 128,
  "vocab_size": 129280
}

```

è¯¥æ–‡ä»¶ä¸­çš„å…³é”®ä¿¡æ¯ä¸ºï¼š

## 2.1 DeepSeek-V1çš„KVcacheæ˜¯å¦‚ä½•è®¡ç®—çš„ï¼Ÿ

### 2.1.1 æ¨¡å‹å‚æ•°åˆ†æ

ä»æä¾›çš„config.jsonæ–‡ä»¶ä¸­æå–çš„å…³é”®å‚æ•°ï¼š

- hidden_size: 7168
- num_attention_heads: 128Â (Qå¤´æ•°é‡)
- num_key_value_heads: 128Â (KVå¤´æ•°é‡ï¼Œæ— MHAç»“æ„)
- num_hidden_layers: 61Â (Transformerå±‚æ•°)
- max_position_embeddings: 163840 (æœ€å¤§ä½ç½®åµŒå…¥ï¼Œè¡¨ç¤ºæ”¯æŒçš„æœ€å¤§åºåˆ—é•¿åº¦)
- torch_dtype: "bfloat16" (æ¨¡å‹å‚æ•°æ•°æ®ç±»å‹)
---

### 2.1.2 KVç¼“å­˜æ˜¾å­˜è®¡ç®—

åŸºç¡€å‚æ•°è®¡ç®—

- head_size = hidden_size / num_attention_heads = 7168 / 128 = 56
- æ•°æ®ç±»å‹å¤§å° = bfloat16 = 2å­—èŠ‚
å•ä¸ªæ³¨æ„åŠ›å±‚çš„KVcacheè®¡ç®—

- key cache size = num_key_value_heads * head_size = 128 * 56 = 7168
- value cache size = key cache size = 7168
- å•å±‚å•tokençš„KVç¼“å­˜å¤§å° = (keyç¼“å­˜é¡¹ + valueç¼“å­˜é¡¹) * dtype = (7168 + 7168) * 2 = 28672 â‰ˆ 28kb
å…¨æ¨¡å‹æœ€å¤§KVç¼“å­˜è®¡ç®—

- å…¨æ¨¡å‹å•tokençš„KVç¼“å­˜å¤§å° = å•å±‚tokençš„KVç¼“å­˜ * æ¨¡å‹å±‚æ•° = 28kb * 61 = 1708kb â‰ˆ 1.67MB
- æœ€å¤§åºåˆ—é•¿åº¦ä¸‹çš„KVç¼“å­˜å¤§å° = å…¨æ¨¡å‹å•tokençš„KVç¼“å­˜å¤§å° * æœ€å¤§åºåˆ—é•¿åº¦ = 273612.8MB â‰ˆ 267.2GB
æŒ‰å—åˆ†é…è®¡ç®—

å¦‚æœä½¿ç”¨vLLMçš„åˆ†å—æœºåˆ¶ï¼Œå‡è®¾block_size=16ï¼š

- æ¯ä¸ªå—å¤§å° = æ•°æ®ç±»å‹å¤§å° * æ³¨æ„åŠ›å±‚æ•° * block_size * (key_cache_entry + value_cache_entry) = 2 * 61 * 16 * (7168+7168) â‰ˆ 27.9MB
- éœ€è¦å—çš„æ•°é‡ = æœ€å¤§åºåˆ—é•¿åº¦ / block_size = 163840 / 16 = 10240å—
- æ€»æ˜¾å­˜ = æ¯ä¸ªå—å¤§å° * éœ€è¦çš„å—æ•°é‡ = 27.9MB * 10240 â‰ˆ 286GB
å®é™…åº”ç”¨è€ƒè™‘

1. å®é™…ä½¿ç”¨æ—¶å¯èƒ½ä¸ä¼šç”¨æœ€å¤§çš„åºåˆ—é•¿åº¦ï¼Œå¸¸è§çš„è®¾ç½®ä¸º4K - 32K
1. ä¸åŒçš„block_sizeçš„é€‰æ‹©ä¼šå½±å“æ˜¾å­˜ä½¿ç”¨æ•ˆç‡
ä¾‹å¦‚ï¼Œå¦‚æœä½¿ç”¨8Kçš„åºåˆ—é•¿åº¦:

- KVç¼“å­˜ = 1.67MB * 8192 â‰ˆ 13.7GB
---

### 2.1.3 è®¡ç®—KVcacheä»£ç çš„ä½ç½®

åœ¨0.7.2ç‰ˆæœ¬ä¸­ï¼Œvllm\worker\worker.py 

```python
@torch.inference_mode()
    def determine_num_available_blocks(self) -> Tuple[int, int]:
        """ç¡®å®šå¯ç”¨çš„KVç¼“å­˜å—æ•°é‡ã€‚

        è¯¥æ–¹æ³•ä¼šé¦–å…ˆå¯¹ç°æœ‰å†…å­˜ä½¿ç”¨æƒ…å†µè¿›è¡Œåˆ†æ,ç„¶åè®¡ç®—åœ¨å‰©ä½™å¯ç”¨å†…å­˜ä¸‹å¯ä»¥åˆ†é…çš„æœ€å¤§GPUå’ŒCPUå—æ•°é‡ã€‚

        ä¸»è¦æ­¥éª¤:
        1. æ¸…ç©ºGPUç¼“å­˜å¹¶é‡ç½®å†…å­˜ç»Ÿè®¡ä¿¡æ¯
        2. æ‰§è¡Œä¸€æ¬¡å‰å‘ä¼ æ’­æ¥åˆ†ææ¨¡å‹çš„å†…å­˜ä½¿ç”¨æƒ…å†µ
        3. æ ¹æ®GPUå†…å­˜åˆ©ç”¨ç‡å’Œå¯ç”¨å†…å­˜è®¡ç®—å¯åˆ†é…çš„KVç¼“å­˜å—æ•°é‡
        4. è®°å½•è¯¦ç»†çš„å†…å­˜ä½¿ç”¨æƒ…å†µ

        Returns:
            Tuple[int, int]: è¿”å›(GPUå—æ•°é‡, CPUå—æ•°é‡)çš„å…ƒç»„
        """
        # æ¸…ç©ºGPUç¼“å­˜å¹¶é‡ç½®å†…å­˜ç»Ÿè®¡ä¿¡æ¯
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()

        # è·å–å½“å‰ç©ºé—²å†…å­˜å’Œæ€»GPUå†…å­˜
        free_memory_pre_profile, total_gpu_memory = torch.cuda.mem_get_info()

        # ä½¿ç”¨è™šæ‹Ÿè¾“å…¥æ‰§è¡Œä¸€æ¬¡å‰å‘ä¼ æ’­,åˆ†ææ¨¡å‹å†…å­˜ä½¿ç”¨æƒ…å†µ
        with memory_profiling(
                self.baseline_snapshot,
                weights_memory=self.model_runner.model_memory_usage) as result:
            self.model_runner.profile_run()

        # éªŒè¯å†…å­˜å ç”¨æ˜¯å¦å¢åŠ 
        self._assert_memory_footprint_increased_during_profiling()

        # è®¡ç®—å½“å‰å®ä¾‹å¯ç”¨çš„æ€»å†…å­˜
        memory_for_current_instance = total_gpu_memory * \
            self.cache_config.gpu_memory_utilization
        # è®¡ç®—å¯ç”¨äºKVç¼“å­˜çš„å†…å­˜
        available_kv_cache_memory = (memory_for_current_instance -
                                     result.non_kv_cache_memory)

        # è®¡ç®—å¯åˆ†é…çš„å—æ•°é‡
        cache_block_size = self.get_cache_block_size_bytes()
        if cache_block_size == 0:
            num_gpu_blocks = 0
            num_cpu_blocks = 0
        else:
            # æ ¹æ®å¯ç”¨å†…å­˜è®¡ç®—GPUå’ŒCPUå—æ•°é‡
            num_gpu_blocks = int(available_kv_cache_memory // cache_block_size)
            num_cpu_blocks = int(self.cache_config.swap_space_bytes //
                                 cache_block_size)
        num_gpu_blocks = max(num_gpu_blocks, 0)
        num_cpu_blocks = max(num_cpu_blocks, 0)

        # æ„å»ºè¯¦ç»†çš„å†…å­˜ä½¿ç”¨æƒ…å†µæ—¥å¿—
        msg = (f"Memory profiling takes {result.profile_time:.2f} seconds\n"
               "the current vLLM instance can use "
               "total_gpu_memory "
               f"({(total_gpu_memory / GiB_bytes):.2f}GiB)"
               " x gpu_memory_utilization "
               f"({self.cache_config.gpu_memory_utilization:.2f})"
               f" = {(memory_for_current_instance / GiB_bytes):.2f}GiB\n"
               "model weights take "
               f"{(result.weights_memory / GiB_bytes):.2f}GiB;"
               " non_torch_memory takes "
               f"{(result.non_torch_increase / GiB_bytes):.2f}GiB;"
               " PyTorch activation peak memory takes "
               f"{(result.torch_peak_increase / GiB_bytes):.2f}GiB;"
               " the rest of the memory reserved for KV Cache is "
               f"{(available_kv_cache_memory / GiB_bytes):.2f}GiB.")

        logger.info(msg)
        # Final cleanup
        gc.collect()

        return num_gpu_blocks, num_cpu_blocks
```

---

## 2.2 åœ¨vLLMä¸­å®ç°PageAttentionçš„ä»£ç åœ¨å“ªé‡Œï¼Ÿ

å…³äºPageAttentionæŠ€æœ¯çš„å®ç°ï¼Œå®ƒä¸»è¦åœ¨ä»¥ä¸‹æ–‡ä»¶ä¸­ï¼š

1.  vllm/attention/ops/paged_attn.pyï¼šåŒ…å«PagedAttentionç±»ï¼Œæä¾›KVç¼“å­˜å½¢çŠ¶ã€åˆ†å‰²å’Œå†™å…¥ç­‰æ“ä½œçš„æ¥å£
1.  vllm/_custom_ops.pyï¼šåŒ…å«paged_attention_v1å‡½æ•°ï¼Œè¿æ¥åˆ°C++åº•å±‚å®ç°
1. csrc/attention/attention_kernels.cuï¼šåŒ…å«PageAttentionçš„CUDAå†…æ ¸å®ç°
1.  docs/source/design/kernel/paged_attention.mdï¼šè¯¦ç»†è§£é‡Šäº†PageAttentionçš„è®¾è®¡å’Œå®ç°åŸç†
PageAttentionæ˜¯vLLMçš„æ ¸å¿ƒæŠ€æœ¯ï¼Œé€šè¿‡å°†KVç¼“å­˜å­˜å‚¨åœ¨ä¸è¿ç»­çš„å†…å­˜å—ä¸­ï¼Œå®ç°äº†é«˜æ•ˆçš„å†…å­˜ç®¡ç†ã€‚è¿™äº›å†…å­˜å—å¯ä»¥æ ¹æ®éœ€è¦åŠ¨æ€åˆ†é…å’Œé‡Šæ”¾ï¼Œä½¿å¾—å•ä¸ªGPUèƒ½å¤ŸæœåŠ¡æ›´å¤šå¹¶å‘è¯·æ±‚ã€‚è¯¥æŠ€æœ¯çš„å…³é”®ç‰¹ç‚¹æ˜¯é€šè¿‡åˆ†é¡µæœºåˆ¶é¿å…äº†æ˜¾å­˜ç¢ç‰‡ï¼Œå¹¶æ”¯æŒé«˜æ•ˆçš„ä¸Šä¸‹æ–‡å¤„ç†ã€‚

---

# 3. éªŒè¯

ä½¿ç”¨4090DåŸºäºvllmå¯ç”¨é»˜è®¤å‚æ•°è¿›è¡Œæ¨ç†ã€‚æŸ¥çœ‹å¯åŠ¨æœåŠ¡çš„æ—¥å¿—å†…å®¹ã€‚

## 3.1 æ¨¡å‹æ—¥å¿—

```bash
INFO 04-14 06:00:50 [__init__.py:239] Automatically detected platform cuda.
INFO 04-14 06:00:52 [api_server.py:1034] vLLM API server version 0.8.3
INFO 04-14 06:00:52 [api_server.py:1035] args: Namespace(subparser='serve', model_tag='/data/DeepSeek-R1-Distill-Qwen-1.5B', config='', host=None, port=8000, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/data/DeepSeek-R1-Distill-Qwen-1.5B', task='auto', tokenizer=None, hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, prefix_caching_hash_algo='builtin', disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=None, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, use_tqdm_on_load=True, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_config=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, enable_reasoning=False, reasoning_parser=None, disable_cascade_attn=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False, dispatch_function=<function ServeSubcommand.cmd at 0x7f883404a7a0>)
INFO 04-14 06:01:01 [config.py:600] This model supports multiple tasks: {'score', 'generate', 'classify', 'embed', 'reward'}. Defaulting to 'generate'.
INFO 04-14 06:01:01 [config.py:1780] Chunked prefill is enabled with max_num_batched_tokens=2048.
INFO 04-14 06:01:08 [__init__.py:239] Automatically detected platform cuda.
INFO 04-14 06:01:11 [core.py:61] Initializing a V1 LLM engine (v0.8.3) with config: model='/data/DeepSeek-R1-Distill-Qwen-1.5B', speculative_config=None, tokenizer='/data/DeepSeek-R1-Distill-Qwen-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/DeepSeek-R1-Distill-Qwen-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
WARNING 04-14 06:01:11 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fd287cd4670>
INFO 04-14 06:01:12 [parallel_state.py:957] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 04-14 06:01:12 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 04-14 06:01:12 [gpu_model_runner.py:1258] Starting to load model /data/DeepSeek-R1-Distill-Qwen-1.5B...
WARNING 04-14 06:01:12 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
INFO 04-14 06:01:13 [loader.py:447] Loading weights took 0.95 seconds
INFO 04-14 06:01:14 [gpu_model_runner.py:1273] Model loading took 3.3465 GiB and 1.192773 seconds
INFO 04-14 06:01:23 [backends.py:416] Using cache directory: /root/.cache/vllm/torch_compile_cache/74d872966c/rank_0_0 for vLLM's torch.compile
INFO 04-14 06:01:23 [backends.py:426] Dynamo bytecode transform time: 9.12 s
INFO 04-14 06:01:26 [backends.py:132] Cache the graph of shape None for later use
INFO 04-14 06:01:54 [backends.py:144] Compiling a graph for general shape takes 30.65 s
INFO 04-14 06:02:03 [monitor.py:33] torch.compile takes 39.77 s in total
INFO 04-14 06:02:04 [kv_cache_utils.py:578] GPU KV cache size: 426,448 tokens
INFO 04-14 06:02:04 [kv_cache_utils.py:581] Maximum concurrency for 131,072 tokens per request: 3.25x
INFO 04-14 06:02:33 [gpu_model_runner.py:1608] Graph capturing finished in 29 secs, took 1.47 GiB
INFO 04-14 06:02:33 [core.py:162] init engine (profile, create kv cache, warmup model) took 79.67 seconds
WARNING 04-14 06:02:33 [config.py:1088] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
INFO 04-14 06:02:33 [serving_chat.py:117] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.95}
INFO 04-14 06:02:33 [serving_completion.py:61] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.95}
INFO 04-14 06:02:33 [api_server.py:1081] Starting vLLM API server on http://0.0.0.0:8000
INFO 04-14 06:02:33 [launcher.py:26] Available routes are:
INFO 04-14 06:02:33 [launcher.py:34] Route: /openapi.json, Methods: HEAD, GET
INFO 04-14 06:02:33 [launcher.py:34] Route: /docs, Methods: HEAD, GET
INFO 04-14 06:02:33 [launcher.py:34] Route: /docs/oauth2-redirect, Methods: HEAD, GET
INFO 04-14 06:02:33 [launcher.py:34] Route: /redoc, Methods: HEAD, GET
INFO 04-14 06:02:33 [launcher.py:34] Route: /health, Methods: GET
INFO 04-14 06:02:33 [launcher.py:34] Route: /load, Methods: GET
INFO 04-14 06:02:33 [launcher.py:34] Route: /ping, Methods: GET, POST
INFO 04-14 06:02:33 [launcher.py:34] Route: /tokenize, Methods: POST
INFO 04-14 06:02:33 [launcher.py:34] Route: /detokenize, Methods: POST
INFO 04-14 06:02:33 [launcher.py:34] Route: /v1/models, Methods: GET
INFO 04-14 06:02:33 [launcher.py:34] Route: /version, Methods: GET
INFO 04-14 06:02:33 [launcher.py:34] Route: /v1/chat/completions, Methods: POST
INFO 04-14 06:02:33 [launcher.py:34] Route: /v1/completions, Methods: POST
INFO 04-14 06:02:33 [launcher.py:34] Route: /v1/embeddings, Methods: POST
INFO 04-14 06:02:33 [launcher.py:34] Route: /pooling, Methods: POST
INFO 04-14 06:02:33 [launcher.py:34] Route: /score, Methods: POST
INFO 04-14 06:02:33 [launcher.py:34] Route: /v1/score, Methods: POST
INFO 04-14 06:02:33 [launcher.py:34] Route: /v1/audio/transcriptions, Methods: POST
INFO 04-14 06:02:33 [launcher.py:34] Route: /rerank, Methods: POST
INFO 04-14 06:02:33 [launcher.py:34] Route: /v1/rerank, Methods: POST
INFO 04-14 06:02:33 [launcher.py:34] Route: /v2/rerank, Methods: POST
INFO 04-14 06:02:33 [launcher.py:34] Route: /invocations, Methods: POST
INFO 04-14 06:02:44 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 04-14 06:02:54 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 04-14 06:03:04 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 04-14 06:03:14 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 04-14 06:03:24 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 04-14 06:03:34 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 04-14 06:03:44 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 04-14 06:03:54 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 04-14 06:04:04 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 04-14 06:04:12 [launcher.py:74] Shutting down FastAPI HTTP server.
INFO 04-14 06:04:14 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%

```

---

## 3.2 æ¨¡å‹é…ç½®

```bash
{
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1536,
  "initializer_range": 0.02,
  "intermediate_size": 8960,
  "max_position_embeddings": 131072,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 12,
  "num_hidden_layers": 28,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_theta": 10000,
  "sliding_window": 4096,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.0",
  "use_cache": true,
  "use_mrope": false,
  "use_sliding_window": false,
  "vocab_size": 151936
}

```

---

## 3.3 è®¡ç®—éªŒè¯

### 3.3.1 æ¯ä¸ªtokençš„KVç¼“å­˜å¤§å°è®¡ç®—

- head_size = hidden_size / num_attention_heads = 1536 / 12 = 128
- å•tokenå•å±‚çš„KVç¼“å­˜ = 2(Kå’ŒV) * num_key_value_heads * head_size * dtype  = 2 * 2 * 128 * 2 = 1024å­—èŠ‚
- å…¨æ¨¡å‹å•token KVç¼“å­˜ = å•tokenå•å±‚çš„KVç¼“å­˜ * num_hidden_layers = 1024 * 28 = 28672å­—èŠ‚
---

### 3.3.2 æ˜¾å­˜åˆ†é…

- æ€»å¯ç”¨æ˜¾å­˜ = 24GB * 0.9 = 21.6GB
- éƒ¨åˆ†æ˜¾å­˜ç”¨äºæ¨¡å‹æƒé‡ã€æ¿€æ´»å€¼ç­‰
- å‰©ä½™æ˜¾å­˜ç”¨äºKVç¼“å­˜
---

### 3.3.3 å¯ç¼“å­˜çš„tokenæ•°é‡

æ—¥å¿—çš„ç»“æœä¸ºï¼šINFO 04-14 06:02:04 [kv_cache_utils.py:578] GPU KV cache size: 426,448 tokens

- æ ¹æ®æ—¥å¿—ç»“æœåæ¨å¯å¾—åˆ°ï¼šå‰©ä½™æ˜¾å­˜ç”¨äºKVç¼“å­˜ / æ¯ä¸ªtokençš„KVç¼“å­˜å¤§å° = 426448
- å‰©ä½™æ˜¾å­˜ç”¨äºKVç¼“å­˜å¤§å°ä¸ºï¼š11.38739 GB
---

# 4. vLLMä¸­å¦‚ä½•è®¡ç®—KVcache

æ¥ä¸Šä¸€èŠ‚ï¼Œä»vLLMä»£ç å±‚é¢åˆ†æï¼Œ426,448ä¸ªtokençš„KVç¼“å­˜å®¹é‡æ˜¯é€šè¿‡ä»¥ä¸‹æ­¥éª¤è®¡ç®—å¾—å‡ºçš„ï¼š

## 4.1 æ ¸å¿ƒè®¡ç®—å‡½æ•°

```bash
worker.py: determine_num_available_blocks() â†’ è®¡ç®—å¯ç”¨å—æ•°é‡
cache_engine.py: get_cache_block_size() â†’ è®¡ç®—æ¯ä¸ªå—å¤§å°
kv_cache_utils.py: get_kv_cache_configs() â†’ é…ç½®KVç¼“å­˜
```

## 4.1.1 determine_num_available_blocks()

å¯¹äºä½ çš„4090D(24GB)ï¼Œä½¿ç”¨0.9çš„gpu_memory_utilizationï¼š

- æ€»æ˜¾å­˜: 24GBÂ Ã— 0.9Â â‰ˆ 21.6GB
- å‡å»æ¨¡å‹æƒé‡å’Œæ¿€æ´»å€¼ç­‰éKVç¼“å­˜éƒ¨åˆ†
---

## 4.1.2 get_cache_blockZ_size()

> ğŸ’¡ è®¡ç®—ç¼“å­˜å—çš„å¤§å°

```python
@staticmethod
def get_cache_block_size(cache_config, model_config, parallel_config):
    head_size = model_config.get_head_size()  # 128
    num_heads = model_config.get_num_kv_heads(parallel_config)  # 2
    num_attention_layers = model_config.get_num_layers_by_block_type(
        parallel_config, LayerBlockType.attention)  # 28
    
    # è·å–æ•°æ®ç±»å‹å¤§å°
    if cache_config.cache_dtype == "auto":
        dtype = model_config.dtype  # bfloat16
    else:
        dtype = STR_DTYPE_TO_TORCH_DTYPE[cache_config.cache_dtype]
    
    # è®¡ç®—keyç¼“å­˜é¡¹å’Œvalueç¼“å­˜é¡¹å¤§å°
    key_cache_entry = num_heads * head_size  # 2 * 128 = 256
    value_cache_entry = key_cache_entry  # 256
    
    # è®¡ç®—ä¸€ä¸ªå—çš„æ€»å…ƒç´ æ•°
    total = num_attention_layers * cache_config.block_size * (key_cache_entry + value_cache_entry)
    # 28 * 16 * (256 + 256) = 28 * 16 * 512 = 229,376ä¸ªå…ƒç´ 
    
    # æ¯ä¸ªå…ƒç´ çš„å­—èŠ‚å¤§å°(bfloat16 = 2å­—èŠ‚)
    dtype_size = get_dtype_size(dtype)  # 2
    
    # è¿”å›å—å¤§å°(å­—èŠ‚)
    return dtype_size * total  # 2 * 229,376 = 458,752å­—èŠ‚
```

---

## 4.1.3 determine_num_available_blocks()

> ğŸ’¡ è®¡ç®—å¯ç”¨å—çš„æ•°é‡

é‡æ–°å›åˆ°determine_num_available_blocks() 

```python
cache_block_size = self.get_cache_block_size_bytes()
num_gpu_blocks = int(available_kv_cache_memory // cache_block_size)
```

è¿™é‡Œclaude-3.7-sonnetå›å¤ç»“æœä¸ºï¼š

è®¡ç®—å¯ç”¨å—æ•°ï¼š

- å‡è®¾å¯ç”¨äºKVç¼“å­˜çš„æ˜¾å­˜çº¦ä¸º12.2GB
- æ¯ä¸ªå—å¤§å°ä¸º458.75KB
- å—æ•°é‡Â = 12.2GBÂ Ã· 458.75KBÂ â‰ˆ 26,653å—
æˆ‘è§‰å¾—12.2GBä¸æ˜¯å¾ˆä¸¥è°¨ï¼Œä¸“é—¨å»æŸ¥äº†ä¸€ä¸‹å¦‚ä½•ä¼°ç®—å‡º12.2GBçš„è¿‡ç¨‹ã€‚ä»£å…¥qwen1.5bæ¨¡å‹ï¼Œæ¨èåœºæ™¯ä¸‹ä¸éœ€è¦è®¡ç®—æ¢¯åº¦ï¼Œæ‰€ä»¥äº§ç”Ÿçš„æ˜¾å­˜ç±»å‹åº”è¯¥ä¸ºï¼šæ¨¡å‹æƒé‡å’Œå‰ä¼ çš„æ¿€æ´»å€¼ã€‚æ¨¡å‹æƒé‡çš„è®¡ç®—ï¼šå‚æ•°é‡ * dtype â‰ˆ 3GBï¼Œå‰ä¼ çš„æ¿€æ´»å€¼ï¼Œæ²¡æŸ¥åˆ°æ¥æºä¾æ®ã€‚

---

## 4.1.4 _get_kv_cache_config_uniform_type()

```python
def _get_kv_cache_config_uniform_type(vllm_config: VllmConfig,
                                      kv_cache_spec: KVCacheSpec,
                                      available_memory: int,
                                      num_layers: int) -> KVCacheConfig:
    """
    ä¸ºå…·æœ‰å•ä¸€ç±»å‹KVç¼“å­˜çš„æ¨¡å‹ç”ŸæˆKVç¼“å­˜é…ç½®ã€‚
    å°†å¯ç”¨å†…å­˜å¹³å‡åˆ†é…ç»™æ‰€æœ‰å±‚ã€‚

    å‚æ•°:
        vllm_config: å…¨å±€VllmConfigé…ç½®
        kv_cache_spec: æ¨¡å‹çš„KVç¼“å­˜è§„æ ¼
        available_memory: KVç¼“å­˜å¯ç”¨çš„å†…å­˜å¤§å°(å­—èŠ‚)
        num_layers: æ¨¡å‹çš„å±‚æ•°

    è¿”å›:
        ç”Ÿæˆçš„KVCacheConfigé…ç½®
    """

    # è·å–æ‰€æœ‰å±‚çš„é¡µé¢å¤§å°(å­—èŠ‚)
    page_sizes = {layer.page_size_bytes for layer in kv_cache_spec.values()}
    # ç¡®ä¿æ‰€æœ‰å±‚ä½¿ç”¨ç›¸åŒçš„é¡µé¢å¤§å°
    assert len(page_sizes) == 1
    page_size = page_sizes.pop()

    # è®¡ç®—æ¯å±‚å¯åˆ†é…çš„å—æ•°
    num_blocks = int(available_memory // page_size // num_layers)
    num_blocks = max(num_blocks, 0)

    # å¦‚æœé…ç½®ä¸­æŒ‡å®šäº†GPUå—æ•°çš„è¦†ç›–å€¼,åˆ™ä½¿ç”¨è¯¥å€¼
    if vllm_config.cache_config.num_gpu_blocks_override is not None:
        num_gpu_blocks_override = \
            vllm_config.cache_config.num_gpu_blocks_override
        logger.info(
            "ä½¿ç”¨num_gpu_blocks_override=%dè¦†ç›–åŸå§‹num_gpu_blocks=%d",
            num_gpu_blocks_override, num_blocks)
        num_blocks = num_gpu_blocks_override

    # è®°å½•GPUå—æ•°ä¿¡æ¯
    logger.info("GPUå—æ•°: %d", num_blocks)
    # è®¡ç®—æœ€å¤§å¹¶å‘åº¦
    max_concurrency = (num_blocks * vllm_config.cache_config.block_size /
                       vllm_config.model_config.max_model_len)
    logger.info("æ¯ä¸ªè¯·æ±‚%sä¸ªtokenæ—¶çš„æœ€å¤§å¹¶å‘åº¦: %.2fx",
                vllm_config.model_config.max_model_len, max_concurrency)

    # è®¡ç®—æ¯å±‚çš„å†…å­˜å¤§å°
    per_layer_size = page_size * num_blocks

    # åˆ›å»ºKVç¼“å­˜é…ç½®
    kv_cache_config = KVCacheConfig(
        num_blocks=num_blocks,
        # ä¸ºæ¯ä¸€å±‚åˆ›å»ºç›¸åŒå¤§å°çš„KVç¼“å­˜å¼ é‡
        tensors={
            layer_name: KVCacheTensor(size=per_layer_size)
            for layer_name in kv_cache_spec
        },
        # å°†æ‰€æœ‰å±‚ç»„ç»‡ä¸ºä¸€ä¸ªç»„
        groups=[[layer_name for layer_name in kv_cache_spec]],
        kv_cache_spec=kv_cache_spec)
    return kv_cache_config
    
# æ€»tokenå®¹é‡ = å—æ•°é‡ * æ¯å—å­˜å‚¨çš„tokenæ•°
total_tokens = num_gpu_blocks * block_size
logger.info("GPU KV cache size: %d tokens", total_tokens)
```

æ€»tokenå®¹é‡ = 26653 * 16 = 426448ï¼Œ ä¸æ—¥å¿—å¯¹æ¯”å‘ç°æ•°å€¼ä¸€è‡´ã€‚

