---
title: 大模型评测指标体系详解
date: '2025-12-03T05:15:00.000Z'
lastmod: '2025-12-03T05:38:00.000Z'
draft: false
tags:
- LLMs
categories:
- AI
---

> 💡 

## 1. 知识与推理类指标

### 📚 MMLU (Massive Multitask Language Understanding)

```python
特点：
- 覆盖57个学科领域
- 包含人文、社科、STEM等
- 约15,908道多选题
- 难度：高中到专家级

评分范围：0-100%
顶级模型表现：
- GPT-4: ~86.4%
- Claude-3: ~86.8%
- Gemini Ultra: ~90.0%
- Qwen2.5-72B: ~85.2%

测试示例：
Q: "The complex number z = 3 + 4i has modulus..."
A: 5 (正确)

```

### 🎓 GPQA (Graduate-level Google Proof Q&A)

```python
特点：
- 研究生级别的科学问题
- 涵盖物理、化学、生物
- 专家级难度（PhD水平）
- 约448道题

评分范围：0-100%
顶级模型表现：
- GPT-4: ~36-39%
- Claude-3 Opus: ~50.4%
- Gemini Pro 1.5: ~41.5%
- 人类专家: ~65-80%

难度级别：
- GPQA-Diamond: 最难子集（198题）
- GPQA-Extended: 扩展版本

```

### 🔢 MATH (Mathematics)

```python
特点：
- 12,500道竞赛级数学题
- 涵盖7个数学领域
- 需要多步推理
- LaTeX格式答案

领域分布：
- Algebra: 代数
- Counting & Probability: 计数与概率
- Number Theory: 数论
- Geometry: 几何
- Precalculus: 预备微积分
- Intermediate Algebra: 中级代数

顶级模型表现：
- GPT-4: ~42.5%
- Claude-3 Opus: ~60.1%
- Gemini 1.5 Pro: ~58.5%
- Qwen2.5-Math: ~83.6%

```

### 🏆 AIME 2024 (American Invitational Mathematics Examination)

```python
特点：
- 美国数学邀请赛题目
- 15道高难度数学题
- 需要创造性解题思路
- 答案为0-999的整数

评分标准：
- 满分：15分
- 及格线：通常6-7分

顶级模型表现：
- GPT-4: 2-3/15
- Claude-3 Opus: 2/15
- Gemini 1.5 Pro: 2-3/15
- 专门数学模型: 5-7/15

```

## 2. 编程能力指标

### 💻 HumanEval

```python
特点：
- 164个Python编程任务
- 函数级代码生成
- 包含单元测试
- Pass@k评分

评分指标：
- Pass@1: 一次生成通过率
- Pass@10: 10次生成至少1次通过
- Pass@100: 100次生成至少1次通过

顶级模型表现：
- GPT-4: ~87.1%
- Claude-3: ~84.9%
- DeepSeek-Coder-V2: ~90.2%
- CodeLlama-70B: ~80.5%

```

### 🏅 Codeforces Rating

```python
特点：
- 基于Codeforces竞赛平台
- 算法竞赛题目
- 难度从800到3500
- 实时竞赛排名

评分等级：
- Newbie: < 1200
- Pupil: 1200-1400
- Specialist: 1400-1600
- Expert: 1600-1900
- Candidate Master: 1900-2100
- Master: 2100-2300
- International Master: 2300-2400
- Grandmaster: 2400-2600
- Legendary Grandmaster: > 3000

模型表现：
- GPT-4: ~1200-1400 (Pupil)
- Claude-3: ~1300-1500
- Gemini-Pro: ~1100-1300
- DeepSeek-Coder: ~1400-1600

```

### 🔧 MBPP (Mostly Basic Python Programming)

```python
特点：
- 974个Python编程任务
- 基础到中级难度
- 自然语言描述
- 包含测试用例

顶级模型表现：
- GPT-4: ~86.8%
- Claude-3: ~88.0%
- CodeLlama-70B: ~82.3%

```

## 3. 中文能力指标

### 🇨🇳 C-Eval

```python
特点：
- 13,948个中文多选题
- 52个学科
- 覆盖人文到理工
- 中国特色知识

难度级别：
- Middle: 初中
- High: 高中
- College: 大学
- Professional: 专业级

顶级模型表现：
- GPT-4: ~68-71%
- Qwen2.5-72B: ~89.5%
- GLM-4: ~72.3%
- Baichuan2: ~58.1%

```

### 📖 CMMLU

```python
特点：
- 中文多领域理解
- 11,528个题目
- 67个细分领域
- 包含古文、成语等

顶级模型表现：
- Qwen2.5: ~87%
- GPT-4: ~71%
- Yi-34B: ~82%

```

## 4. 推理能力指标

### 🧩 GSM8K (Grade School Math 8K)

```python
特点：
- 8,792道小学数学应用题
- 需要多步推理
- 2-8步解题过程
- 支持Chain-of-Thought

顶级模型表现：
- GPT-4: ~92.0%
- Claude-3: ~95.0%
- Gemini Ultra: ~94.4%
- Llama-3.1-70B: ~93.0%

```

### 🎯 BBH (Big Bench Hard)

```python
特点：
- 23个具挑战性的任务
- 涵盖逻辑、常识、数学
- 需要复杂推理
- 6,511个样本

任务类型：
- Boolean Expressions
- Causal Judgement
- Date Understanding
- Logical Deduction
- Object Counting
- Word Sorting

顶级模型表现：
- GPT-4: ~83.1%
- Claude-3: ~86.8%
- Gemini Pro: ~83.6%

```

### 🌟 ARC-Challenge

```python
特点：
- 科学推理题目
- 小学科学水平
- 2,590个题目
- 需要常识推理

顶级模型表现：
- GPT-4: ~96.3%
- Claude-3: ~96.7%
- Llama-3.1: ~93.0%

```

## 5. 长文本能力指标

### 📜 Needle in a Haystack

```python
特点：
- 测试长文本信息检索
- 上下文长度: 1K-128K tokens
- 不同位置插入关键信息
- 测试注意力机制

评分维度：
- 准确率 vs 文档长度
- 准确率 vs 信息位置

```

### 📚 LongBench

```python
特点：
- 6个任务类型
- 20个子任务
- 4,750个测试样本
- 支持中英双语

任务类型：
- Single-Doc QA: 单文档问答
- Multi-Doc QA: 多文档问答
- Summarization: 摘要
- Few-shot Learning: 少样本学习
- Code Completion: 代码补全
- Synthetic Tasks: 合成任务

```

## 6. 多模态指标

### 🖼️ MMMU (Massive Multi-discipline Multimodal Understanding)

```python
特点：
- 11,570个多模态题目
- 覆盖6大学科
- 需要图文理解
- 大学级难度

顶级模型表现：
- GPT-4V: ~56.8%
- Gemini Ultra: ~59.4%
- Claude-3 Vision: ~59.4%
- Qwen-VL-Plus: ~45.2%

```

### 📊 ChartQA

```python
特点：
- 图表理解问答
- 4,804个真实图表
- 9,608个问答对
- 需要数据推理

```

## 7. 安全性指标

### 🛡️ TruthfulQA

```python
特点：
- 817个问题
- 测试诚实性
- 避免虚假信息
- 人类易错题目

顶级模型表现：
- GPT-4: ~95.6%
- Claude-3: ~94.8%
- Llama-3.1: ~86.3%

```

### ⚠️ ToxiGen

```python
特点：
- 毒性内容检测
- 13个身份群体
- 274K条测试数据
- 测试偏见和歧视

```

## 8. 综合评测平台

### 🏆 主流排行榜

```python
1. Hugging Face Open LLM Leaderboard
   - 综合6个核心指标
   - 自动化评测
   - 开源模型为主

2. LMSYS Chatbot Arena
   - 人类偏好评测
   - ELO排名系统
   - 实时对战

3. OpenCompass
   - 中文模型为主
   - 100+评测集
   - 详细能力图谱

4. SuperGLUE
   - 8个语言理解任务
   - 比GLUE更难
   - 人类基准~89.8%

```

## 9. 评测代码示例

```python
# 使用 lm-evaluation-harness
from lm_eval import evaluator
from lm_eval.models.huggingface import HFLM

# 评测MMLU
model = HFLM(pretrained="Qwen/Qwen2.5-7B-Instruct")
results = evaluator.simple_evaluate(
    model=model,
    tasks=["mmlu"],
    num_fewshot=5,
    batch_size=8
)

# 评测多个指标
tasks = ["mmlu", "gsm8k", "humaneval", "arc_challenge"]
results = evaluator.simple_evaluate(
    model=model,
    tasks=tasks,
    num_fewshot=5
)

print(f"MMLU Score: {results['mmlu']['acc']}")

```

## 10. 指标选择建议

### 根据应用场景选择：

```python
场景映射 = {
    "通用助手": ["MMLU", "GSM8K", "TruthfulQA"],
    "代码生成": ["HumanEval", "MBPP", "CodeForces"],
    "数学推理": ["MATH", "AIME", "GSM8K"],
    "中文应用": ["C-Eval", "CMMLU", "CLUE"],
    "科研助手": ["GPQA", "MMLU-STEM", "ARC"],
    "长文本": ["LongBench", "Needle-in-Haystack"],
    "多模态": ["MMMU", "ChartQA", "VQAv2"]
}

```

---

---

---

---

---

---

---

---

---

> References

