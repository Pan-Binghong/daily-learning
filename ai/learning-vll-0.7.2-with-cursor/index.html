<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Learning vLL (0.7.2) with Cursor | Pan Binghong's Tech Blog</title><meta name=keywords content="VLLM"><meta name=description content='
ğŸ’¡ ä¹‹å‰ä¸€ç›´ç”¨cursorè¾…åŠ©å†™ä»£ç ï¼Œçªç„¶è¯•äº†ä¸€ä¸‹å»åˆ©ç”¨cursoræ¥å­¦ä»£ç ï¼Œå‘ç°æ•ˆæœå˜å˜å¥½ã€‚è¿™é‡Œè®°å½•ä¸€ä¸‹æ¯”è¾ƒç«çš„vllmä¸­æå‡æ¨ç†é€Ÿåº¦çš„ä¸€äº›å…³é”®ä»£ç ã€‚

1. What is vLLMï¼Ÿ

vLLMæ˜¯å¤§æ¨¡å‹æ¨ç†åŠ é€Ÿæ¡†æ¶ã€‚ä¸»è¦åœ¨KVcacheç®¡ç†ä¸­é‡‡ç”¨äº†PageAttentionæŠ€æœ¯ï¼Œæ”¯æŒå¸‚é¢ä¸Šä¸»æµçš„å¼€æºå¤§æ¨¡å‹ï¼Œæ”¯æŒé‡åŒ–ç±»å‹ä¸ºï¼šGPTQ,Â AWQ, INT4, INT8, and FP8

2. KVcacheç›¸å…³çš„è®¡ç®—ä»£ç 
ä½¿ç”¨claude-3.7-sonnet thinkingï¼Œå¯¹vllmæ•´ä¸ªé¡¹ç›®è¿›è¡Œæé—®ã€‚

ä¸ºäº†æ›´å¥½ä¸¾ä¾‹ï¼Œä¸‹è½½äº†deepseek-v3-0324ç‰ˆæœ¬çš„config.jsonï¼Œä»£å…¥é¡¹ç›®è¿›è¡Œæé—®ã€‚


 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71


{
  "architectures": [
    "DeepseekV3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "configuration_deepseek.DeepseekV3Config",
    "AutoModel": "modeling_deepseek.DeepseekV3Model",
    "AutoModelForCausalLM": "modeling_deepseek.DeepseekV3ForCausalLM"
  },
  "aux_loss_alpha": 0.001,
  "bos_token_id": 0,
  "eos_token_id": 1,
  "ep_size": 1,
  "first_k_dense_replace": 3,
  "hidden_act": "silu",
  "hidden_size": 7168,
  "initializer_range": 0.02,
  "intermediate_size": 18432,
  "kv_lora_rank": 512,
  "max_position_embeddings": 163840,
  "model_type": "deepseek_v3",
  "moe_intermediate_size": 2048,
  "moe_layer_freq": 1,
  "n_group": 8,
  "n_routed_experts": 256,
  "n_shared_experts": 1,
  "norm_topk_prob": true,
  "num_attention_heads": 128,
  "num_experts_per_tok": 8,
  "num_hidden_layers": 61,
  "num_key_value_heads": 128,
  "num_nextn_predict_layers": 1,
  "pretraining_tp": 1,æ¨¡å‹ä¸­Transformerå±‚çš„æ•°é‡ã€‚æ›´å¤šçš„å±‚æ•°ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æ›´æ·±å±‚æ¬¡çš„ç‰¹å¾å’Œå…³ç³»ï¼Œä½†ä¹Ÿå¢åŠ äº†æ¨¡å‹çš„æ·±åº¦å’Œè®¡ç®—æˆæœ¬ã€‚

  "q_lora_rank": 1536,
  "qk_nope_head_dim": 128,
  "qk_rope_head_dim": 64,
  "quantization_config": {
    "activation_scheme": "dynamic",
    "fmt": "e4m3",
    "quant_method": "fp8",
    "weight_block_size": [
      128,
      128
    ]
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "beta_fast": 32,
    "beta_slow": 1,
    "factor": 40,
    "mscale": 1.0,
    "mscale_all_dim": 1.0,
    "original_max_position_embeddings": 4096,
    "type": "yarn"
  },
  "rope_theta": 10000,
  "routed_scaling_factor": 2.5,
  "scoring_func": "sigmoid",
  "seq_aux": true,
  "tie_word_embeddings": false,
  "topk_group": 4,
  "topk_method": "noaux_tc",
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "v_head_dim": 128,
  "vocab_size": 129280
}


è¯¥æ–‡ä»¶ä¸­çš„å…³é”®ä¿¡æ¯ä¸ºï¼š'><meta name=author content="Pan Binghong"><link rel=canonical href=https://Pan-Binghong.github.io/daily-learning/ai/learning-vll-0.7.2-with-cursor/><link crossorigin=anonymous href=/daily-learning/assets/css/stylesheet.8c4274f6592e00e67f9d9e38f6a29695c6a6ce5eded446ed371ff6565dbb189f.css integrity="sha256-jEJ09lkuAOZ/nZ449qKWlcamzl7e1EbtNx/2Vl27GJ8=" rel="preload stylesheet" as=style><link rel=icon href=https://Pan-Binghong.github.io/daily-learning/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://Pan-Binghong.github.io/daily-learning/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://Pan-Binghong.github.io/daily-learning/favicon-32x32.png><link rel=apple-touch-icon href=https://Pan-Binghong.github.io/daily-learning/apple-touch-icon.png><link rel=mask-icon href=https://Pan-Binghong.github.io/daily-learning/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://Pan-Binghong.github.io/daily-learning/ai/learning-vll-0.7.2-with-cursor/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><link rel=stylesheet href=https://unpkg.com/github-calendar@latest/dist/github-calendar-responsive.css><style>.reading-progress-bar{position:fixed;top:0;left:0;height:3px;background:linear-gradient(90deg,var(--primary),#00d4ff);z-index:9999;transition:width .1s ease;box-shadow:0 0 10px rgba(0,212,255,.5)}</style><script>document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();const t=document.querySelector(this.getAttribute("href"));t&&t.scrollIntoView({behavior:"smooth",block:"start"})})})})</script><meta property="og:url" content="https://Pan-Binghong.github.io/daily-learning/ai/learning-vll-0.7.2-with-cursor/"><meta property="og:site_name" content="Pan Binghong's Tech Blog"><meta property="og:title" content="Learning vLL (0.7.2) with Cursor"><meta property="og:description" content=' ğŸ’¡ ä¹‹å‰ä¸€ç›´ç”¨cursorè¾…åŠ©å†™ä»£ç ï¼Œçªç„¶è¯•äº†ä¸€ä¸‹å»åˆ©ç”¨cursoræ¥å­¦ä»£ç ï¼Œå‘ç°æ•ˆæœå˜å˜å¥½ã€‚è¿™é‡Œè®°å½•ä¸€ä¸‹æ¯”è¾ƒç«çš„vllmä¸­æå‡æ¨ç†é€Ÿåº¦çš„ä¸€äº›å…³é”®ä»£ç ã€‚
1. What is vLLMï¼Ÿ vLLMæ˜¯å¤§æ¨¡å‹æ¨ç†åŠ é€Ÿæ¡†æ¶ã€‚ä¸»è¦åœ¨KVcacheç®¡ç†ä¸­é‡‡ç”¨äº†PageAttentionæŠ€æœ¯ï¼Œæ”¯æŒå¸‚é¢ä¸Šä¸»æµçš„å¼€æºå¤§æ¨¡å‹ï¼Œæ”¯æŒé‡åŒ–ç±»å‹ä¸ºï¼šGPTQ,Â AWQ, INT4, INT8, and FP8
2. KVcacheç›¸å…³çš„è®¡ç®—ä»£ç  ä½¿ç”¨claude-3.7-sonnet thinkingï¼Œå¯¹vllmæ•´ä¸ªé¡¹ç›®è¿›è¡Œæé—®ã€‚
ä¸ºäº†æ›´å¥½ä¸¾ä¾‹ï¼Œä¸‹è½½äº†deepseek-v3-0324ç‰ˆæœ¬çš„config.jsonï¼Œä»£å…¥é¡¹ç›®è¿›è¡Œæé—®ã€‚
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 { "architectures": [ "DeepseekV3ForCausalLM" ], "attention_bias": false, "attention_dropout": 0.0, "auto_map": { "AutoConfig": "configuration_deepseek.DeepseekV3Config", "AutoModel": "modeling_deepseek.DeepseekV3Model", "AutoModelForCausalLM": "modeling_deepseek.DeepseekV3ForCausalLM" }, "aux_loss_alpha": 0.001, "bos_token_id": 0, "eos_token_id": 1, "ep_size": 1, "first_k_dense_replace": 3, "hidden_act": "silu", "hidden_size": 7168, "initializer_range": 0.02, "intermediate_size": 18432, "kv_lora_rank": 512, "max_position_embeddings": 163840, "model_type": "deepseek_v3", "moe_intermediate_size": 2048, "moe_layer_freq": 1, "n_group": 8, "n_routed_experts": 256, "n_shared_experts": 1, "norm_topk_prob": true, "num_attention_heads": 128, "num_experts_per_tok": 8, "num_hidden_layers": 61, "num_key_value_heads": 128, "num_nextn_predict_layers": 1, "pretraining_tp": 1,æ¨¡å‹ä¸­Transformerå±‚çš„æ•°é‡ã€‚æ›´å¤šçš„å±‚æ•°ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æ›´æ·±å±‚æ¬¡çš„ç‰¹å¾å’Œå…³ç³»ï¼Œä½†ä¹Ÿå¢åŠ äº†æ¨¡å‹çš„æ·±åº¦å’Œè®¡ç®—æˆæœ¬ã€‚ "q_lora_rank": 1536, "qk_nope_head_dim": 128, "qk_rope_head_dim": 64, "quantization_config": { "activation_scheme": "dynamic", "fmt": "e4m3", "quant_method": "fp8", "weight_block_size": [ 128, 128 ] }, "rms_norm_eps": 1e-06, "rope_scaling": { "beta_fast": 32, "beta_slow": 1, "factor": 40, "mscale": 1.0, "mscale_all_dim": 1.0, "original_max_position_embeddings": 4096, "type": "yarn" }, "rope_theta": 10000, "routed_scaling_factor": 2.5, "scoring_func": "sigmoid", "seq_aux": true, "tie_word_embeddings": false, "topk_group": 4, "topk_method": "noaux_tc", "torch_dtype": "bfloat16", "transformers_version": "4.46.3", "use_cache": true, "v_head_dim": 128, "vocab_size": 129280 } è¯¥æ–‡ä»¶ä¸­çš„å…³é”®ä¿¡æ¯ä¸ºï¼š'><meta property="og:locale" content="zh-CN"><meta property="og:type" content="article"><meta property="article:section" content="ai"><meta property="article:published_time" content="2025-04-14T00:38:00+00:00"><meta property="article:modified_time" content="2025-12-08T02:53:55+00:00"><meta property="article:tag" content="VLLM"><meta name=twitter:card content="summary"><meta name=twitter:title content="Learning vLL (0.7.2) with Cursor"><meta name=twitter:description content='
ğŸ’¡ ä¹‹å‰ä¸€ç›´ç”¨cursorè¾…åŠ©å†™ä»£ç ï¼Œçªç„¶è¯•äº†ä¸€ä¸‹å»åˆ©ç”¨cursoræ¥å­¦ä»£ç ï¼Œå‘ç°æ•ˆæœå˜å˜å¥½ã€‚è¿™é‡Œè®°å½•ä¸€ä¸‹æ¯”è¾ƒç«çš„vllmä¸­æå‡æ¨ç†é€Ÿåº¦çš„ä¸€äº›å…³é”®ä»£ç ã€‚

1. What is vLLMï¼Ÿ

vLLMæ˜¯å¤§æ¨¡å‹æ¨ç†åŠ é€Ÿæ¡†æ¶ã€‚ä¸»è¦åœ¨KVcacheç®¡ç†ä¸­é‡‡ç”¨äº†PageAttentionæŠ€æœ¯ï¼Œæ”¯æŒå¸‚é¢ä¸Šä¸»æµçš„å¼€æºå¤§æ¨¡å‹ï¼Œæ”¯æŒé‡åŒ–ç±»å‹ä¸ºï¼šGPTQ,Â AWQ, INT4, INT8, and FP8

2. KVcacheç›¸å…³çš„è®¡ç®—ä»£ç 
ä½¿ç”¨claude-3.7-sonnet thinkingï¼Œå¯¹vllmæ•´ä¸ªé¡¹ç›®è¿›è¡Œæé—®ã€‚

ä¸ºäº†æ›´å¥½ä¸¾ä¾‹ï¼Œä¸‹è½½äº†deepseek-v3-0324ç‰ˆæœ¬çš„config.jsonï¼Œä»£å…¥é¡¹ç›®è¿›è¡Œæé—®ã€‚


 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71


{
  "architectures": [
    "DeepseekV3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "configuration_deepseek.DeepseekV3Config",
    "AutoModel": "modeling_deepseek.DeepseekV3Model",
    "AutoModelForCausalLM": "modeling_deepseek.DeepseekV3ForCausalLM"
  },
  "aux_loss_alpha": 0.001,
  "bos_token_id": 0,
  "eos_token_id": 1,
  "ep_size": 1,
  "first_k_dense_replace": 3,
  "hidden_act": "silu",
  "hidden_size": 7168,
  "initializer_range": 0.02,
  "intermediate_size": 18432,
  "kv_lora_rank": 512,
  "max_position_embeddings": 163840,
  "model_type": "deepseek_v3",
  "moe_intermediate_size": 2048,
  "moe_layer_freq": 1,
  "n_group": 8,
  "n_routed_experts": 256,
  "n_shared_experts": 1,
  "norm_topk_prob": true,
  "num_attention_heads": 128,
  "num_experts_per_tok": 8,
  "num_hidden_layers": 61,
  "num_key_value_heads": 128,
  "num_nextn_predict_layers": 1,
  "pretraining_tp": 1,æ¨¡å‹ä¸­Transformerå±‚çš„æ•°é‡ã€‚æ›´å¤šçš„å±‚æ•°ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æ›´æ·±å±‚æ¬¡çš„ç‰¹å¾å’Œå…³ç³»ï¼Œä½†ä¹Ÿå¢åŠ äº†æ¨¡å‹çš„æ·±åº¦å’Œè®¡ç®—æˆæœ¬ã€‚

  "q_lora_rank": 1536,
  "qk_nope_head_dim": 128,
  "qk_rope_head_dim": 64,
  "quantization_config": {
    "activation_scheme": "dynamic",
    "fmt": "e4m3",
    "quant_method": "fp8",
    "weight_block_size": [
      128,
      128
    ]
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "beta_fast": 32,
    "beta_slow": 1,
    "factor": 40,
    "mscale": 1.0,
    "mscale_all_dim": 1.0,
    "original_max_position_embeddings": 4096,
    "type": "yarn"
  },
  "rope_theta": 10000,
  "routed_scaling_factor": 2.5,
  "scoring_func": "sigmoid",
  "seq_aux": true,
  "tie_word_embeddings": false,
  "topk_group": 4,
  "topk_method": "noaux_tc",
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "v_head_dim": 128,
  "vocab_size": 129280
}


è¯¥æ–‡ä»¶ä¸­çš„å…³é”®ä¿¡æ¯ä¸ºï¼š'><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Ais","item":"https://Pan-Binghong.github.io/daily-learning/ai/"},{"@type":"ListItem","position":2,"name":"Learning vLL (0.7.2) with Cursor","item":"https://Pan-Binghong.github.io/daily-learning/ai/learning-vll-0.7.2-with-cursor/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Learning vLL (0.7.2) with Cursor","name":"Learning vLL (0.7.2) with Cursor","description":" ğŸ’¡ ä¹‹å‰ä¸€ç›´ç”¨cursorè¾…åŠ©å†™ä»£ç ï¼Œçªç„¶è¯•äº†ä¸€ä¸‹å»åˆ©ç”¨cursoræ¥å­¦ä»£ç ï¼Œå‘ç°æ•ˆæœå˜å˜å¥½ã€‚è¿™é‡Œè®°å½•ä¸€ä¸‹æ¯”è¾ƒç«çš„vllmä¸­æå‡æ¨ç†é€Ÿåº¦çš„ä¸€äº›å…³é”®ä»£ç ã€‚\n1. What is vLLMï¼Ÿ vLLMæ˜¯å¤§æ¨¡å‹æ¨ç†åŠ é€Ÿæ¡†æ¶ã€‚ä¸»è¦åœ¨KVcacheç®¡ç†ä¸­é‡‡ç”¨äº†PageAttentionæŠ€æœ¯ï¼Œæ”¯æŒå¸‚é¢ä¸Šä¸»æµçš„å¼€æºå¤§æ¨¡å‹ï¼Œæ”¯æŒé‡åŒ–ç±»å‹ä¸ºï¼šGPTQ,Â AWQ, INT4, INT8, and FP8\n2. KVcacheç›¸å…³çš„è®¡ç®—ä»£ç  ä½¿ç”¨claude-3.7-sonnet thinkingï¼Œå¯¹vllmæ•´ä¸ªé¡¹ç›®è¿›è¡Œæé—®ã€‚\nä¸ºäº†æ›´å¥½ä¸¾ä¾‹ï¼Œä¸‹è½½äº†deepseek-v3-0324ç‰ˆæœ¬çš„config.jsonï¼Œä»£å…¥é¡¹ç›®è¿›è¡Œæé—®ã€‚\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 { \u0026#34;architectures\u0026#34;: [ \u0026#34;DeepseekV3ForCausalLM\u0026#34; ], \u0026#34;attention_bias\u0026#34;: false, \u0026#34;attention_dropout\u0026#34;: 0.0, \u0026#34;auto_map\u0026#34;: { \u0026#34;AutoConfig\u0026#34;: \u0026#34;configuration_deepseek.DeepseekV3Config\u0026#34;, \u0026#34;AutoModel\u0026#34;: \u0026#34;modeling_deepseek.DeepseekV3Model\u0026#34;, \u0026#34;AutoModelForCausalLM\u0026#34;: \u0026#34;modeling_deepseek.DeepseekV3ForCausalLM\u0026#34; }, \u0026#34;aux_loss_alpha\u0026#34;: 0.001, \u0026#34;bos_token_id\u0026#34;: 0, \u0026#34;eos_token_id\u0026#34;: 1, \u0026#34;ep_size\u0026#34;: 1, \u0026#34;first_k_dense_replace\u0026#34;: 3, \u0026#34;hidden_act\u0026#34;: \u0026#34;silu\u0026#34;, \u0026#34;hidden_size\u0026#34;: 7168, \u0026#34;initializer_range\u0026#34;: 0.02, \u0026#34;intermediate_size\u0026#34;: 18432, \u0026#34;kv_lora_rank\u0026#34;: 512, \u0026#34;max_position_embeddings\u0026#34;: 163840, \u0026#34;model_type\u0026#34;: \u0026#34;deepseek_v3\u0026#34;, \u0026#34;moe_intermediate_size\u0026#34;: 2048, \u0026#34;moe_layer_freq\u0026#34;: 1, \u0026#34;n_group\u0026#34;: 8, \u0026#34;n_routed_experts\u0026#34;: 256, \u0026#34;n_shared_experts\u0026#34;: 1, \u0026#34;norm_topk_prob\u0026#34;: true, \u0026#34;num_attention_heads\u0026#34;: 128, \u0026#34;num_experts_per_tok\u0026#34;: 8, \u0026#34;num_hidden_layers\u0026#34;: 61, \u0026#34;num_key_value_heads\u0026#34;: 128, \u0026#34;num_nextn_predict_layers\u0026#34;: 1, \u0026#34;pretraining_tp\u0026#34;: 1,æ¨¡å‹ä¸­Transformerå±‚çš„æ•°é‡ã€‚æ›´å¤šçš„å±‚æ•°ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æ›´æ·±å±‚æ¬¡çš„ç‰¹å¾å’Œå…³ç³»ï¼Œä½†ä¹Ÿå¢åŠ äº†æ¨¡å‹çš„æ·±åº¦å’Œè®¡ç®—æˆæœ¬ã€‚ \u0026#34;q_lora_rank\u0026#34;: 1536, \u0026#34;qk_nope_head_dim\u0026#34;: 128, \u0026#34;qk_rope_head_dim\u0026#34;: 64, \u0026#34;quantization_config\u0026#34;: { \u0026#34;activation_scheme\u0026#34;: \u0026#34;dynamic\u0026#34;, \u0026#34;fmt\u0026#34;: \u0026#34;e4m3\u0026#34;, \u0026#34;quant_method\u0026#34;: \u0026#34;fp8\u0026#34;, \u0026#34;weight_block_size\u0026#34;: [ 128, 128 ] }, \u0026#34;rms_norm_eps\u0026#34;: 1e-06, \u0026#34;rope_scaling\u0026#34;: { \u0026#34;beta_fast\u0026#34;: 32, \u0026#34;beta_slow\u0026#34;: 1, \u0026#34;factor\u0026#34;: 40, \u0026#34;mscale\u0026#34;: 1.0, \u0026#34;mscale_all_dim\u0026#34;: 1.0, \u0026#34;original_max_position_embeddings\u0026#34;: 4096, \u0026#34;type\u0026#34;: \u0026#34;yarn\u0026#34; }, \u0026#34;rope_theta\u0026#34;: 10000, \u0026#34;routed_scaling_factor\u0026#34;: 2.5, \u0026#34;scoring_func\u0026#34;: \u0026#34;sigmoid\u0026#34;, \u0026#34;seq_aux\u0026#34;: true, \u0026#34;tie_word_embeddings\u0026#34;: false, \u0026#34;topk_group\u0026#34;: 4, \u0026#34;topk_method\u0026#34;: \u0026#34;noaux_tc\u0026#34;, \u0026#34;torch_dtype\u0026#34;: \u0026#34;bfloat16\u0026#34;, \u0026#34;transformers_version\u0026#34;: \u0026#34;4.46.3\u0026#34;, \u0026#34;use_cache\u0026#34;: true, \u0026#34;v_head_dim\u0026#34;: 128, \u0026#34;vocab_size\u0026#34;: 129280 } è¯¥æ–‡ä»¶ä¸­çš„å…³é”®ä¿¡æ¯ä¸ºï¼š\n","keywords":["VLLM"],"articleBody":" ğŸ’¡ ä¹‹å‰ä¸€ç›´ç”¨cursorè¾…åŠ©å†™ä»£ç ï¼Œçªç„¶è¯•äº†ä¸€ä¸‹å»åˆ©ç”¨cursoræ¥å­¦ä»£ç ï¼Œå‘ç°æ•ˆæœå˜å˜å¥½ã€‚è¿™é‡Œè®°å½•ä¸€ä¸‹æ¯”è¾ƒç«çš„vllmä¸­æå‡æ¨ç†é€Ÿåº¦çš„ä¸€äº›å…³é”®ä»£ç ã€‚\n1. What is vLLMï¼Ÿ vLLMæ˜¯å¤§æ¨¡å‹æ¨ç†åŠ é€Ÿæ¡†æ¶ã€‚ä¸»è¦åœ¨KVcacheç®¡ç†ä¸­é‡‡ç”¨äº†PageAttentionæŠ€æœ¯ï¼Œæ”¯æŒå¸‚é¢ä¸Šä¸»æµçš„å¼€æºå¤§æ¨¡å‹ï¼Œæ”¯æŒé‡åŒ–ç±»å‹ä¸ºï¼šGPTQ,Â AWQ, INT4, INT8, and FP8\n2. KVcacheç›¸å…³çš„è®¡ç®—ä»£ç  ä½¿ç”¨claude-3.7-sonnet thinkingï¼Œå¯¹vllmæ•´ä¸ªé¡¹ç›®è¿›è¡Œæé—®ã€‚\nä¸ºäº†æ›´å¥½ä¸¾ä¾‹ï¼Œä¸‹è½½äº†deepseek-v3-0324ç‰ˆæœ¬çš„config.jsonï¼Œä»£å…¥é¡¹ç›®è¿›è¡Œæé—®ã€‚\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 { \"architectures\": [ \"DeepseekV3ForCausalLM\" ], \"attention_bias\": false, \"attention_dropout\": 0.0, \"auto_map\": { \"AutoConfig\": \"configuration_deepseek.DeepseekV3Config\", \"AutoModel\": \"modeling_deepseek.DeepseekV3Model\", \"AutoModelForCausalLM\": \"modeling_deepseek.DeepseekV3ForCausalLM\" }, \"aux_loss_alpha\": 0.001, \"bos_token_id\": 0, \"eos_token_id\": 1, \"ep_size\": 1, \"first_k_dense_replace\": 3, \"hidden_act\": \"silu\", \"hidden_size\": 7168, \"initializer_range\": 0.02, \"intermediate_size\": 18432, \"kv_lora_rank\": 512, \"max_position_embeddings\": 163840, \"model_type\": \"deepseek_v3\", \"moe_intermediate_size\": 2048, \"moe_layer_freq\": 1, \"n_group\": 8, \"n_routed_experts\": 256, \"n_shared_experts\": 1, \"norm_topk_prob\": true, \"num_attention_heads\": 128, \"num_experts_per_tok\": 8, \"num_hidden_layers\": 61, \"num_key_value_heads\": 128, \"num_nextn_predict_layers\": 1, \"pretraining_tp\": 1,æ¨¡å‹ä¸­Transformerå±‚çš„æ•°é‡ã€‚æ›´å¤šçš„å±‚æ•°ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æ›´æ·±å±‚æ¬¡çš„ç‰¹å¾å’Œå…³ç³»ï¼Œä½†ä¹Ÿå¢åŠ äº†æ¨¡å‹çš„æ·±åº¦å’Œè®¡ç®—æˆæœ¬ã€‚ \"q_lora_rank\": 1536, \"qk_nope_head_dim\": 128, \"qk_rope_head_dim\": 64, \"quantization_config\": { \"activation_scheme\": \"dynamic\", \"fmt\": \"e4m3\", \"quant_method\": \"fp8\", \"weight_block_size\": [ 128, 128 ] }, \"rms_norm_eps\": 1e-06, \"rope_scaling\": { \"beta_fast\": 32, \"beta_slow\": 1, \"factor\": 40, \"mscale\": 1.0, \"mscale_all_dim\": 1.0, \"original_max_position_embeddings\": 4096, \"type\": \"yarn\" }, \"rope_theta\": 10000, \"routed_scaling_factor\": 2.5, \"scoring_func\": \"sigmoid\", \"seq_aux\": true, \"tie_word_embeddings\": false, \"topk_group\": 4, \"topk_method\": \"noaux_tc\", \"torch_dtype\": \"bfloat16\", \"transformers_version\": \"4.46.3\", \"use_cache\": true, \"v_head_dim\": 128, \"vocab_size\": 129280 } è¯¥æ–‡ä»¶ä¸­çš„å…³é”®ä¿¡æ¯ä¸ºï¼š\n2.1 DeepSeek-V1çš„KVcacheæ˜¯å¦‚ä½•è®¡ç®—çš„ï¼Ÿ 2.1.1 æ¨¡å‹å‚æ•°åˆ†æ ä»æä¾›çš„config.jsonæ–‡ä»¶ä¸­æå–çš„å…³é”®å‚æ•°ï¼š\nhidden_size: 7168 num_attention_heads: 128Â (Qå¤´æ•°é‡) num_key_value_heads: 128Â (KVå¤´æ•°é‡ï¼Œæ— MHAç»“æ„) num_hidden_layers: 61Â (Transformerå±‚æ•°) max_position_embeddings: 163840 (æœ€å¤§ä½ç½®åµŒå…¥ï¼Œè¡¨ç¤ºæ”¯æŒçš„æœ€å¤§åºåˆ—é•¿åº¦) torch_dtype: â€œbfloat16â€ (æ¨¡å‹å‚æ•°æ•°æ®ç±»å‹) 2.1.2 KVç¼“å­˜æ˜¾å­˜è®¡ç®— åŸºç¡€å‚æ•°è®¡ç®—\nhead_size = hidden_size / num_attention_heads = 7168 / 128 = 56\næ•°æ®ç±»å‹å¤§å° = bfloat16 = 2å­—èŠ‚ å•ä¸ªæ³¨æ„åŠ›å±‚çš„KVcacheè®¡ç®—\nkey cache size = num_key_value_heads * head_size = 128 * 56 = 7168\nvalue cache size = key cache size = 7168\nå•å±‚å•tokençš„KVç¼“å­˜å¤§å° = (keyç¼“å­˜é¡¹ + valueç¼“å­˜é¡¹) * dtype = (7168 + 7168) * 2 = 28672 â‰ˆ 28kb å…¨æ¨¡å‹æœ€å¤§KVç¼“å­˜è®¡ç®—\nå…¨æ¨¡å‹å•tokençš„KVç¼“å­˜å¤§å° = å•å±‚tokençš„KVç¼“å­˜ * æ¨¡å‹å±‚æ•° = 28kb * 61 = 1708kb â‰ˆ 1.67MB\næœ€å¤§åºåˆ—é•¿åº¦ä¸‹çš„KVç¼“å­˜å¤§å° = å…¨æ¨¡å‹å•tokençš„KVç¼“å­˜å¤§å° * æœ€å¤§åºåˆ—é•¿åº¦ = 273612.8MB â‰ˆ 267.2GB æŒ‰å—åˆ†é…è®¡ç®—\nå¦‚æœä½¿ç”¨vLLMçš„åˆ†å—æœºåˆ¶ï¼Œå‡è®¾block_size=16ï¼š\næ¯ä¸ªå—å¤§å° = æ•°æ®ç±»å‹å¤§å° * æ³¨æ„åŠ›å±‚æ•° * block_size * (key_cache_entry + value_cache_entry) = 2 * 61 * 16 * (7168+7168) â‰ˆ 27.9MB éœ€è¦å—çš„æ•°é‡ = æœ€å¤§åºåˆ—é•¿åº¦ / block_size = 163840 / 16 = 10240å— æ€»æ˜¾å­˜ = æ¯ä¸ªå—å¤§å° * éœ€è¦çš„å—æ•°é‡ = 27.9MB * 10240 â‰ˆ 286GB å®é™…åº”ç”¨è€ƒè™‘ å®é™…ä½¿ç”¨æ—¶å¯èƒ½ä¸ä¼šç”¨æœ€å¤§çš„åºåˆ—é•¿åº¦ï¼Œå¸¸è§çš„è®¾ç½®ä¸º4K - 32K ä¸åŒçš„block_sizeçš„é€‰æ‹©ä¼šå½±å“æ˜¾å­˜ä½¿ç”¨æ•ˆç‡ ä¾‹å¦‚ï¼Œå¦‚æœä½¿ç”¨8Kçš„åºåˆ—é•¿åº¦: KVç¼“å­˜ = 1.67MB * 8192 â‰ˆ 13.7GB 2.1.3 è®¡ç®—KVcacheä»£ç çš„ä½ç½® åœ¨0.7.2ç‰ˆæœ¬ä¸­ï¼Œvllm\\worker\\worker.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 @torch.inference_mode() def determine_num_available_blocks(self) -\u003e Tuple[int, int]: \"\"\"ç¡®å®šå¯ç”¨çš„KVç¼“å­˜å—æ•°é‡ã€‚ è¯¥æ–¹æ³•ä¼šé¦–å…ˆå¯¹ç°æœ‰å†…å­˜ä½¿ç”¨æƒ…å†µè¿›è¡Œåˆ†æ,ç„¶åè®¡ç®—åœ¨å‰©ä½™å¯ç”¨å†…å­˜ä¸‹å¯ä»¥åˆ†é…çš„æœ€å¤§GPUå’ŒCPUå—æ•°é‡ã€‚ ä¸»è¦æ­¥éª¤: 1. æ¸…ç©ºGPUç¼“å­˜å¹¶é‡ç½®å†…å­˜ç»Ÿè®¡ä¿¡æ¯ 2. æ‰§è¡Œä¸€æ¬¡å‰å‘ä¼ æ’­æ¥åˆ†ææ¨¡å‹çš„å†…å­˜ä½¿ç”¨æƒ…å†µ 3. æ ¹æ®GPUå†…å­˜åˆ©ç”¨ç‡å’Œå¯ç”¨å†…å­˜è®¡ç®—å¯åˆ†é…çš„KVç¼“å­˜å—æ•°é‡ 4. è®°å½•è¯¦ç»†çš„å†…å­˜ä½¿ç”¨æƒ…å†µ Returns: Tuple[int, int]: è¿”å›(GPUå—æ•°é‡, CPUå—æ•°é‡)çš„å…ƒç»„ \"\"\" # æ¸…ç©ºGPUç¼“å­˜å¹¶é‡ç½®å†…å­˜ç»Ÿè®¡ä¿¡æ¯ torch.cuda.empty_cache() torch.cuda.reset_peak_memory_stats() # è·å–å½“å‰ç©ºé—²å†…å­˜å’Œæ€»GPUå†…å­˜ free_memory_pre_profile, total_gpu_memory = torch.cuda.mem_get_info() # ä½¿ç”¨è™šæ‹Ÿè¾“å…¥æ‰§è¡Œä¸€æ¬¡å‰å‘ä¼ æ’­,åˆ†ææ¨¡å‹å†…å­˜ä½¿ç”¨æƒ…å†µ with memory_profiling( self.baseline_snapshot, weights_memory=self.model_runner.model_memory_usage) as result: self.model_runner.profile_run() # éªŒè¯å†…å­˜å ç”¨æ˜¯å¦å¢åŠ  self._assert_memory_footprint_increased_during_profiling() # è®¡ç®—å½“å‰å®ä¾‹å¯ç”¨çš„æ€»å†…å­˜ memory_for_current_instance = total_gpu_memory * \\ self.cache_config.gpu_memory_utilization # è®¡ç®—å¯ç”¨äºKVç¼“å­˜çš„å†…å­˜ available_kv_cache_memory = (memory_for_current_instance - result.non_kv_cache_memory) # è®¡ç®—å¯åˆ†é…çš„å—æ•°é‡ cache_block_size = self.get_cache_block_size_bytes() if cache_block_size == 0: num_gpu_blocks = 0 num_cpu_blocks = 0 else: # æ ¹æ®å¯ç”¨å†…å­˜è®¡ç®—GPUå’ŒCPUå—æ•°é‡ num_gpu_blocks = int(available_kv_cache_memory // cache_block_size) num_cpu_blocks = int(self.cache_config.swap_space_bytes // cache_block_size) num_gpu_blocks = max(num_gpu_blocks, 0) num_cpu_blocks = max(num_cpu_blocks, 0) # æ„å»ºè¯¦ç»†çš„å†…å­˜ä½¿ç”¨æƒ…å†µæ—¥å¿— msg = (f\"Memory profiling takes {result.profile_time:.2f} seconds\\n\" \"the current vLLM instance can use \" \"total_gpu_memory \" f\"({(total_gpu_memory / GiB_bytes):.2f}GiB)\" \" x gpu_memory_utilization \" f\"({self.cache_config.gpu_memory_utilization:.2f})\" f\" = {(memory_for_current_instance / GiB_bytes):.2f}GiB\\n\" \"model weights take \" f\"{(result.weights_memory / GiB_bytes):.2f}GiB;\" \" non_torch_memory takes \" f\"{(result.non_torch_increase / GiB_bytes):.2f}GiB;\" \" PyTorch activation peak memory takes \" f\"{(result.torch_peak_increase / GiB_bytes):.2f}GiB;\" \" the rest of the memory reserved for KV Cache is \" f\"{(available_kv_cache_memory / GiB_bytes):.2f}GiB.\") logger.info(msg) # Final cleanup gc.collect() return num_gpu_blocks, num_cpu_blocks 2.2 åœ¨vLLMä¸­å®ç°PageAttentionçš„ä»£ç åœ¨å“ªé‡Œï¼Ÿ å…³äºPageAttentionæŠ€æœ¯çš„å®ç°ï¼Œå®ƒä¸»è¦åœ¨ä»¥ä¸‹æ–‡ä»¶ä¸­ï¼š\nvllm/attention/ops/paged_attn.pyï¼šåŒ…å«PagedAttentionç±»ï¼Œæä¾›KVç¼“å­˜å½¢çŠ¶ã€åˆ†å‰²å’Œå†™å…¥ç­‰æ“ä½œçš„æ¥å£ vllm/_custom_ops.pyï¼šåŒ…å«paged_attention_v1å‡½æ•°ï¼Œè¿æ¥åˆ°C++åº•å±‚å®ç° csrc/attention/attention_kernels.cuï¼šåŒ…å«PageAttentionçš„CUDAå†…æ ¸å®ç° docs/source/design/kernel/paged_attention.mdï¼šè¯¦ç»†è§£é‡Šäº†PageAttentionçš„è®¾è®¡å’Œå®ç°åŸç† PageAttentionæ˜¯vLLMçš„æ ¸å¿ƒæŠ€æœ¯ï¼Œé€šè¿‡å°†KVç¼“å­˜å­˜å‚¨åœ¨ä¸è¿ç»­çš„å†…å­˜å—ä¸­ï¼Œå®ç°äº†é«˜æ•ˆçš„å†…å­˜ç®¡ç†ã€‚è¿™äº›å†…å­˜å—å¯ä»¥æ ¹æ®éœ€è¦åŠ¨æ€åˆ†é…å’Œé‡Šæ”¾ï¼Œä½¿å¾—å•ä¸ªGPUèƒ½å¤ŸæœåŠ¡æ›´å¤šå¹¶å‘è¯·æ±‚ã€‚è¯¥æŠ€æœ¯çš„å…³é”®ç‰¹ç‚¹æ˜¯é€šè¿‡åˆ†é¡µæœºåˆ¶é¿å…äº†æ˜¾å­˜ç¢ç‰‡ï¼Œå¹¶æ”¯æŒé«˜æ•ˆçš„ä¸Šä¸‹æ–‡å¤„ç†ã€‚ 3. éªŒè¯ ä½¿ç”¨4090DåŸºäºvllmå¯ç”¨é»˜è®¤å‚æ•°è¿›è¡Œæ¨ç†ã€‚æŸ¥çœ‹å¯åŠ¨æœåŠ¡çš„æ—¥å¿—å†…å®¹ã€‚\n3.1 æ¨¡å‹æ—¥å¿— 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 INFO 04-14 06:00:50 [__init__.py:239] Automatically detected platform cuda. INFO 04-14 06:00:52 [api_server.py:1034] vLLM API server version 0.8.3 INFO 04-14 06:00:52 [api_server.py:1035] args: Namespace(subparser='serve', model_tag='/data/DeepSeek-R1-Distill-Qwen-1.5B', config='', host=None, port=8000, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/data/DeepSeek-R1-Distill-Qwen-1.5B', task='auto', tokenizer=None, hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=","wordCount":"2323","inLanguage":"en","datePublished":"2025-04-14T00:38:00Z","dateModified":"2025-12-08T02:53:55Z","author":{"@type":"Person","name":"Pan Binghong"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://Pan-Binghong.github.io/daily-learning/ai/learning-vll-0.7.2-with-cursor/"},"publisher":{"@type":"Organization","name":"Pan Binghong's Tech Blog","logo":{"@type":"ImageObject","url":"https://Pan-Binghong.github.io/daily-learning/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://Pan-Binghong.github.io/daily-learning/ accesskey=h title="Pan Binghong's Tech Blog (Alt + H)">Pan Binghong's Tech Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://Pan-Binghong.github.io/daily-learning/ title="ğŸ  é¦–é¡µ"><span>ğŸ  é¦–é¡µ</span></a></li><li><a href=https://Pan-Binghong.github.io/daily-learning/ai/ title="ğŸ¤– AI"><span>ğŸ¤– AI</span></a></li><li><a href=https://Pan-Binghong.github.io/daily-learning/knowledge/ title="ğŸ“š çŸ¥è¯†åº“"><span>ğŸ“š çŸ¥è¯†åº“</span></a></li><li><a href=https://Pan-Binghong.github.io/daily-learning/backend/ title="âš™ï¸ åç«¯"><span>âš™ï¸ åç«¯</span></a></li><li><a href=https://Pan-Binghong.github.io/daily-learning/devops/ title="ğŸ”§ DevOps"><span>ğŸ”§ DevOps</span></a></li><li><a href=https://Pan-Binghong.github.io/daily-learning/other/ title="ğŸ“ å…¶ä»–"><span>ğŸ“ å…¶ä»–</span></a></li><li><a href=https://Pan-Binghong.github.io/daily-learning/search/ title="ğŸ” æœç´¢"><span>ğŸ” æœç´¢</span></a></li><li><a href=https://Pan-Binghong.github.io/daily-learning/tags/ title="ğŸ·ï¸ æ ‡ç­¾"><span>ğŸ·ï¸ æ ‡ç­¾</span></a></li><li><a href=https://Pan-Binghong.github.io/daily-learning/archives/ title="ğŸ“‚ å½’æ¡£"><span>ğŸ“‚ å½’æ¡£</span></a></li><li><a href=https://Pan-Binghong.github.io/daily-learning/about/ title="ğŸ‘¤ å…³äº"><span>ğŸ‘¤ å…³äº</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://Pan-Binghong.github.io/daily-learning/>Home</a>&nbsp;Â»&nbsp;<a href=https://Pan-Binghong.github.io/daily-learning/ai/>Ais</a></div><h1 class="post-title entry-hint-parent">Learning vLL (0.7.2) with Cursor</h1><div class=post-meta><span title='2025-04-14 00:38:00 +0000 UTC'>April 14, 2025</span>&nbsp;Â·&nbsp;<span>11 min</span>&nbsp;Â·&nbsp;<span>2323 words</span>&nbsp;Â·&nbsp;<span>Pan Binghong</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#1-what-is-vllm aria-label="1. What is vLLMï¼Ÿ">1. What is vLLMï¼Ÿ</a></li><li><a href=#2-kvcache%e7%9b%b8%e5%85%b3%e7%9a%84%e8%ae%a1%e7%ae%97%e4%bb%a3%e7%a0%81 aria-label="2. KVcacheç›¸å…³çš„è®¡ç®—ä»£ç ">2. KVcacheç›¸å…³çš„è®¡ç®—ä»£ç </a><ul><li><a href=#21-deepseek-v1%e7%9a%84kvcache%e6%98%af%e5%a6%82%e4%bd%95%e8%ae%a1%e7%ae%97%e7%9a%84 aria-label="2.1 DeepSeek-V1çš„KVcacheæ˜¯å¦‚ä½•è®¡ç®—çš„ï¼Ÿ">2.1 DeepSeek-V1çš„KVcacheæ˜¯å¦‚ä½•è®¡ç®—çš„ï¼Ÿ</a><ul><li><a href=#211-%e6%a8%a1%e5%9e%8b%e5%8f%82%e6%95%b0%e5%88%86%e6%9e%90 aria-label="2.1.1 æ¨¡å‹å‚æ•°åˆ†æ">2.1.1 æ¨¡å‹å‚æ•°åˆ†æ</a></li><li><a href=#212-kv%e7%bc%93%e5%ad%98%e6%98%be%e5%ad%98%e8%ae%a1%e7%ae%97 aria-label="2.1.2 KVç¼“å­˜æ˜¾å­˜è®¡ç®—">2.1.2 KVç¼“å­˜æ˜¾å­˜è®¡ç®—</a></li><li><a href=#213-%e8%ae%a1%e7%ae%97kvcache%e4%bb%a3%e7%a0%81%e7%9a%84%e4%bd%8d%e7%bd%ae aria-label="2.1.3 è®¡ç®—KVcacheä»£ç çš„ä½ç½®">2.1.3 è®¡ç®—KVcacheä»£ç çš„ä½ç½®</a></li></ul></li><li><a href=#22-%e5%9c%a8vllm%e4%b8%ad%e5%ae%9e%e7%8e%b0pageattention%e7%9a%84%e4%bb%a3%e7%a0%81%e5%9c%a8%e5%93%aa%e9%87%8c aria-label="2.2 åœ¨vLLMä¸­å®ç°PageAttentionçš„ä»£ç åœ¨å“ªé‡Œï¼Ÿ">2.2 åœ¨vLLMä¸­å®ç°PageAttentionçš„ä»£ç åœ¨å“ªé‡Œï¼Ÿ</a></li></ul></li><li><a href=#3-%e9%aa%8c%e8%af%81 aria-label="3. éªŒè¯">3. éªŒè¯</a><ul><li><a href=#31-%e6%a8%a1%e5%9e%8b%e6%97%a5%e5%bf%97 aria-label="3.1 æ¨¡å‹æ—¥å¿—">3.1 æ¨¡å‹æ—¥å¿—</a></li><li><a href=#32-%e6%a8%a1%e5%9e%8b%e9%85%8d%e7%bd%ae aria-label="3.2 æ¨¡å‹é…ç½®">3.2 æ¨¡å‹é…ç½®</a></li><li><a href=#33-%e8%ae%a1%e7%ae%97%e9%aa%8c%e8%af%81 aria-label="3.3 è®¡ç®—éªŒè¯">3.3 è®¡ç®—éªŒè¯</a><ul><li><a href=#331-%e6%af%8f%e4%b8%aatoken%e7%9a%84kv%e7%bc%93%e5%ad%98%e5%a4%a7%e5%b0%8f%e8%ae%a1%e7%ae%97 aria-label="3.3.1 æ¯ä¸ªtokençš„KVç¼“å­˜å¤§å°è®¡ç®—">3.3.1 æ¯ä¸ªtokençš„KVç¼“å­˜å¤§å°è®¡ç®—</a></li><li><a href=#332-%e6%98%be%e5%ad%98%e5%88%86%e9%85%8d aria-label="3.3.2 æ˜¾å­˜åˆ†é…">3.3.2 æ˜¾å­˜åˆ†é…</a></li><li><a href=#333-%e5%8f%af%e7%bc%93%e5%ad%98%e7%9a%84token%e6%95%b0%e9%87%8f aria-label="3.3.3 å¯ç¼“å­˜çš„tokenæ•°é‡">3.3.3 å¯ç¼“å­˜çš„tokenæ•°é‡</a></li></ul></li></ul></li><li><a href=#4-vllm%e4%b8%ad%e5%a6%82%e4%bd%95%e8%ae%a1%e7%ae%97kvcache aria-label="4. vLLMä¸­å¦‚ä½•è®¡ç®—KVcache">4. vLLMä¸­å¦‚ä½•è®¡ç®—KVcache</a><ul><li><a href=#41-%e6%a0%b8%e5%bf%83%e8%ae%a1%e7%ae%97%e5%87%bd%e6%95%b0 aria-label="4.1 æ ¸å¿ƒè®¡ç®—å‡½æ•°">4.1 æ ¸å¿ƒè®¡ç®—å‡½æ•°</a></li><li><a href=#411-determine_num_available_blocks aria-label="4.1.1 determine_num_available_blocks()">4.1.1 determine_num_available_blocks()</a></li><li><a href=#412-get_cache_blockz_size aria-label="4.1.2 get_cache_blockZ_size()">4.1.2 get_cache_blockZ_size()</a></li><li><a href=#413-determine_num_available_blocks aria-label="4.1.3 determine_num_available_blocks()">4.1.3 determine_num_available_blocks()</a></li><li><a href=#414-_get_kv_cache_config_uniform_type aria-label="4.1.4 _get_kv_cache_config_uniform_type()">4.1.4 _get_kv_cache_config_uniform_type()</a></li></ul></li></ul></div></details></div><div class=post-content><blockquote><p>ğŸ’¡ ä¹‹å‰ä¸€ç›´ç”¨cursorè¾…åŠ©å†™ä»£ç ï¼Œçªç„¶è¯•äº†ä¸€ä¸‹å»åˆ©ç”¨cursoræ¥å­¦ä»£ç ï¼Œå‘ç°æ•ˆæœå˜å˜å¥½ã€‚è¿™é‡Œè®°å½•ä¸€ä¸‹æ¯”è¾ƒç«çš„vllmä¸­æå‡æ¨ç†é€Ÿåº¦çš„ä¸€äº›å…³é”®ä»£ç ã€‚</p></blockquote><h1 id=1-what-is-vllm>1. What is vLLMï¼Ÿ<a hidden class=anchor aria-hidden=true href=#1-what-is-vllm>#</a></h1><p><img loading=lazy src="https://prod-files-secure.s3.us-west-2.amazonaws.com/fc187c04-cf34-444f-b5f2-bdcdfad76660/ccc90792-56ef-4142-8b21-8ecb32787141/image.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB4667CHJANLH%2F20251208%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20251208T025210Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjENP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJHMEUCIQD2bCzZ0EVdZe7zBZDO9NXx8ugDbRVvAqtEWvMZGTYavQIgF2RgKaHUZN5ujmNCUZcBpJj%2FzCCDnE6E%2FTm648aqUlMqiAQInP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAAGgw2Mzc0MjMxODM4MDUiDEmUGYmpJzDFCA1vTCrcA9jM%2FI8uOAPBA%2FNgjk5D2LQftmbly%2B2RXHJiOGPHfUTT3vkrQN8cSM5zmFgWZAwIdY4l1hQROAWzlQc3jJxMYz%2FNziY181CkBpFwHjvK1gJsC5sPYuT8ID3Kdzjvb1kLf3Fm%2Bc7E1mwUPLlV4oC2TlySukD4gz%2Fuqf2JLQjGnNTCjyKDrufO7TN9XPyqCoO08SQaxQZha6jSEi2TuPWmpoP277vUR70do8og%2Flx8Dp0oIPEpXkHCnz7BPXOhPVBWSCnhlFWtpAWxXAE3KW9Vljt6295P5sUQkMQ8HB0Wqao36AhNSJtqSf0Fx3qsUbc5OX%2FdL%2BdVLwsPyVByiqg1QHuyryjn360%2BGHXsCitg2pw9C9aS6AUtMSILmZTHSfE%2Fwx%2B9IcLHDIHpZ%2BJjG3Z%2BtZxZY%2FP46biZytDDUVhLg1ja31M3%2F9qHMjskSqJaAEgenYnau0EcmwHIxOreCyzNiijqLKjqWP0ijP3LOUNEWHZqG%2BQtwq52m%2FWVpH%2B7QAGifpUZKh89vZk8zV006O8%2FZa2urM3KK7H0cHpw1WtKjlUVnMQThozNlIxhFLdqV6aRu7PC6zPgIjn%2BjIXAGKMQPI5BJtgK8oW8YWCielggsu%2BgC4vZ%2FTJFROvHLyN%2BMLbu2MkGOqUBdOc0Fjl6YjBuAE0GMEEJuEquTpLecIx2SwOpz%2FUU7gCQnZmuQ6ymeJNiFD8unMO6twi7wp1NcGwopHyY%2FSmyhD4EWo0OIP%2Ff8fsRjYIQ8sXa%2BcZT4gjSvdNHgxsJTXa15i8waJpZknL4zuDXQbtxu7xBapfYV1EabaXnYVTsrNZ9Uur%2BgY5ZBI0aExW8g0QYQWGbaCUsM8%2B8GEStT2tYCQAT4DY%2F&X-Amz-Signature=0f4ed5c583a45c8dc1280e1defff9d8c9f1c9a1903e236edc480e77fcf13baf5&X-Amz-SignedHeaders=host&x-amz-checksum-mode=ENABLED&x-id=GetObject"></p><p>vLLMæ˜¯å¤§æ¨¡å‹æ¨ç†åŠ é€Ÿæ¡†æ¶ã€‚ä¸»è¦åœ¨KVcacheç®¡ç†ä¸­é‡‡ç”¨äº†PageAttentionæŠ€æœ¯ï¼Œæ”¯æŒå¸‚é¢ä¸Šä¸»æµçš„å¼€æºå¤§æ¨¡å‹ï¼Œæ”¯æŒé‡åŒ–ç±»å‹ä¸ºï¼šGPTQ,Â AWQ, INT4, INT8, and FP8</p><hr><h1 id=2-kvcacheç›¸å…³çš„è®¡ç®—ä»£ç >2. KVcacheç›¸å…³çš„è®¡ç®—ä»£ç <a hidden class=anchor aria-hidden=true href=#2-kvcacheç›¸å…³çš„è®¡ç®—ä»£ç >#</a></h1><p>ä½¿ç”¨claude-3.7-sonnet thinkingï¼Œå¯¹vllmæ•´ä¸ªé¡¹ç›®è¿›è¡Œæé—®ã€‚</p><p><img loading=lazy src="https://prod-files-secure.s3.us-west-2.amazonaws.com/fc187c04-cf34-444f-b5f2-bdcdfad76660/cb54e376-a945-4f40-a5b9-ff4fc10e9fc2/image.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB4667CHJANLH%2F20251208%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20251208T025210Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjENP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJHMEUCIQD2bCzZ0EVdZe7zBZDO9NXx8ugDbRVvAqtEWvMZGTYavQIgF2RgKaHUZN5ujmNCUZcBpJj%2FzCCDnE6E%2FTm648aqUlMqiAQInP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAAGgw2Mzc0MjMxODM4MDUiDEmUGYmpJzDFCA1vTCrcA9jM%2FI8uOAPBA%2FNgjk5D2LQftmbly%2B2RXHJiOGPHfUTT3vkrQN8cSM5zmFgWZAwIdY4l1hQROAWzlQc3jJxMYz%2FNziY181CkBpFwHjvK1gJsC5sPYuT8ID3Kdzjvb1kLf3Fm%2Bc7E1mwUPLlV4oC2TlySukD4gz%2Fuqf2JLQjGnNTCjyKDrufO7TN9XPyqCoO08SQaxQZha6jSEi2TuPWmpoP277vUR70do8og%2Flx8Dp0oIPEpXkHCnz7BPXOhPVBWSCnhlFWtpAWxXAE3KW9Vljt6295P5sUQkMQ8HB0Wqao36AhNSJtqSf0Fx3qsUbc5OX%2FdL%2BdVLwsPyVByiqg1QHuyryjn360%2BGHXsCitg2pw9C9aS6AUtMSILmZTHSfE%2Fwx%2B9IcLHDIHpZ%2BJjG3Z%2BtZxZY%2FP46biZytDDUVhLg1ja31M3%2F9qHMjskSqJaAEgenYnau0EcmwHIxOreCyzNiijqLKjqWP0ijP3LOUNEWHZqG%2BQtwq52m%2FWVpH%2B7QAGifpUZKh89vZk8zV006O8%2FZa2urM3KK7H0cHpw1WtKjlUVnMQThozNlIxhFLdqV6aRu7PC6zPgIjn%2BjIXAGKMQPI5BJtgK8oW8YWCielggsu%2BgC4vZ%2FTJFROvHLyN%2BMLbu2MkGOqUBdOc0Fjl6YjBuAE0GMEEJuEquTpLecIx2SwOpz%2FUU7gCQnZmuQ6ymeJNiFD8unMO6twi7wp1NcGwopHyY%2FSmyhD4EWo0OIP%2Ff8fsRjYIQ8sXa%2BcZT4gjSvdNHgxsJTXa15i8waJpZknL4zuDXQbtxu7xBapfYV1EabaXnYVTsrNZ9Uur%2BgY5ZBI0aExW8g0QYQWGbaCUsM8%2B8GEStT2tYCQAT4DY%2F&X-Amz-Signature=370d6d89f21616b98f2ac0b8f79780e0f11c83ddee84e5380e0b7bacdf750c8c&X-Amz-SignedHeaders=host&x-amz-checksum-mode=ENABLED&x-id=GetObject"></p><p>ä¸ºäº†æ›´å¥½ä¸¾ä¾‹ï¼Œä¸‹è½½äº†deepseek-v3-0324ç‰ˆæœ¬çš„config.jsonï¼Œä»£å…¥é¡¹ç›®è¿›è¡Œæé—®ã€‚</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span><span class=lnt>61
</span><span class=lnt>62
</span><span class=lnt>63
</span><span class=lnt>64
</span><span class=lnt>65
</span><span class=lnt>66
</span><span class=lnt>67
</span><span class=lnt>68
</span><span class=lnt>69
</span><span class=lnt>70
</span><span class=lnt>71
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;architectures&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;DeepseekV3ForCausalLM&#34;</span>
</span></span><span class=line><span class=cl>  <span class=p>],</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;attention_bias&#34;</span><span class=p>:</span> <span class=kc>false</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;attention_dropout&#34;</span><span class=p>:</span> <span class=mf>0.0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;auto_map&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;AutoConfig&#34;</span><span class=p>:</span> <span class=s2>&#34;configuration_deepseek.DeepseekV3Config&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;AutoModel&#34;</span><span class=p>:</span> <span class=s2>&#34;modeling_deepseek.DeepseekV3Model&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;AutoModelForCausalLM&#34;</span><span class=p>:</span> <span class=s2>&#34;modeling_deepseek.DeepseekV3ForCausalLM&#34;</span>
</span></span><span class=line><span class=cl>  <span class=p>},</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;aux_loss_alpha&#34;</span><span class=p>:</span> <span class=mf>0.001</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;bos_token_id&#34;</span><span class=p>:</span> <span class=mi>0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;eos_token_id&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;ep_size&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;first_k_dense_replace&#34;</span><span class=p>:</span> <span class=mi>3</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;hidden_act&#34;</span><span class=p>:</span> <span class=s2>&#34;silu&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;hidden_size&#34;</span><span class=p>:</span> <span class=mi>7168</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;initializer_range&#34;</span><span class=p>:</span> <span class=mf>0.02</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;intermediate_size&#34;</span><span class=p>:</span> <span class=mi>18432</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;kv_lora_rank&#34;</span><span class=p>:</span> <span class=mi>512</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;max_position_embeddings&#34;</span><span class=p>:</span> <span class=mi>163840</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;model_type&#34;</span><span class=p>:</span> <span class=s2>&#34;deepseek_v3&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;moe_intermediate_size&#34;</span><span class=p>:</span> <span class=mi>2048</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;moe_layer_freq&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;n_group&#34;</span><span class=p>:</span> <span class=mi>8</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;n_routed_experts&#34;</span><span class=p>:</span> <span class=mi>256</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;n_shared_experts&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;norm_topk_prob&#34;</span><span class=p>:</span> <span class=kc>true</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;num_attention_heads&#34;</span><span class=p>:</span> <span class=mi>128</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;num_experts_per_tok&#34;</span><span class=p>:</span> <span class=mi>8</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;num_hidden_layers&#34;</span><span class=p>:</span> <span class=mi>61</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;num_key_value_heads&#34;</span><span class=p>:</span> <span class=mi>128</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;num_nextn_predict_layers&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;pretraining_tp&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span><span class=err>æ¨¡å‹ä¸­Transformerå±‚çš„æ•°é‡ã€‚æ›´å¤šçš„å±‚æ•°ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æ›´æ·±å±‚æ¬¡çš„ç‰¹å¾å’Œå…³ç³»ï¼Œä½†ä¹Ÿå¢åŠ äº†æ¨¡å‹çš„æ·±åº¦å’Œè®¡ç®—æˆæœ¬ã€‚</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;q_lora_rank&#34;</span><span class=p>:</span> <span class=mi>1536</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;qk_nope_head_dim&#34;</span><span class=p>:</span> <span class=mi>128</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;qk_rope_head_dim&#34;</span><span class=p>:</span> <span class=mi>64</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;quantization_config&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;activation_scheme&#34;</span><span class=p>:</span> <span class=s2>&#34;dynamic&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;fmt&#34;</span><span class=p>:</span> <span class=s2>&#34;e4m3&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;quant_method&#34;</span><span class=p>:</span> <span class=s2>&#34;fp8&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;weight_block_size&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>      <span class=mi>128</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=mi>128</span>
</span></span><span class=line><span class=cl>    <span class=p>]</span>
</span></span><span class=line><span class=cl>  <span class=p>},</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;rms_norm_eps&#34;</span><span class=p>:</span> <span class=mf>1e-06</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;rope_scaling&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;beta_fast&#34;</span><span class=p>:</span> <span class=mi>32</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;beta_slow&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;factor&#34;</span><span class=p>:</span> <span class=mi>40</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;mscale&#34;</span><span class=p>:</span> <span class=mf>1.0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;mscale_all_dim&#34;</span><span class=p>:</span> <span class=mf>1.0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;original_max_position_embeddings&#34;</span><span class=p>:</span> <span class=mi>4096</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;yarn&#34;</span>
</span></span><span class=line><span class=cl>  <span class=p>},</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;rope_theta&#34;</span><span class=p>:</span> <span class=mi>10000</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;routed_scaling_factor&#34;</span><span class=p>:</span> <span class=mf>2.5</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;scoring_func&#34;</span><span class=p>:</span> <span class=s2>&#34;sigmoid&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;seq_aux&#34;</span><span class=p>:</span> <span class=kc>true</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;tie_word_embeddings&#34;</span><span class=p>:</span> <span class=kc>false</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;topk_group&#34;</span><span class=p>:</span> <span class=mi>4</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;topk_method&#34;</span><span class=p>:</span> <span class=s2>&#34;noaux_tc&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;torch_dtype&#34;</span><span class=p>:</span> <span class=s2>&#34;bfloat16&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;transformers_version&#34;</span><span class=p>:</span> <span class=s2>&#34;4.46.3&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;use_cache&#34;</span><span class=p>:</span> <span class=kc>true</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;v_head_dim&#34;</span><span class=p>:</span> <span class=mi>128</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;vocab_size&#34;</span><span class=p>:</span> <span class=mi>129280</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>è¯¥æ–‡ä»¶ä¸­çš„å…³é”®ä¿¡æ¯ä¸ºï¼š</p><h2 id=21-deepseek-v1çš„kvcacheæ˜¯å¦‚ä½•è®¡ç®—çš„>2.1 DeepSeek-V1çš„KVcacheæ˜¯å¦‚ä½•è®¡ç®—çš„ï¼Ÿ<a hidden class=anchor aria-hidden=true href=#21-deepseek-v1çš„kvcacheæ˜¯å¦‚ä½•è®¡ç®—çš„>#</a></h2><h3 id=211-æ¨¡å‹å‚æ•°åˆ†æ>2.1.1 æ¨¡å‹å‚æ•°åˆ†æ<a hidden class=anchor aria-hidden=true href=#211-æ¨¡å‹å‚æ•°åˆ†æ>#</a></h3><p>ä»æä¾›çš„config.jsonæ–‡ä»¶ä¸­æå–çš„å…³é”®å‚æ•°ï¼š</p><ul><li>hidden_size: 7168</li><li>num_attention_heads: 128Â (Qå¤´æ•°é‡)</li><li>num_key_value_heads: 128Â (KVå¤´æ•°é‡ï¼Œæ— MHAç»“æ„)</li><li>num_hidden_layers: 61Â (Transformerå±‚æ•°)</li><li>max_position_embeddings: 163840 (æœ€å¤§ä½ç½®åµŒå…¥ï¼Œè¡¨ç¤ºæ”¯æŒçš„æœ€å¤§åºåˆ—é•¿åº¦)</li><li>torch_dtype: &ldquo;bfloat16&rdquo; (æ¨¡å‹å‚æ•°æ•°æ®ç±»å‹)</li></ul><hr><h3 id=212-kvç¼“å­˜æ˜¾å­˜è®¡ç®—>2.1.2 KVç¼“å­˜æ˜¾å­˜è®¡ç®—<a hidden class=anchor aria-hidden=true href=#212-kvç¼“å­˜æ˜¾å­˜è®¡ç®—>#</a></h3><p>åŸºç¡€å‚æ•°è®¡ç®—</p><ul><li><p>head_size = hidden_size / num_attention_heads = 7168 / 128 = 56</p></li><li><p>æ•°æ®ç±»å‹å¤§å° = bfloat16 = 2å­—èŠ‚
å•ä¸ªæ³¨æ„åŠ›å±‚çš„KVcacheè®¡ç®—</p></li><li><p>key cache size = num_key_value_heads * head_size = 128 * 56 = 7168</p></li><li><p>value cache size = key cache size = 7168</p></li><li><p>å•å±‚å•tokençš„KVç¼“å­˜å¤§å° = (keyç¼“å­˜é¡¹ + valueç¼“å­˜é¡¹) * dtype = (7168 + 7168) * 2 = 28672 â‰ˆ 28kb
å…¨æ¨¡å‹æœ€å¤§KVç¼“å­˜è®¡ç®—</p></li><li><p>å…¨æ¨¡å‹å•tokençš„KVç¼“å­˜å¤§å° = å•å±‚tokençš„KVç¼“å­˜ * æ¨¡å‹å±‚æ•° = 28kb * 61 = 1708kb â‰ˆ 1.67MB</p></li><li><p>æœ€å¤§åºåˆ—é•¿åº¦ä¸‹çš„KVç¼“å­˜å¤§å° = å…¨æ¨¡å‹å•tokençš„KVç¼“å­˜å¤§å° * æœ€å¤§åºåˆ—é•¿åº¦ = 273612.8MB â‰ˆ 267.2GB
æŒ‰å—åˆ†é…è®¡ç®—</p></li></ul><p>å¦‚æœä½¿ç”¨vLLMçš„åˆ†å—æœºåˆ¶ï¼Œå‡è®¾block_size=16ï¼š</p><ul><li>æ¯ä¸ªå—å¤§å° = æ•°æ®ç±»å‹å¤§å° * æ³¨æ„åŠ›å±‚æ•° * block_size * (key_cache_entry + value_cache_entry) = 2 * 61 * 16 * (7168+7168) â‰ˆ 27.9MB</li><li>éœ€è¦å—çš„æ•°é‡ = æœ€å¤§åºåˆ—é•¿åº¦ / block_size = 163840 / 16 = 10240å—</li><li>æ€»æ˜¾å­˜ = æ¯ä¸ªå—å¤§å° * éœ€è¦çš„å—æ•°é‡ = 27.9MB * 10240 â‰ˆ 286GB
å®é™…åº”ç”¨è€ƒè™‘</li></ul><ol><li>å®é™…ä½¿ç”¨æ—¶å¯èƒ½ä¸ä¼šç”¨æœ€å¤§çš„åºåˆ—é•¿åº¦ï¼Œå¸¸è§çš„è®¾ç½®ä¸º4K - 32K</li><li>ä¸åŒçš„block_sizeçš„é€‰æ‹©ä¼šå½±å“æ˜¾å­˜ä½¿ç”¨æ•ˆç‡
ä¾‹å¦‚ï¼Œå¦‚æœä½¿ç”¨8Kçš„åºåˆ—é•¿åº¦:</li></ol><ul><li>KVç¼“å­˜ = 1.67MB * 8192 â‰ˆ 13.7GB</li></ul><hr><h3 id=213-è®¡ç®—kvcacheä»£ç çš„ä½ç½®>2.1.3 è®¡ç®—KVcacheä»£ç çš„ä½ç½®<a hidden class=anchor aria-hidden=true href=#213-è®¡ç®—kvcacheä»£ç çš„ä½ç½®>#</a></h3><p>åœ¨0.7.2ç‰ˆæœ¬ä¸­ï¼Œvllm\worker\worker.py</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span><span class=lnt>61
</span><span class=lnt>62
</span><span class=lnt>63
</span><span class=lnt>64
</span><span class=lnt>65
</span><span class=lnt>66
</span><span class=lnt>67
</span><span class=lnt>68
</span><span class=lnt>69
</span><span class=lnt>70
</span><span class=lnt>71
</span><span class=lnt>72
</span><span class=lnt>73
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nd>@torch.inference_mode</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>determine_num_available_blocks</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tuple</span><span class=p>[</span><span class=nb>int</span><span class=p>,</span> <span class=nb>int</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;ç¡®å®šå¯ç”¨çš„KVç¼“å­˜å—æ•°é‡ã€‚
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        è¯¥æ–¹æ³•ä¼šé¦–å…ˆå¯¹ç°æœ‰å†…å­˜ä½¿ç”¨æƒ…å†µè¿›è¡Œåˆ†æ,ç„¶åè®¡ç®—åœ¨å‰©ä½™å¯ç”¨å†…å­˜ä¸‹å¯ä»¥åˆ†é…çš„æœ€å¤§GPUå’ŒCPUå—æ•°é‡ã€‚
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        ä¸»è¦æ­¥éª¤:
</span></span></span><span class=line><span class=cl><span class=s2>        1. æ¸…ç©ºGPUç¼“å­˜å¹¶é‡ç½®å†…å­˜ç»Ÿè®¡ä¿¡æ¯
</span></span></span><span class=line><span class=cl><span class=s2>        2. æ‰§è¡Œä¸€æ¬¡å‰å‘ä¼ æ’­æ¥åˆ†ææ¨¡å‹çš„å†…å­˜ä½¿ç”¨æƒ…å†µ
</span></span></span><span class=line><span class=cl><span class=s2>        3. æ ¹æ®GPUå†…å­˜åˆ©ç”¨ç‡å’Œå¯ç”¨å†…å­˜è®¡ç®—å¯åˆ†é…çš„KVç¼“å­˜å—æ•°é‡
</span></span></span><span class=line><span class=cl><span class=s2>        4. è®°å½•è¯¦ç»†çš„å†…å­˜ä½¿ç”¨æƒ…å†µ
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        Returns:
</span></span></span><span class=line><span class=cl><span class=s2>            Tuple[int, int]: è¿”å›(GPUå—æ•°é‡, CPUå—æ•°é‡)çš„å…ƒç»„
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=c1># æ¸…ç©ºGPUç¼“å­˜å¹¶é‡ç½®å†…å­˜ç»Ÿè®¡ä¿¡æ¯</span>
</span></span><span class=line><span class=cl>        <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>empty_cache</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>reset_peak_memory_stats</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># è·å–å½“å‰ç©ºé—²å†…å­˜å’Œæ€»GPUå†…å­˜</span>
</span></span><span class=line><span class=cl>        <span class=n>free_memory_pre_profile</span><span class=p>,</span> <span class=n>total_gpu_memory</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>mem_get_info</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># ä½¿ç”¨è™šæ‹Ÿè¾“å…¥æ‰§è¡Œä¸€æ¬¡å‰å‘ä¼ æ’­,åˆ†ææ¨¡å‹å†…å­˜ä½¿ç”¨æƒ…å†µ</span>
</span></span><span class=line><span class=cl>        <span class=k>with</span> <span class=n>memory_profiling</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=bp>self</span><span class=o>.</span><span class=n>baseline_snapshot</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>weights_memory</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>model_runner</span><span class=o>.</span><span class=n>model_memory_usage</span><span class=p>)</span> <span class=k>as</span> <span class=n>result</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>model_runner</span><span class=o>.</span><span class=n>profile_run</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># éªŒè¯å†…å­˜å ç”¨æ˜¯å¦å¢åŠ </span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_assert_memory_footprint_increased_during_profiling</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># è®¡ç®—å½“å‰å®ä¾‹å¯ç”¨çš„æ€»å†…å­˜</span>
</span></span><span class=line><span class=cl>        <span class=n>memory_for_current_instance</span> <span class=o>=</span> <span class=n>total_gpu_memory</span> <span class=o>*</span> \
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>cache_config</span><span class=o>.</span><span class=n>gpu_memory_utilization</span>
</span></span><span class=line><span class=cl>        <span class=c1># è®¡ç®—å¯ç”¨äºKVç¼“å­˜çš„å†…å­˜</span>
</span></span><span class=line><span class=cl>        <span class=n>available_kv_cache_memory</span> <span class=o>=</span> <span class=p>(</span><span class=n>memory_for_current_instance</span> <span class=o>-</span>
</span></span><span class=line><span class=cl>                                     <span class=n>result</span><span class=o>.</span><span class=n>non_kv_cache_memory</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># è®¡ç®—å¯åˆ†é…çš„å—æ•°é‡</span>
</span></span><span class=line><span class=cl>        <span class=n>cache_block_size</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>get_cache_block_size_bytes</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>cache_block_size</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>num_gpu_blocks</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>            <span class=n>num_cpu_blocks</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># æ ¹æ®å¯ç”¨å†…å­˜è®¡ç®—GPUå’ŒCPUå—æ•°é‡</span>
</span></span><span class=line><span class=cl>            <span class=n>num_gpu_blocks</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=n>available_kv_cache_memory</span> <span class=o>//</span> <span class=n>cache_block_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>num_cpu_blocks</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>cache_config</span><span class=o>.</span><span class=n>swap_space_bytes</span> <span class=o>//</span>
</span></span><span class=line><span class=cl>                                 <span class=n>cache_block_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>num_gpu_blocks</span> <span class=o>=</span> <span class=nb>max</span><span class=p>(</span><span class=n>num_gpu_blocks</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>num_cpu_blocks</span> <span class=o>=</span> <span class=nb>max</span><span class=p>(</span><span class=n>num_cpu_blocks</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># æ„å»ºè¯¦ç»†çš„å†…å­˜ä½¿ç”¨æƒ…å†µæ—¥å¿—</span>
</span></span><span class=line><span class=cl>        <span class=n>msg</span> <span class=o>=</span> <span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Memory profiling takes </span><span class=si>{</span><span class=n>result</span><span class=o>.</span><span class=n>profile_time</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2> seconds</span><span class=se>\n</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>               <span class=s2>&#34;the current vLLM instance can use &#34;</span>
</span></span><span class=line><span class=cl>               <span class=s2>&#34;total_gpu_memory &#34;</span>
</span></span><span class=line><span class=cl>               <span class=sa>f</span><span class=s2>&#34;(</span><span class=si>{</span><span class=p>(</span><span class=n>total_gpu_memory</span> <span class=o>/</span> <span class=n>GiB_bytes</span><span class=p>)</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>GiB)&#34;</span>
</span></span><span class=line><span class=cl>               <span class=s2>&#34; x gpu_memory_utilization &#34;</span>
</span></span><span class=line><span class=cl>               <span class=sa>f</span><span class=s2>&#34;(</span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>cache_config</span><span class=o>.</span><span class=n>gpu_memory_utilization</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>)&#34;</span>
</span></span><span class=line><span class=cl>               <span class=sa>f</span><span class=s2>&#34; = </span><span class=si>{</span><span class=p>(</span><span class=n>memory_for_current_instance</span> <span class=o>/</span> <span class=n>GiB_bytes</span><span class=p>)</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>GiB</span><span class=se>\n</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>               <span class=s2>&#34;model weights take &#34;</span>
</span></span><span class=line><span class=cl>               <span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=p>(</span><span class=n>result</span><span class=o>.</span><span class=n>weights_memory</span> <span class=o>/</span> <span class=n>GiB_bytes</span><span class=p>)</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>GiB;&#34;</span>
</span></span><span class=line><span class=cl>               <span class=s2>&#34; non_torch_memory takes &#34;</span>
</span></span><span class=line><span class=cl>               <span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=p>(</span><span class=n>result</span><span class=o>.</span><span class=n>non_torch_increase</span> <span class=o>/</span> <span class=n>GiB_bytes</span><span class=p>)</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>GiB;&#34;</span>
</span></span><span class=line><span class=cl>               <span class=s2>&#34; PyTorch activation peak memory takes &#34;</span>
</span></span><span class=line><span class=cl>               <span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=p>(</span><span class=n>result</span><span class=o>.</span><span class=n>torch_peak_increase</span> <span class=o>/</span> <span class=n>GiB_bytes</span><span class=p>)</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>GiB;&#34;</span>
</span></span><span class=line><span class=cl>               <span class=s2>&#34; the rest of the memory reserved for KV Cache is &#34;</span>
</span></span><span class=line><span class=cl>               <span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=p>(</span><span class=n>available_kv_cache_memory</span> <span class=o>/</span> <span class=n>GiB_bytes</span><span class=p>)</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>GiB.&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=n>msg</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># Final cleanup</span>
</span></span><span class=line><span class=cl>        <span class=n>gc</span><span class=o>.</span><span class=n>collect</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>num_gpu_blocks</span><span class=p>,</span> <span class=n>num_cpu_blocks</span>
</span></span></code></pre></td></tr></table></div></div><hr><h2 id=22-åœ¨vllmä¸­å®ç°pageattentionçš„ä»£ç åœ¨å“ªé‡Œ>2.2 åœ¨vLLMä¸­å®ç°PageAttentionçš„ä»£ç åœ¨å“ªé‡Œï¼Ÿ<a hidden class=anchor aria-hidden=true href=#22-åœ¨vllmä¸­å®ç°pageattentionçš„ä»£ç åœ¨å“ªé‡Œ>#</a></h2><p>å…³äºPageAttentionæŠ€æœ¯çš„å®ç°ï¼Œå®ƒä¸»è¦åœ¨ä»¥ä¸‹æ–‡ä»¶ä¸­ï¼š</p><ol><li>vllm/attention/ops/paged_attn.pyï¼šåŒ…å«PagedAttentionç±»ï¼Œæä¾›KVç¼“å­˜å½¢çŠ¶ã€åˆ†å‰²å’Œå†™å…¥ç­‰æ“ä½œçš„æ¥å£</li><li>vllm/_custom_ops.pyï¼šåŒ…å«paged_attention_v1å‡½æ•°ï¼Œè¿æ¥åˆ°C++åº•å±‚å®ç°</li><li>csrc/attention/attention_kernels.cuï¼šåŒ…å«PageAttentionçš„CUDAå†…æ ¸å®ç°</li><li>docs/source/design/kernel/paged_attention.mdï¼šè¯¦ç»†è§£é‡Šäº†PageAttentionçš„è®¾è®¡å’Œå®ç°åŸç†
PageAttentionæ˜¯vLLMçš„æ ¸å¿ƒæŠ€æœ¯ï¼Œé€šè¿‡å°†KVç¼“å­˜å­˜å‚¨åœ¨ä¸è¿ç»­çš„å†…å­˜å—ä¸­ï¼Œå®ç°äº†é«˜æ•ˆçš„å†…å­˜ç®¡ç†ã€‚è¿™äº›å†…å­˜å—å¯ä»¥æ ¹æ®éœ€è¦åŠ¨æ€åˆ†é…å’Œé‡Šæ”¾ï¼Œä½¿å¾—å•ä¸ªGPUèƒ½å¤ŸæœåŠ¡æ›´å¤šå¹¶å‘è¯·æ±‚ã€‚è¯¥æŠ€æœ¯çš„å…³é”®ç‰¹ç‚¹æ˜¯é€šè¿‡åˆ†é¡µæœºåˆ¶é¿å…äº†æ˜¾å­˜ç¢ç‰‡ï¼Œå¹¶æ”¯æŒé«˜æ•ˆçš„ä¸Šä¸‹æ–‡å¤„ç†ã€‚</li></ol><hr><h1 id=3-éªŒè¯>3. éªŒè¯<a hidden class=anchor aria-hidden=true href=#3-éªŒè¯>#</a></h1><p>ä½¿ç”¨4090DåŸºäºvllmå¯ç”¨é»˜è®¤å‚æ•°è¿›è¡Œæ¨ç†ã€‚æŸ¥çœ‹å¯åŠ¨æœåŠ¡çš„æ—¥å¿—å†…å®¹ã€‚</p><h2 id=31-æ¨¡å‹æ—¥å¿—>3.1 æ¨¡å‹æ—¥å¿—<a hidden class=anchor aria-hidden=true href=#31-æ¨¡å‹æ—¥å¿—>#</a></h2><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span><span class=lnt>61
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>INFO 04-14 06:00:50 <span class=o>[</span>__init__.py:239<span class=o>]</span> Automatically detected platform cuda.
</span></span><span class=line><span class=cl>INFO 04-14 06:00:52 <span class=o>[</span>api_server.py:1034<span class=o>]</span> vLLM API server version 0.8.3
</span></span><span class=line><span class=cl>INFO 04-14 06:00:52 <span class=o>[</span>api_server.py:1035<span class=o>]</span> args: Namespace<span class=o>(</span><span class=nv>subparser</span><span class=o>=</span><span class=s1>&#39;serve&#39;</span>, <span class=nv>model_tag</span><span class=o>=</span><span class=s1>&#39;/data/DeepSeek-R1-Distill-Qwen-1.5B&#39;</span>, <span class=nv>config</span><span class=o>=</span><span class=s1>&#39;&#39;</span>, <span class=nv>host</span><span class=o>=</span>None, <span class=nv>port</span><span class=o>=</span>8000, <span class=nv>uvicorn_log_level</span><span class=o>=</span><span class=s1>&#39;info&#39;</span>, <span class=nv>disable_uvicorn_access_log</span><span class=o>=</span>False, <span class=nv>allow_credentials</span><span class=o>=</span>False, <span class=nv>allowed_origins</span><span class=o>=[</span><span class=s1>&#39;*&#39;</span><span class=o>]</span>, <span class=nv>allowed_methods</span><span class=o>=[</span><span class=s1>&#39;*&#39;</span><span class=o>]</span>, <span class=nv>allowed_headers</span><span class=o>=[</span><span class=s1>&#39;*&#39;</span><span class=o>]</span>, <span class=nv>api_key</span><span class=o>=</span>None, <span class=nv>lora_modules</span><span class=o>=</span>None, <span class=nv>prompt_adapters</span><span class=o>=</span>None, <span class=nv>chat_template</span><span class=o>=</span>None, <span class=nv>chat_template_content_format</span><span class=o>=</span><span class=s1>&#39;auto&#39;</span>, <span class=nv>response_role</span><span class=o>=</span><span class=s1>&#39;assistant&#39;</span>, <span class=nv>ssl_keyfile</span><span class=o>=</span>None, <span class=nv>ssl_certfile</span><span class=o>=</span>None, <span class=nv>ssl_ca_certs</span><span class=o>=</span>None, <span class=nv>enable_ssl_refresh</span><span class=o>=</span>False, <span class=nv>ssl_cert_reqs</span><span class=o>=</span>0, <span class=nv>root_path</span><span class=o>=</span>None, <span class=nv>middleware</span><span class=o>=[]</span>, <span class=nv>return_tokens_as_token_ids</span><span class=o>=</span>False, <span class=nv>disable_frontend_multiprocessing</span><span class=o>=</span>False, <span class=nv>enable_request_id_headers</span><span class=o>=</span>False, <span class=nv>enable_auto_tool_choice</span><span class=o>=</span>False, <span class=nv>tool_call_parser</span><span class=o>=</span>None, <span class=nv>tool_parser_plugin</span><span class=o>=</span><span class=s1>&#39;&#39;</span>, <span class=nv>model</span><span class=o>=</span><span class=s1>&#39;/data/DeepSeek-R1-Distill-Qwen-1.5B&#39;</span>, <span class=nv>task</span><span class=o>=</span><span class=s1>&#39;auto&#39;</span>, <span class=nv>tokenizer</span><span class=o>=</span>None, <span class=nv>hf_config_path</span><span class=o>=</span>None, <span class=nv>skip_tokenizer_init</span><span class=o>=</span>False, <span class=nv>revision</span><span class=o>=</span>None, <span class=nv>code_revision</span><span class=o>=</span>None, <span class=nv>tokenizer_revision</span><span class=o>=</span>None, <span class=nv>tokenizer_mode</span><span class=o>=</span><span class=s1>&#39;auto&#39;</span>, <span class=nv>trust_remote_code</span><span class=o>=</span>False, <span class=nv>allowed_local_media_path</span><span class=o>=</span>None, <span class=nv>download_dir</span><span class=o>=</span>None, <span class=nv>load_format</span><span class=o>=</span><span class=s1>&#39;auto&#39;</span>, <span class=nv>config_format</span><span class=o>=</span>&lt;ConfigFormat.AUTO: <span class=s1>&#39;auto&#39;</span>&gt;, <span class=nv>dtype</span><span class=o>=</span><span class=s1>&#39;auto&#39;</span>, <span class=nv>kv_cache_dtype</span><span class=o>=</span><span class=s1>&#39;auto&#39;</span>, <span class=nv>max_model_len</span><span class=o>=</span>None, <span class=nv>guided_decoding_backend</span><span class=o>=</span><span class=s1>&#39;xgrammar&#39;</span>, <span class=nv>logits_processor_pattern</span><span class=o>=</span>None, <span class=nv>model_impl</span><span class=o>=</span><span class=s1>&#39;auto&#39;</span>, <span class=nv>distributed_executor_backend</span><span class=o>=</span>None, <span class=nv>pipeline_parallel_size</span><span class=o>=</span>1, <span class=nv>tensor_parallel_size</span><span class=o>=</span>1, <span class=nv>data_parallel_size</span><span class=o>=</span>1, <span class=nv>enable_expert_parallel</span><span class=o>=</span>False, <span class=nv>max_parallel_loading_workers</span><span class=o>=</span>None, <span class=nv>ray_workers_use_nsight</span><span class=o>=</span>False, <span class=nv>block_size</span><span class=o>=</span>None, <span class=nv>enable_prefix_caching</span><span class=o>=</span>None, <span class=nv>prefix_caching_hash_algo</span><span class=o>=</span><span class=s1>&#39;builtin&#39;</span>, <span class=nv>disable_sliding_window</span><span class=o>=</span>False, <span class=nv>use_v2_block_manager</span><span class=o>=</span>True, <span class=nv>num_lookahead_slots</span><span class=o>=</span>0, <span class=nv>seed</span><span class=o>=</span>None, <span class=nv>swap_space</span><span class=o>=</span>4, <span class=nv>cpu_offload_gb</span><span class=o>=</span>0, <span class=nv>gpu_memory_utilization</span><span class=o>=</span>0.9, <span class=nv>num_gpu_blocks_override</span><span class=o>=</span>None, <span class=nv>max_num_batched_tokens</span><span class=o>=</span>None, <span class=nv>max_num_partial_prefills</span><span class=o>=</span>1, <span class=nv>max_long_partial_prefills</span><span class=o>=</span>1, <span class=nv>long_prefill_token_threshold</span><span class=o>=</span>0, <span class=nv>max_num_seqs</span><span class=o>=</span>None, <span class=nv>max_logprobs</span><span class=o>=</span>20, <span class=nv>disable_log_stats</span><span class=o>=</span>False, <span class=nv>quantization</span><span class=o>=</span>None, <span class=nv>rope_scaling</span><span class=o>=</span>None, <span class=nv>rope_theta</span><span class=o>=</span>None, <span class=nv>hf_overrides</span><span class=o>=</span>None, <span class=nv>enforce_eager</span><span class=o>=</span>False, <span class=nv>max_seq_len_to_capture</span><span class=o>=</span>8192, <span class=nv>disable_custom_all_reduce</span><span class=o>=</span>False, <span class=nv>tokenizer_pool_size</span><span class=o>=</span>0, <span class=nv>tokenizer_pool_type</span><span class=o>=</span><span class=s1>&#39;ray&#39;</span>, <span class=nv>tokenizer_pool_extra_config</span><span class=o>=</span>None, <span class=nv>limit_mm_per_prompt</span><span class=o>=</span>None, <span class=nv>mm_processor_kwargs</span><span class=o>=</span>None, <span class=nv>disable_mm_preprocessor_cache</span><span class=o>=</span>False, <span class=nv>enable_lora</span><span class=o>=</span>False, <span class=nv>enable_lora_bias</span><span class=o>=</span>False, <span class=nv>max_loras</span><span class=o>=</span>1, <span class=nv>max_lora_rank</span><span class=o>=</span>16, <span class=nv>lora_extra_vocab_size</span><span class=o>=</span>256, <span class=nv>lora_dtype</span><span class=o>=</span><span class=s1>&#39;auto&#39;</span>, <span class=nv>long_lora_scaling_factors</span><span class=o>=</span>None, <span class=nv>max_cpu_loras</span><span class=o>=</span>None, <span class=nv>fully_sharded_loras</span><span class=o>=</span>False, <span class=nv>enable_prompt_adapter</span><span class=o>=</span>False, <span class=nv>max_prompt_adapters</span><span class=o>=</span>1, <span class=nv>max_prompt_adapter_token</span><span class=o>=</span>0, <span class=nv>device</span><span class=o>=</span><span class=s1>&#39;auto&#39;</span>, <span class=nv>num_scheduler_steps</span><span class=o>=</span>1, <span class=nv>use_tqdm_on_load</span><span class=o>=</span>True, <span class=nv>multi_step_stream_outputs</span><span class=o>=</span>True, <span class=nv>scheduler_delay_factor</span><span class=o>=</span>0.0, <span class=nv>enable_chunked_prefill</span><span class=o>=</span>None, <span class=nv>speculative_config</span><span class=o>=</span>None, <span class=nv>model_loader_extra_config</span><span class=o>=</span>None, <span class=nv>ignore_patterns</span><span class=o>=[]</span>, <span class=nv>preemption_mode</span><span class=o>=</span>None, <span class=nv>served_model_name</span><span class=o>=</span>None, <span class=nv>qlora_adapter_name_or_path</span><span class=o>=</span>None, <span class=nv>show_hidden_metrics_for_version</span><span class=o>=</span>None, <span class=nv>otlp_traces_endpoint</span><span class=o>=</span>None, <span class=nv>collect_detailed_traces</span><span class=o>=</span>None, <span class=nv>disable_async_output_proc</span><span class=o>=</span>False, <span class=nv>scheduling_policy</span><span class=o>=</span><span class=s1>&#39;fcfs&#39;</span>, <span class=nv>scheduler_cls</span><span class=o>=</span><span class=s1>&#39;vllm.core.scheduler.Scheduler&#39;</span>, <span class=nv>override_neuron_config</span><span class=o>=</span>None, <span class=nv>override_pooler_config</span><span class=o>=</span>None, <span class=nv>compilation_config</span><span class=o>=</span>None, <span class=nv>kv_transfer_config</span><span class=o>=</span>None, <span class=nv>worker_cls</span><span class=o>=</span><span class=s1>&#39;auto&#39;</span>, <span class=nv>worker_extension_cls</span><span class=o>=</span><span class=s1>&#39;&#39;</span>, <span class=nv>generation_config</span><span class=o>=</span><span class=s1>&#39;auto&#39;</span>, <span class=nv>override_generation_config</span><span class=o>=</span>None, <span class=nv>enable_sleep_mode</span><span class=o>=</span>False, <span class=nv>calculate_kv_scales</span><span class=o>=</span>False, <span class=nv>additional_config</span><span class=o>=</span>None, <span class=nv>enable_reasoning</span><span class=o>=</span>False, <span class=nv>reasoning_parser</span><span class=o>=</span>None, <span class=nv>disable_cascade_attn</span><span class=o>=</span>False, <span class=nv>disable_log_requests</span><span class=o>=</span>False, <span class=nv>max_log_len</span><span class=o>=</span>None, <span class=nv>disable_fastapi_docs</span><span class=o>=</span>False, <span class=nv>enable_prompt_tokens_details</span><span class=o>=</span>False, <span class=nv>enable_server_load_tracking</span><span class=o>=</span>False, <span class=nv>dispatch_function</span><span class=o>=</span>&lt;<span class=k>function</span> ServeSubcommand.cmd at 0x7f883404a7a0&gt;<span class=o>)</span>
</span></span><span class=line><span class=cl>INFO 04-14 06:01:01 <span class=o>[</span>config.py:600<span class=o>]</span> This model supports multiple tasks: <span class=o>{</span><span class=s1>&#39;score&#39;</span>, <span class=s1>&#39;generate&#39;</span>, <span class=s1>&#39;classify&#39;</span>, <span class=s1>&#39;embed&#39;</span>, <span class=s1>&#39;reward&#39;</span><span class=o>}</span>. Defaulting to <span class=s1>&#39;generate&#39;</span>.
</span></span><span class=line><span class=cl>INFO 04-14 06:01:01 <span class=o>[</span>config.py:1780<span class=o>]</span> Chunked prefill is enabled with <span class=nv>max_num_batched_tokens</span><span class=o>=</span>2048.
</span></span><span class=line><span class=cl>INFO 04-14 06:01:08 <span class=o>[</span>__init__.py:239<span class=o>]</span> Automatically detected platform cuda.
</span></span><span class=line><span class=cl>INFO 04-14 06:01:11 <span class=o>[</span>core.py:61<span class=o>]</span> Initializing a V1 LLM engine <span class=o>(</span>v0.8.3<span class=o>)</span> with config: <span class=nv>model</span><span class=o>=</span><span class=s1>&#39;/data/DeepSeek-R1-Distill-Qwen-1.5B&#39;</span>, <span class=nv>speculative_config</span><span class=o>=</span>None, <span class=nv>tokenizer</span><span class=o>=</span><span class=s1>&#39;/data/DeepSeek-R1-Distill-Qwen-1.5B&#39;</span>, <span class=nv>skip_tokenizer_init</span><span class=o>=</span>False, <span class=nv>tokenizer_mode</span><span class=o>=</span>auto, <span class=nv>revision</span><span class=o>=</span>None, <span class=nv>override_neuron_config</span><span class=o>=</span>None, <span class=nv>tokenizer_revision</span><span class=o>=</span>None, <span class=nv>trust_remote_code</span><span class=o>=</span>False, <span class=nv>dtype</span><span class=o>=</span>torch.bfloat16, <span class=nv>max_seq_len</span><span class=o>=</span>131072, <span class=nv>download_dir</span><span class=o>=</span>None, <span class=nv>load_format</span><span class=o>=</span>auto, <span class=nv>tensor_parallel_size</span><span class=o>=</span>1, <span class=nv>pipeline_parallel_size</span><span class=o>=</span>1, <span class=nv>disable_custom_all_reduce</span><span class=o>=</span>False, <span class=nv>quantization</span><span class=o>=</span>None, <span class=nv>enforce_eager</span><span class=o>=</span>False, <span class=nv>kv_cache_dtype</span><span class=o>=</span>auto,  <span class=nv>device_config</span><span class=o>=</span>cuda, <span class=nv>decoding_config</span><span class=o>=</span>DecodingConfig<span class=o>(</span><span class=nv>guided_decoding_backend</span><span class=o>=</span><span class=s1>&#39;xgrammar&#39;</span>, <span class=nv>reasoning_backend</span><span class=o>=</span>None<span class=o>)</span>, <span class=nv>observability_config</span><span class=o>=</span>ObservabilityConfig<span class=o>(</span><span class=nv>show_hidden_metrics</span><span class=o>=</span>False, <span class=nv>otlp_traces_endpoint</span><span class=o>=</span>None, <span class=nv>collect_model_forward_time</span><span class=o>=</span>False, <span class=nv>collect_model_execute_time</span><span class=o>=</span>False<span class=o>)</span>, <span class=nv>seed</span><span class=o>=</span>None, <span class=nv>served_model_name</span><span class=o>=</span>/data/DeepSeek-R1-Distill-Qwen-1.5B, <span class=nv>num_scheduler_steps</span><span class=o>=</span>1, <span class=nv>multi_step_stream_outputs</span><span class=o>=</span>True, <span class=nv>enable_prefix_caching</span><span class=o>=</span>True, <span class=nv>chunked_prefill_enabled</span><span class=o>=</span>True, <span class=nv>use_async_output_proc</span><span class=o>=</span>True, <span class=nv>disable_mm_preprocessor_cache</span><span class=o>=</span>False, <span class=nv>mm_processor_kwargs</span><span class=o>=</span>None, <span class=nv>pooler_config</span><span class=o>=</span>None, <span class=nv>compilation_config</span><span class=o>={</span><span class=s2>&#34;level&#34;</span>:3,<span class=s2>&#34;custom_ops&#34;</span>:<span class=o>[</span><span class=s2>&#34;none&#34;</span><span class=o>]</span>,<span class=s2>&#34;splitting_ops&#34;</span>:<span class=o>[</span><span class=s2>&#34;vllm.unified_attention&#34;</span>,<span class=s2>&#34;vllm.unified_attention_with_output&#34;</span><span class=o>]</span>,<span class=s2>&#34;use_inductor&#34;</span>:true,<span class=s2>&#34;compile_sizes&#34;</span>:<span class=o>[]</span>,<span class=s2>&#34;use_cudagraph&#34;</span>:true,<span class=s2>&#34;cudagraph_num_of_warmups&#34;</span>:1,<span class=s2>&#34;cudagraph_capture_sizes&#34;</span>:<span class=o>[</span>512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1<span class=o>]</span>,<span class=s2>&#34;max_capture_size&#34;</span>:512<span class=o>}</span>
</span></span><span class=line><span class=cl>WARNING 04-14 06:01:11 <span class=o>[</span>utils.py:2413<span class=o>]</span> Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in &lt;vllm.v1.worker.gpu_worker.Worker object at 0x7fd287cd4670&gt;
</span></span><span class=line><span class=cl>INFO 04-14 06:01:12 <span class=o>[</span>parallel_state.py:957<span class=o>]</span> rank <span class=m>0</span> in world size <span class=m>1</span> is assigned as DP rank 0, PP rank 0, TP rank <span class=m>0</span>
</span></span><span class=line><span class=cl>INFO 04-14 06:01:12 <span class=o>[</span>cuda.py:221<span class=o>]</span> Using Flash Attention backend on V1 engine.
</span></span><span class=line><span class=cl>INFO 04-14 06:01:12 <span class=o>[</span>gpu_model_runner.py:1258<span class=o>]</span> Starting to load model /data/DeepSeek-R1-Distill-Qwen-1.5B...
</span></span><span class=line><span class=cl>WARNING 04-14 06:01:12 <span class=o>[</span>topk_topp_sampler.py:69<span class=o>]</span> FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p <span class=p>&amp;</span> top-k sampling. For the best performance, please install FlashInfer.
</span></span><span class=line><span class=cl>INFO 04-14 06:01:13 <span class=o>[</span>loader.py:447<span class=o>]</span> Loading weights took 0.95 seconds
</span></span><span class=line><span class=cl>INFO 04-14 06:01:14 <span class=o>[</span>gpu_model_runner.py:1273<span class=o>]</span> Model loading took 3.3465 GiB and 1.192773 seconds
</span></span><span class=line><span class=cl>INFO 04-14 06:01:23 <span class=o>[</span>backends.py:416<span class=o>]</span> Using cache directory: /root/.cache/vllm/torch_compile_cache/74d872966c/rank_0_0 <span class=k>for</span> vLLM<span class=s1>&#39;s torch.compile
</span></span></span><span class=line><span class=cl><span class=s1>INFO 04-14 06:01:23 [backends.py:426] Dynamo bytecode transform time: 9.12 s
</span></span></span><span class=line><span class=cl><span class=s1>INFO 04-14 06:01:26 [backends.py:132] Cache the graph of shape None for later use
</span></span></span><span class=line><span class=cl><span class=s1>INFO 04-14 06:01:54 [backends.py:144] Compiling a graph for general shape takes 30.65 s
</span></span></span><span class=line><span class=cl><span class=s1>INFO 04-14 06:02:03 [monitor.py:33] torch.compile takes 39.77 s in total
</span></span></span><span class=line><span class=cl><span class=s1>INFO 04-14 06:02:04 [kv_cache_utils.py:578] GPU KV cache size: 426,448 tokens
</span></span></span><span class=line><span class=cl><span class=s1>INFO 04-14 06:02:04 [kv_cache_utils.py:581] Maximum concurrency for 131,072 tokens per request: 3.25x
</span></span></span><span class=line><span class=cl><span class=s1>INFO 04-14 06:02:33 [gpu_model_runner.py:1608] Graph capturing finished in 29 secs, took 1.47 GiB
</span></span></span><span class=line><span class=cl><span class=s1>INFO 04-14 06:02:33 [core.py:162] init engine (profile, create kv cache, warmup model) took 79.67 seconds
</span></span></span><span class=line><span class=cl><span class=s1>WARNING 04-14 06:02:33 [config.py:1088] Default sampling parameters have been overridden by the model&#39;</span>s Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with <span class=sb>`</span>--generation-config vllm<span class=sb>`</span>.
</span></span><span class=line><span class=cl>INFO 04-14 06:02:33 <span class=o>[</span>serving_chat.py:117<span class=o>]</span> Using default chat sampling params from model: <span class=o>{</span><span class=s1>&#39;temperature&#39;</span>: 0.6, <span class=s1>&#39;top_p&#39;</span>: 0.95<span class=o>}</span>
</span></span><span class=line><span class=cl>INFO 04-14 06:02:33 <span class=o>[</span>serving_completion.py:61<span class=o>]</span> Using default completion sampling params from model: <span class=o>{</span><span class=s1>&#39;temperature&#39;</span>: 0.6, <span class=s1>&#39;top_p&#39;</span>: 0.95<span class=o>}</span>
</span></span><span class=line><span class=cl>INFO 04-14 06:02:33 <span class=o>[</span>api_server.py:1081<span class=o>]</span> Starting vLLM API server on http://0.0.0.0:8000
</span></span><span class=line><span class=cl>INFO 04-14 06:02:33 <span class=o>[</span>launcher.py:26<span class=o>]</span> Available routes are:
</span></span><span class=line><span class=cl>INFO 04-14 06:02:33 <span class=o>[</span>launcher.py:34<span class=o>]</span> Route: /openapi.json, Methods: HEAD, GET
</span></span><span class=line><span class=cl>INFO 04-14 06:02:33 <span class=o>[</span>launcher.py:34<span class=o>]</span> Route: /docs, Methods: HEAD, GET
</span></span><span class=line><span class=cl>INFO 04-14 06:02:33 <span class=o>[</span>launcher.py:34<span class=o>]</span> Route: /docs/oauth2-redirect, Methods: HEAD, GET
</span></span><span class=line><span class=cl>INFO 04-14 06:02:33 <span class=o>[</span>launcher.py:34<span class=o>]</span> Route: /redoc, Methods: HEAD, GET
</span></span><span class=line><span class=cl>INFO 04-14 06:02:33 <span class=o>[</span>launcher.py:34<span class=o>]</span> Route: /health, Methods: GET
</span></span><span class=line><span class=cl>INFO 04-14 06:02:33 <span class=o>[</span>launcher.py:34<span class=o>]</span> Route: /load, Methods: GET
</span></span><span class=line><span class=cl>INFO 04-14 06:02:33 <span class=o>[</span>launcher.py:34<span class=o>]</span> Route: /ping, Methods: GET, POST
</span></span><span class=line><span class=cl>INFO 04-14 06:02:33 <span class=o>[</span>launcher.py:34<span class=o>]</span> Route: /tokenize, Methods: POST
</span></span><span class=line><span class=cl>INFO 04-14 06:02:33 <span class=o>[</span>launcher.py:34<span class=o>]</span> Route: /detokenize, Methods: POST
</span></span><span class=line><span class=cl>INFO 04-14 06:02:33 <span class=o>[</span>launcher.py:34<span class=o>]</span> Route: /v1/models, Methods: GET
</span></span><span class=line><span class=cl>INFO 04-14 06:02:33 <span class=o>[</span>launcher.py:34<span class=o>]</span> Route: /version, Methods: GET
</span></span><span class=line><span class=cl>INFO 04-14 06:02:33 <span class=o>[</span>launcher.py:34<span class=o>]</span> Route: /v1/chat/completions, Methods: POST
</span></span><span class=line><span class=cl>INFO 04-14 06:02:33 <span class=o>[</span>launcher.py:34<span class=o>]</span> Route: /v1/completions, Methods: POST
</span></span><span class=line><span class=cl>INFO 04-14 06:02:33 <span class=o>[</span>launcher.py:34<span class=o>]</span> Route: /v1/embeddings, Methods: POST
</span></span><span class=line><span class=cl>INFO 04-14 06:02:33 <span class=o>[</span>launcher.py:34<span class=o>]</span> Route: /pooling, Methods: POST
</span></span><span class=line><span class=cl>INFO 04-14 06:02:33 <span class=o>[</span>launcher.py:34<span class=o>]</span> Route: /score, Methods: POST
</span></span><span class=line><span class=cl>INFO 04-14 06:02:33 <span class=o>[</span>launcher.py:34<span class=o>]</span> Route: /v1/score, Methods: POST
</span></span><span class=line><span class=cl>INFO 04-14 06:02:33 <span class=o>[</span>launcher.py:34<span class=o>]</span> Route: /v1/audio/transcriptions, Methods: POST
</span></span><span class=line><span class=cl>INFO 04-14 06:02:33 <span class=o>[</span>launcher.py:34<span class=o>]</span> Route: /rerank, Methods: POST
</span></span><span class=line><span class=cl>INFO 04-14 06:02:33 <span class=o>[</span>launcher.py:34<span class=o>]</span> Route: /v1/rerank, Methods: POST
</span></span><span class=line><span class=cl>INFO 04-14 06:02:33 <span class=o>[</span>launcher.py:34<span class=o>]</span> Route: /v2/rerank, Methods: POST
</span></span><span class=line><span class=cl>INFO 04-14 06:02:33 <span class=o>[</span>launcher.py:34<span class=o>]</span> Route: /invocations, Methods: POST
</span></span><span class=line><span class=cl>INFO 04-14 06:02:44 <span class=o>[</span>loggers.py:87<span class=o>]</span> Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: <span class=m>0</span> reqs, Waiting: <span class=m>0</span> reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
</span></span><span class=line><span class=cl>INFO 04-14 06:02:54 <span class=o>[</span>loggers.py:87<span class=o>]</span> Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: <span class=m>0</span> reqs, Waiting: <span class=m>0</span> reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
</span></span><span class=line><span class=cl>INFO 04-14 06:03:04 <span class=o>[</span>loggers.py:87<span class=o>]</span> Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: <span class=m>0</span> reqs, Waiting: <span class=m>0</span> reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
</span></span><span class=line><span class=cl>INFO 04-14 06:03:14 <span class=o>[</span>loggers.py:87<span class=o>]</span> Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: <span class=m>0</span> reqs, Waiting: <span class=m>0</span> reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
</span></span><span class=line><span class=cl>INFO 04-14 06:03:24 <span class=o>[</span>loggers.py:87<span class=o>]</span> Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: <span class=m>0</span> reqs, Waiting: <span class=m>0</span> reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
</span></span><span class=line><span class=cl>INFO 04-14 06:03:34 <span class=o>[</span>loggers.py:87<span class=o>]</span> Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: <span class=m>0</span> reqs, Waiting: <span class=m>0</span> reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
</span></span><span class=line><span class=cl>INFO 04-14 06:03:44 <span class=o>[</span>loggers.py:87<span class=o>]</span> Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: <span class=m>0</span> reqs, Waiting: <span class=m>0</span> reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
</span></span><span class=line><span class=cl>INFO 04-14 06:03:54 <span class=o>[</span>loggers.py:87<span class=o>]</span> Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: <span class=m>0</span> reqs, Waiting: <span class=m>0</span> reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
</span></span><span class=line><span class=cl>INFO 04-14 06:04:04 <span class=o>[</span>loggers.py:87<span class=o>]</span> Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: <span class=m>0</span> reqs, Waiting: <span class=m>0</span> reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
</span></span><span class=line><span class=cl>INFO 04-14 06:04:12 <span class=o>[</span>launcher.py:74<span class=o>]</span> Shutting down FastAPI HTTP server.
</span></span><span class=line><span class=cl>INFO 04-14 06:04:14 <span class=o>[</span>loggers.py:87<span class=o>]</span> Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: <span class=m>0</span> reqs, Waiting: <span class=m>0</span> reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
</span></span></code></pre></td></tr></table></div></div><hr><h2 id=32-æ¨¡å‹é…ç½®>3.2 æ¨¡å‹é…ç½®<a hidden class=anchor aria-hidden=true href=#32-æ¨¡å‹é…ç½®>#</a></h2><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=o>{</span>
</span></span><span class=line><span class=cl>  <span class=s2>&#34;architectures&#34;</span>: <span class=o>[</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;Qwen2ForCausalLM&#34;</span>
</span></span><span class=line><span class=cl>  <span class=o>]</span>,
</span></span><span class=line><span class=cl>  <span class=s2>&#34;attention_dropout&#34;</span>: 0.0,
</span></span><span class=line><span class=cl>  <span class=s2>&#34;bos_token_id&#34;</span>: 151643,
</span></span><span class=line><span class=cl>  <span class=s2>&#34;eos_token_id&#34;</span>: 151643,
</span></span><span class=line><span class=cl>  <span class=s2>&#34;hidden_act&#34;</span>: <span class=s2>&#34;silu&#34;</span>,
</span></span><span class=line><span class=cl>  <span class=s2>&#34;hidden_size&#34;</span>: 1536,
</span></span><span class=line><span class=cl>  <span class=s2>&#34;initializer_range&#34;</span>: 0.02,
</span></span><span class=line><span class=cl>  <span class=s2>&#34;intermediate_size&#34;</span>: 8960,
</span></span><span class=line><span class=cl>  <span class=s2>&#34;max_position_embeddings&#34;</span>: 131072,
</span></span><span class=line><span class=cl>  <span class=s2>&#34;max_window_layers&#34;</span>: 21,
</span></span><span class=line><span class=cl>  <span class=s2>&#34;model_type&#34;</span>: <span class=s2>&#34;qwen2&#34;</span>,
</span></span><span class=line><span class=cl>  <span class=s2>&#34;num_attention_heads&#34;</span>: 12,
</span></span><span class=line><span class=cl>  <span class=s2>&#34;num_hidden_layers&#34;</span>: 28,
</span></span><span class=line><span class=cl>  <span class=s2>&#34;num_key_value_heads&#34;</span>: 2,
</span></span><span class=line><span class=cl>  <span class=s2>&#34;rms_norm_eps&#34;</span>: 1e-06,
</span></span><span class=line><span class=cl>  <span class=s2>&#34;rope_theta&#34;</span>: 10000,
</span></span><span class=line><span class=cl>  <span class=s2>&#34;sliding_window&#34;</span>: 4096,
</span></span><span class=line><span class=cl>  <span class=s2>&#34;tie_word_embeddings&#34;</span>: false,
</span></span><span class=line><span class=cl>  <span class=s2>&#34;torch_dtype&#34;</span>: <span class=s2>&#34;bfloat16&#34;</span>,
</span></span><span class=line><span class=cl>  <span class=s2>&#34;transformers_version&#34;</span>: <span class=s2>&#34;4.44.0&#34;</span>,
</span></span><span class=line><span class=cl>  <span class=s2>&#34;use_cache&#34;</span>: true,
</span></span><span class=line><span class=cl>  <span class=s2>&#34;use_mrope&#34;</span>: false,
</span></span><span class=line><span class=cl>  <span class=s2>&#34;use_sliding_window&#34;</span>: false,
</span></span><span class=line><span class=cl>  <span class=s2>&#34;vocab_size&#34;</span>: <span class=m>151936</span>
</span></span><span class=line><span class=cl><span class=o>}</span>
</span></span></code></pre></td></tr></table></div></div><hr><h2 id=33-è®¡ç®—éªŒè¯>3.3 è®¡ç®—éªŒè¯<a hidden class=anchor aria-hidden=true href=#33-è®¡ç®—éªŒè¯>#</a></h2><h3 id=331-æ¯ä¸ªtokençš„kvç¼“å­˜å¤§å°è®¡ç®—>3.3.1 æ¯ä¸ªtokençš„KVç¼“å­˜å¤§å°è®¡ç®—<a hidden class=anchor aria-hidden=true href=#331-æ¯ä¸ªtokençš„kvç¼“å­˜å¤§å°è®¡ç®—>#</a></h3><ul><li>head_size = hidden_size / num_attention_heads = 1536 / 12 = 128</li><li>å•tokenå•å±‚çš„KVç¼“å­˜ = 2(Kå’ŒV) * num_key_value_heads * head_size * dtype = 2 * 2 * 128 * 2 = 1024å­—èŠ‚</li><li>å…¨æ¨¡å‹å•token KVç¼“å­˜ = å•tokenå•å±‚çš„KVç¼“å­˜ * num_hidden_layers = 1024 * 28 = 28672å­—èŠ‚</li></ul><hr><h3 id=332-æ˜¾å­˜åˆ†é…>3.3.2 æ˜¾å­˜åˆ†é…<a hidden class=anchor aria-hidden=true href=#332-æ˜¾å­˜åˆ†é…>#</a></h3><ul><li>æ€»å¯ç”¨æ˜¾å­˜ = 24GB * 0.9 = 21.6GB</li><li>éƒ¨åˆ†æ˜¾å­˜ç”¨äºæ¨¡å‹æƒé‡ã€æ¿€æ´»å€¼ç­‰</li><li>å‰©ä½™æ˜¾å­˜ç”¨äºKVç¼“å­˜</li></ul><hr><h3 id=333-å¯ç¼“å­˜çš„tokenæ•°é‡>3.3.3 å¯ç¼“å­˜çš„tokenæ•°é‡<a hidden class=anchor aria-hidden=true href=#333-å¯ç¼“å­˜çš„tokenæ•°é‡>#</a></h3><p>æ—¥å¿—çš„ç»“æœä¸ºï¼šINFO 04-14 06:02:04 [kv_cache_utils.py:578] GPU KV cache size: 426,448 tokens</p><ul><li>æ ¹æ®æ—¥å¿—ç»“æœåæ¨å¯å¾—åˆ°ï¼šå‰©ä½™æ˜¾å­˜ç”¨äºKVç¼“å­˜ / æ¯ä¸ªtokençš„KVç¼“å­˜å¤§å° = 426448</li><li>å‰©ä½™æ˜¾å­˜ç”¨äºKVç¼“å­˜å¤§å°ä¸ºï¼š11.38739 GB</li></ul><hr><h1 id=4-vllmä¸­å¦‚ä½•è®¡ç®—kvcache>4. vLLMä¸­å¦‚ä½•è®¡ç®—KVcache<a hidden class=anchor aria-hidden=true href=#4-vllmä¸­å¦‚ä½•è®¡ç®—kvcache>#</a></h1><p>æ¥ä¸Šä¸€èŠ‚ï¼Œä»vLLMä»£ç å±‚é¢åˆ†æï¼Œ426,448ä¸ªtokençš„KVç¼“å­˜å®¹é‡æ˜¯é€šè¿‡ä»¥ä¸‹æ­¥éª¤è®¡ç®—å¾—å‡ºçš„ï¼š</p><h2 id=41-æ ¸å¿ƒè®¡ç®—å‡½æ•°>4.1 æ ¸å¿ƒè®¡ç®—å‡½æ•°<a hidden class=anchor aria-hidden=true href=#41-æ ¸å¿ƒè®¡ç®—å‡½æ•°>#</a></h2><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>worker.py: determine_num_available_blocks<span class=o>()</span> â†’ è®¡ç®—å¯ç”¨å—æ•°é‡
</span></span><span class=line><span class=cl>cache_engine.py: get_cache_block_size<span class=o>()</span> â†’ è®¡ç®—æ¯ä¸ªå—å¤§å°
</span></span><span class=line><span class=cl>kv_cache_utils.py: get_kv_cache_configs<span class=o>()</span> â†’ é…ç½®KVç¼“å­˜
</span></span></code></pre></td></tr></table></div></div><h2 id=411-determine_num_available_blocks>4.1.1 determine_num_available_blocks()<a hidden class=anchor aria-hidden=true href=#411-determine_num_available_blocks>#</a></h2><p>å¯¹äºä½ çš„4090D(24GB)ï¼Œä½¿ç”¨0.9çš„gpu_memory_utilizationï¼š</p><ul><li>æ€»æ˜¾å­˜: 24GBÂ Ã— 0.9Â â‰ˆ 21.6GB</li><li>å‡å»æ¨¡å‹æƒé‡å’Œæ¿€æ´»å€¼ç­‰éKVç¼“å­˜éƒ¨åˆ†</li></ul><hr><h2 id=412-get_cache_blockz_size>4.1.2 get_cache_blockZ_size()<a hidden class=anchor aria-hidden=true href=#412-get_cache_blockz_size>#</a></h2><blockquote><p>ğŸ’¡ è®¡ç®—ç¼“å­˜å—çš„å¤§å°</p></blockquote><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nd>@staticmethod</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>get_cache_block_size</span><span class=p>(</span><span class=n>cache_config</span><span class=p>,</span> <span class=n>model_config</span><span class=p>,</span> <span class=n>parallel_config</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>head_size</span> <span class=o>=</span> <span class=n>model_config</span><span class=o>.</span><span class=n>get_head_size</span><span class=p>()</span>  <span class=c1># 128</span>
</span></span><span class=line><span class=cl>    <span class=n>num_heads</span> <span class=o>=</span> <span class=n>model_config</span><span class=o>.</span><span class=n>get_num_kv_heads</span><span class=p>(</span><span class=n>parallel_config</span><span class=p>)</span>  <span class=c1># 2</span>
</span></span><span class=line><span class=cl>    <span class=n>num_attention_layers</span> <span class=o>=</span> <span class=n>model_config</span><span class=o>.</span><span class=n>get_num_layers_by_block_type</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>parallel_config</span><span class=p>,</span> <span class=n>LayerBlockType</span><span class=o>.</span><span class=n>attention</span><span class=p>)</span>  <span class=c1># 28</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># è·å–æ•°æ®ç±»å‹å¤§å°</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>cache_config</span><span class=o>.</span><span class=n>cache_dtype</span> <span class=o>==</span> <span class=s2>&#34;auto&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>dtype</span> <span class=o>=</span> <span class=n>model_config</span><span class=o>.</span><span class=n>dtype</span>  <span class=c1># bfloat16</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>dtype</span> <span class=o>=</span> <span class=n>STR_DTYPE_TO_TORCH_DTYPE</span><span class=p>[</span><span class=n>cache_config</span><span class=o>.</span><span class=n>cache_dtype</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># è®¡ç®—keyç¼“å­˜é¡¹å’Œvalueç¼“å­˜é¡¹å¤§å°</span>
</span></span><span class=line><span class=cl>    <span class=n>key_cache_entry</span> <span class=o>=</span> <span class=n>num_heads</span> <span class=o>*</span> <span class=n>head_size</span>  <span class=c1># 2 * 128 = 256</span>
</span></span><span class=line><span class=cl>    <span class=n>value_cache_entry</span> <span class=o>=</span> <span class=n>key_cache_entry</span>  <span class=c1># 256</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># è®¡ç®—ä¸€ä¸ªå—çš„æ€»å…ƒç´ æ•°</span>
</span></span><span class=line><span class=cl>    <span class=n>total</span> <span class=o>=</span> <span class=n>num_attention_layers</span> <span class=o>*</span> <span class=n>cache_config</span><span class=o>.</span><span class=n>block_size</span> <span class=o>*</span> <span class=p>(</span><span class=n>key_cache_entry</span> <span class=o>+</span> <span class=n>value_cache_entry</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># 28 * 16 * (256 + 256) = 28 * 16 * 512 = 229,376ä¸ªå…ƒç´ </span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># æ¯ä¸ªå…ƒç´ çš„å­—èŠ‚å¤§å°(bfloat16 = 2å­—èŠ‚)</span>
</span></span><span class=line><span class=cl>    <span class=n>dtype_size</span> <span class=o>=</span> <span class=n>get_dtype_size</span><span class=p>(</span><span class=n>dtype</span><span class=p>)</span>  <span class=c1># 2</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># è¿”å›å—å¤§å°(å­—èŠ‚)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>dtype_size</span> <span class=o>*</span> <span class=n>total</span>  <span class=c1># 2 * 229,376 = 458,752å­—èŠ‚</span>
</span></span></code></pre></td></tr></table></div></div><hr><h2 id=413-determine_num_available_blocks>4.1.3 determine_num_available_blocks()<a hidden class=anchor aria-hidden=true href=#413-determine_num_available_blocks>#</a></h2><blockquote><p>ğŸ’¡ è®¡ç®—å¯ç”¨å—çš„æ•°é‡</p></blockquote><p>é‡æ–°å›åˆ°determine_num_available_blocks()</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>cache_block_size</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>get_cache_block_size_bytes</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>num_gpu_blocks</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=n>available_kv_cache_memory</span> <span class=o>//</span> <span class=n>cache_block_size</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>è¿™é‡Œclaude-3.7-sonnetå›å¤ç»“æœä¸ºï¼š</p><p>è®¡ç®—å¯ç”¨å—æ•°ï¼š</p><ul><li>å‡è®¾å¯ç”¨äºKVç¼“å­˜çš„æ˜¾å­˜çº¦ä¸º12.2GB</li><li>æ¯ä¸ªå—å¤§å°ä¸º458.75KB</li><li>å—æ•°é‡Â = 12.2GBÂ Ã· 458.75KBÂ â‰ˆ 26,653å—
æˆ‘è§‰å¾—12.2GBä¸æ˜¯å¾ˆä¸¥è°¨ï¼Œä¸“é—¨å»æŸ¥äº†ä¸€ä¸‹å¦‚ä½•ä¼°ç®—å‡º12.2GBçš„è¿‡ç¨‹ã€‚ä»£å…¥qwen1.5bæ¨¡å‹ï¼Œæ¨èåœºæ™¯ä¸‹ä¸éœ€è¦è®¡ç®—æ¢¯åº¦ï¼Œæ‰€ä»¥äº§ç”Ÿçš„æ˜¾å­˜ç±»å‹åº”è¯¥ä¸ºï¼šæ¨¡å‹æƒé‡å’Œå‰ä¼ çš„æ¿€æ´»å€¼ã€‚æ¨¡å‹æƒé‡çš„è®¡ç®—ï¼šå‚æ•°é‡ * dtype â‰ˆ 3GBï¼Œå‰ä¼ çš„æ¿€æ´»å€¼ï¼Œæ²¡æŸ¥åˆ°æ¥æºä¾æ®ã€‚</li></ul><hr><h2 id=414-_get_kv_cache_config_uniform_type>4.1.4 _get_kv_cache_config_uniform_type()<a hidden class=anchor aria-hidden=true href=#414-_get_kv_cache_config_uniform_type>#</a></h2><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span><span class=lnt>61
</span><span class=lnt>62
</span><span class=lnt>63
</span><span class=lnt>64
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>_get_kv_cache_config_uniform_type</span><span class=p>(</span><span class=n>vllm_config</span><span class=p>:</span> <span class=n>VllmConfig</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                      <span class=n>kv_cache_spec</span><span class=p>:</span> <span class=n>KVCacheSpec</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                      <span class=n>available_memory</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                      <span class=n>num_layers</span><span class=p>:</span> <span class=nb>int</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>KVCacheConfig</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    ä¸ºå…·æœ‰å•ä¸€ç±»å‹KVç¼“å­˜çš„æ¨¡å‹ç”ŸæˆKVç¼“å­˜é…ç½®ã€‚
</span></span></span><span class=line><span class=cl><span class=s2>    å°†å¯ç”¨å†…å­˜å¹³å‡åˆ†é…ç»™æ‰€æœ‰å±‚ã€‚
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    å‚æ•°:
</span></span></span><span class=line><span class=cl><span class=s2>        vllm_config: å…¨å±€VllmConfigé…ç½®
</span></span></span><span class=line><span class=cl><span class=s2>        kv_cache_spec: æ¨¡å‹çš„KVç¼“å­˜è§„æ ¼
</span></span></span><span class=line><span class=cl><span class=s2>        available_memory: KVç¼“å­˜å¯ç”¨çš„å†…å­˜å¤§å°(å­—èŠ‚)
</span></span></span><span class=line><span class=cl><span class=s2>        num_layers: æ¨¡å‹çš„å±‚æ•°
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    è¿”å›:
</span></span></span><span class=line><span class=cl><span class=s2>        ç”Ÿæˆçš„KVCacheConfigé…ç½®
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># è·å–æ‰€æœ‰å±‚çš„é¡µé¢å¤§å°(å­—èŠ‚)</span>
</span></span><span class=line><span class=cl>    <span class=n>page_sizes</span> <span class=o>=</span> <span class=p>{</span><span class=n>layer</span><span class=o>.</span><span class=n>page_size_bytes</span> <span class=k>for</span> <span class=n>layer</span> <span class=ow>in</span> <span class=n>kv_cache_spec</span><span class=o>.</span><span class=n>values</span><span class=p>()}</span>
</span></span><span class=line><span class=cl>    <span class=c1># ç¡®ä¿æ‰€æœ‰å±‚ä½¿ç”¨ç›¸åŒçš„é¡µé¢å¤§å°</span>
</span></span><span class=line><span class=cl>    <span class=k>assert</span> <span class=nb>len</span><span class=p>(</span><span class=n>page_sizes</span><span class=p>)</span> <span class=o>==</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>    <span class=n>page_size</span> <span class=o>=</span> <span class=n>page_sizes</span><span class=o>.</span><span class=n>pop</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># è®¡ç®—æ¯å±‚å¯åˆ†é…çš„å—æ•°</span>
</span></span><span class=line><span class=cl>    <span class=n>num_blocks</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=n>available_memory</span> <span class=o>//</span> <span class=n>page_size</span> <span class=o>//</span> <span class=n>num_layers</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>num_blocks</span> <span class=o>=</span> <span class=nb>max</span><span class=p>(</span><span class=n>num_blocks</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># å¦‚æœé…ç½®ä¸­æŒ‡å®šäº†GPUå—æ•°çš„è¦†ç›–å€¼,åˆ™ä½¿ç”¨è¯¥å€¼</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>vllm_config</span><span class=o>.</span><span class=n>cache_config</span><span class=o>.</span><span class=n>num_gpu_blocks_override</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>num_gpu_blocks_override</span> <span class=o>=</span> \
</span></span><span class=line><span class=cl>            <span class=n>vllm_config</span><span class=o>.</span><span class=n>cache_config</span><span class=o>.</span><span class=n>num_gpu_blocks_override</span>
</span></span><span class=line><span class=cl>        <span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;ä½¿ç”¨num_gpu_blocks_override=</span><span class=si>%d</span><span class=s2>è¦†ç›–åŸå§‹num_gpu_blocks=</span><span class=si>%d</span><span class=s2>&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>num_gpu_blocks_override</span><span class=p>,</span> <span class=n>num_blocks</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>num_blocks</span> <span class=o>=</span> <span class=n>num_gpu_blocks_override</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># è®°å½•GPUå—æ•°ä¿¡æ¯</span>
</span></span><span class=line><span class=cl>    <span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=s2>&#34;GPUå—æ•°: </span><span class=si>%d</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>num_blocks</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># è®¡ç®—æœ€å¤§å¹¶å‘åº¦</span>
</span></span><span class=line><span class=cl>    <span class=n>max_concurrency</span> <span class=o>=</span> <span class=p>(</span><span class=n>num_blocks</span> <span class=o>*</span> <span class=n>vllm_config</span><span class=o>.</span><span class=n>cache_config</span><span class=o>.</span><span class=n>block_size</span> <span class=o>/</span>
</span></span><span class=line><span class=cl>                       <span class=n>vllm_config</span><span class=o>.</span><span class=n>model_config</span><span class=o>.</span><span class=n>max_model_len</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=s2>&#34;æ¯ä¸ªè¯·æ±‚</span><span class=si>%s</span><span class=s2>ä¸ªtokenæ—¶çš„æœ€å¤§å¹¶å‘åº¦: </span><span class=si>%.2f</span><span class=s2>x&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>vllm_config</span><span class=o>.</span><span class=n>model_config</span><span class=o>.</span><span class=n>max_model_len</span><span class=p>,</span> <span class=n>max_concurrency</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># è®¡ç®—æ¯å±‚çš„å†…å­˜å¤§å°</span>
</span></span><span class=line><span class=cl>    <span class=n>per_layer_size</span> <span class=o>=</span> <span class=n>page_size</span> <span class=o>*</span> <span class=n>num_blocks</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># åˆ›å»ºKVç¼“å­˜é…ç½®</span>
</span></span><span class=line><span class=cl>    <span class=n>kv_cache_config</span> <span class=o>=</span> <span class=n>KVCacheConfig</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>num_blocks</span><span class=o>=</span><span class=n>num_blocks</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=c1># ä¸ºæ¯ä¸€å±‚åˆ›å»ºç›¸åŒå¤§å°çš„KVç¼“å­˜å¼ é‡</span>
</span></span><span class=line><span class=cl>        <span class=n>tensors</span><span class=o>=</span><span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=n>layer_name</span><span class=p>:</span> <span class=n>KVCacheTensor</span><span class=p>(</span><span class=n>size</span><span class=o>=</span><span class=n>per_layer_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>layer_name</span> <span class=ow>in</span> <span class=n>kv_cache_spec</span>
</span></span><span class=line><span class=cl>        <span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=c1># å°†æ‰€æœ‰å±‚ç»„ç»‡ä¸ºä¸€ä¸ªç»„</span>
</span></span><span class=line><span class=cl>        <span class=n>groups</span><span class=o>=</span><span class=p>[[</span><span class=n>layer_name</span> <span class=k>for</span> <span class=n>layer_name</span> <span class=ow>in</span> <span class=n>kv_cache_spec</span><span class=p>]],</span>
</span></span><span class=line><span class=cl>        <span class=n>kv_cache_spec</span><span class=o>=</span><span class=n>kv_cache_spec</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>kv_cache_config</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl><span class=c1># æ€»tokenå®¹é‡ = å—æ•°é‡ * æ¯å—å­˜å‚¨çš„tokenæ•°</span>
</span></span><span class=line><span class=cl><span class=n>total_tokens</span> <span class=o>=</span> <span class=n>num_gpu_blocks</span> <span class=o>*</span> <span class=n>block_size</span>
</span></span><span class=line><span class=cl><span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=s2>&#34;GPU KV cache size: </span><span class=si>%d</span><span class=s2> tokens&#34;</span><span class=p>,</span> <span class=n>total_tokens</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>æ€»tokenå®¹é‡ = 26653 * 16 = 426448ï¼Œ ä¸æ—¥å¿—å¯¹æ¯”å‘ç°æ•°å€¼ä¸€è‡´ã€‚</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://Pan-Binghong.github.io/daily-learning/tags/vllm/>VLLM</a></li></ul><nav class=paginav><a class=prev href=https://Pan-Binghong.github.io/daily-learning/ai/stepvideo-t2v-deployment-manual/><span class=title>Â« Prev</span><br><span>Stepvideo-t2v Deployment Manual</span>
</a><a class=next href=https://Pan-Binghong.github.io/daily-learning/devops/%E4%BD%BF%E7%94%A8-lvm-%E7%AE%A1%E7%90%86%E5%A4%9A%E4%B8%AA-nvme-%E7%A3%81%E7%9B%98/><span class=title>Next Â»</span><br><span>ä½¿ç”¨ LVM ç®¡ç†å¤šä¸ª NVMe ç£ç›˜</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Learning vLL (0.7.2) with Cursor on x" href="https://x.com/intent/tweet/?text=Learning%20vLL%20%280.7.2%29%20with%20Cursor&amp;url=https%3a%2f%2fPan-Binghong.github.io%2fdaily-learning%2fai%2flearning-vll-0.7.2-with-cursor%2f&amp;hashtags=VLLM"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Learning vLL (0.7.2) with Cursor on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fPan-Binghong.github.io%2fdaily-learning%2fai%2flearning-vll-0.7.2-with-cursor%2f&amp;title=Learning%20vLL%20%280.7.2%29%20with%20Cursor&amp;summary=Learning%20vLL%20%280.7.2%29%20with%20Cursor&amp;source=https%3a%2f%2fPan-Binghong.github.io%2fdaily-learning%2fai%2flearning-vll-0.7.2-with-cursor%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Learning vLL (0.7.2) with Cursor on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fPan-Binghong.github.io%2fdaily-learning%2fai%2flearning-vll-0.7.2-with-cursor%2f&title=Learning%20vLL%20%280.7.2%29%20with%20Cursor"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Learning vLL (0.7.2) with Cursor on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fPan-Binghong.github.io%2fdaily-learning%2fai%2flearning-vll-0.7.2-with-cursor%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Learning vLL (0.7.2) with Cursor on whatsapp" href="https://api.whatsapp.com/send?text=Learning%20vLL%20%280.7.2%29%20with%20Cursor%20-%20https%3a%2f%2fPan-Binghong.github.io%2fdaily-learning%2fai%2flearning-vll-0.7.2-with-cursor%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Learning vLL (0.7.2) with Cursor on telegram" href="https://telegram.me/share/url?text=Learning%20vLL%20%280.7.2%29%20with%20Cursor&amp;url=https%3a%2f%2fPan-Binghong.github.io%2fdaily-learning%2fai%2flearning-vll-0.7.2-with-cursor%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Learning vLL (0.7.2) with Cursor on ycombinator" href="https://news.ycombinator.com/submitlink?t=Learning%20vLL%20%280.7.2%29%20with%20Cursor&u=https%3a%2f%2fPan-Binghong.github.io%2fdaily-learning%2fai%2flearning-vll-0.7.2-with-cursor%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer><div class=comments-section><hr class=comments-divider><div class=giscus-comments><h3 class=comments-title>ğŸ’¬ è¯„è®ºè®¨è®º</h3><p class=comments-hint>ä½¿ç”¨ GitHub è´¦å·ç™»å½•å³å¯å‚ä¸è®¨è®ºã€‚è¯„è®ºå°†åŒæ­¥åˆ°
<a href=https://github.com/Pan-Binghong/daily-learning/discussions target=_blank rel=noopener>GitHub Discussions</a></p><script src=https://giscus.app/client.js data-repo=Pan-Binghong/daily-learning data-repo-id=R_kgDONYfgVw data-category=Announcements data-category-id=DIC_kwDONYfgV84CkvE7 data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script></div></div><style>.comments-section{margin-top:3rem}.comments-divider{margin:2rem 0;border:none;border-top:2px solid var(--border)}.comments-title{font-size:1.5rem;margin-bottom:1rem;color:var(--primary)}.comments-hint{font-size:.9rem;color:var(--secondary);margin-bottom:1.5rem;padding:1rem;background:var(--code-bg);border-radius:8px;border-left:4px solid var(--primary)}.comments-hint a{color:var(--primary);text-decoration:underline}.giscus-comments{margin-top:2rem}</style></article></main><footer class=footer><span>&copy; 2025 <a href=https://Pan-Binghong.github.io/daily-learning/>Pan Binghong's Tech Blog</a></span> Â·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg></a><div class=reading-progress-bar id=reading-progress></div><script>window.addEventListener("scroll",function(){const e=document.getElementById("reading-progress");if(!e)return;const t=window.innerHeight,n=document.documentElement.scrollHeight-t,s=window.scrollY,o=s/n*100;e.style.width=o+"%"}),document.addEventListener("DOMContentLoaded",function(){const e=document.querySelectorAll(".copy-text");e.forEach(e=>{e.addEventListener("click",function(){this.textContent="âœ“ å·²å¤åˆ¶",setTimeout(()=>{this.textContent="å¤åˆ¶"},2e3)})})}),document.addEventListener("DOMContentLoaded",function(){const e=document.querySelectorAll('a[href^="http"]');e.forEach(e=>{e.hostname!==window.location.hostname&&(e.setAttribute("target","_blank"),e.setAttribute("rel","noopener noreferrer"))})}),document.addEventListener("DOMContentLoaded",function(){const e=document.querySelectorAll(".post-content img");e.forEach(e=>{e.style.cursor="pointer",e.addEventListener("click",function(){window.open(this.src,"_blank")})})})</script><style>#top-link{transition:all .3s ease}#top-link:hover{transform:translateY(-5px);box-shadow:0 5px 15px rgba(0,0,0,.2)}</style><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>