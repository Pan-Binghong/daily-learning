<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>基于Pytorch架构手撕Transformer | 我的博客</title><meta name=keywords content><meta name=description content='深入学习Tranformer模型结构, 本来依赖GPT4o, 自己快复现完了, 结果在封装Decoder层的时候, 发生了mask维度匹配不对的问题, 查了很多资料最终还是没解决, 后期再搞吧.

Transformer 的背景

什么是Transformer ?
Transformer是谁提出的 ?
Transformer解决了什么问题 ?
Transformer核心组件有哪些 ?


注意力机制

核心公式 :





论文原图 :


Scaled Dot-product Attention


代码实现 :

代码整合(Scaled Dot-product Attention)


 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30


import torch  # 导入PyTorch库
import torch.nn.functional as F  # 导入PyTorch的nn.functional模块，包含了许多神经网络操作的函数
from math import sqrt  # 导入math库中的sqrt函数，用于计算平方根

# 定义Scaled Dot-product Attention函数
def scaled_dot_product_attention(query, key, value, query_mask=None, key_mask=None, mask=None):
    # 获取查询（query）的最后一个维度大小，即键（key）的维度
    dim_k = query.size(-1)
    
    # 计算查询和键的点积，并缩放，得到未归一化的注意力分数
    scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)
    
    # 如果提供了查询掩码（query_mask）和键掩码（key_mask），则计算掩码矩阵
    if query_mask is not None and key_mask is not None:
        mask = torch.bmm(query_mask.unsqueeze(-1), key_mask.unsqueeze(1))
    else:
        # 如果没有提供掩码，则使用之前传入的掩码（如果有的话）
        mask = mask
    
    # 如果存在掩码，则将分数矩阵中与掩码对应位置为0的分数替换为负无穷
    # 这样在应用softmax时，这些位置的权重会接近于0
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -float("inf"))
    
    # 使用softmax函数对分数进行归一化，得到注意力权重
    weights = F.softmax(scores, dim=-1)
    
    # 计算加权后的输出，即将注意力权重与值（value）相乘
    # 这里的输出是经过注意力加权后的值向量，用于下游任务
    return torch.bmm(weights, value)


Multi-head Attention

Multi-head Attention作用 :
首先通过线性映射将 Q,K,V 序列映射到特征空间，每一组线性投影后的向量表示称为一个头 (head)，然后在每组映射后的序列上再应用 Scaled Dot-product Attention, 最后将每个头的向量拼接.


代码整合(Multi-head Attention)

代码实现 :
验证代码


前馈神经网络
The Feed-Forward Layer
没啥好写的, 就是普通的全连接 + 激活函数'><meta name=author content="Pan Binghong"><link rel=canonical href=https://Pan-Binghong.github.io/daily-learning/ai/%E5%9F%BA%E4%BA%8Epytorch%E6%9E%B6%E6%9E%84%E6%89%8B%E6%92%95transformer/><link crossorigin=anonymous href=/daily-learning/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css integrity="sha256-NDzEgLn/yPBMy+XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as=style><link rel=icon href=https://Pan-Binghong.github.io/daily-learning/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://Pan-Binghong.github.io/daily-learning/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://Pan-Binghong.github.io/daily-learning/favicon-32x32.png><link rel=apple-touch-icon href=https://Pan-Binghong.github.io/daily-learning/apple-touch-icon.png><link rel=mask-icon href=https://Pan-Binghong.github.io/daily-learning/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://Pan-Binghong.github.io/daily-learning/ai/%E5%9F%BA%E4%BA%8Epytorch%E6%9E%B6%E6%9E%84%E6%89%8B%E6%92%95transformer/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://Pan-Binghong.github.io/daily-learning/ai/%E5%9F%BA%E4%BA%8Epytorch%E6%9E%B6%E6%9E%84%E6%89%8B%E6%92%95transformer/"><meta property="og:site_name" content="我的博客"><meta property="og:title" content="基于Pytorch架构手撕Transformer"><meta property="og:description" content='深入学习Tranformer模型结构, 本来依赖GPT4o, 自己快复现完了, 结果在封装Decoder层的时候, 发生了mask维度匹配不对的问题, 查了很多资料最终还是没解决, 后期再搞吧.
Transformer 的背景 什么是Transformer ? Transformer是谁提出的 ? Transformer解决了什么问题 ? Transformer核心组件有哪些 ? 注意力机制 核心公式 : 论文原图 : Scaled Dot-product Attention 代码实现 : 代码整合(Scaled Dot-product Attention) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 import torch # 导入PyTorch库 import torch.nn.functional as F # 导入PyTorch的nn.functional模块，包含了许多神经网络操作的函数 from math import sqrt # 导入math库中的sqrt函数，用于计算平方根 # 定义Scaled Dot-product Attention函数 def scaled_dot_product_attention(query, key, value, query_mask=None, key_mask=None, mask=None): # 获取查询（query）的最后一个维度大小，即键（key）的维度 dim_k = query.size(-1) # 计算查询和键的点积，并缩放，得到未归一化的注意力分数 scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k) # 如果提供了查询掩码（query_mask）和键掩码（key_mask），则计算掩码矩阵 if query_mask is not None and key_mask is not None: mask = torch.bmm(query_mask.unsqueeze(-1), key_mask.unsqueeze(1)) else: # 如果没有提供掩码，则使用之前传入的掩码（如果有的话） mask = mask # 如果存在掩码，则将分数矩阵中与掩码对应位置为0的分数替换为负无穷 # 这样在应用softmax时，这些位置的权重会接近于0 if mask is not None: scores = scores.masked_fill(mask == 0, -float("inf")) # 使用softmax函数对分数进行归一化，得到注意力权重 weights = F.softmax(scores, dim=-1) # 计算加权后的输出，即将注意力权重与值（value）相乘 # 这里的输出是经过注意力加权后的值向量，用于下游任务 return torch.bmm(weights, value) Multi-head Attention Multi-head Attention作用 : 首先通过线性映射将 Q,K,V 序列映射到特征空间，每一组线性投影后的向量表示称为一个头 (head)，然后在每组映射后的序列上再应用 Scaled Dot-product Attention, 最后将每个头的向量拼接. 代码整合(Multi-head Attention) 代码实现 : 验证代码 前馈神经网络 The Feed-Forward Layer 没啥好写的, 就是普通的全连接 + 激活函数'><meta property="og:locale" content="zh-CN"><meta property="og:type" content="article"><meta property="article:section" content="ai"><meta property="article:published_time" content="2024-10-29T01:52:00+00:00"><meta property="article:modified_time" content="2025-06-19T03:45:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="基于Pytorch架构手撕Transformer"><meta name=twitter:description content='深入学习Tranformer模型结构, 本来依赖GPT4o, 自己快复现完了, 结果在封装Decoder层的时候, 发生了mask维度匹配不对的问题, 查了很多资料最终还是没解决, 后期再搞吧.

Transformer 的背景

什么是Transformer ?
Transformer是谁提出的 ?
Transformer解决了什么问题 ?
Transformer核心组件有哪些 ?


注意力机制

核心公式 :





论文原图 :


Scaled Dot-product Attention


代码实现 :

代码整合(Scaled Dot-product Attention)


 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30


import torch  # 导入PyTorch库
import torch.nn.functional as F  # 导入PyTorch的nn.functional模块，包含了许多神经网络操作的函数
from math import sqrt  # 导入math库中的sqrt函数，用于计算平方根

# 定义Scaled Dot-product Attention函数
def scaled_dot_product_attention(query, key, value, query_mask=None, key_mask=None, mask=None):
    # 获取查询（query）的最后一个维度大小，即键（key）的维度
    dim_k = query.size(-1)
    
    # 计算查询和键的点积，并缩放，得到未归一化的注意力分数
    scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)
    
    # 如果提供了查询掩码（query_mask）和键掩码（key_mask），则计算掩码矩阵
    if query_mask is not None and key_mask is not None:
        mask = torch.bmm(query_mask.unsqueeze(-1), key_mask.unsqueeze(1))
    else:
        # 如果没有提供掩码，则使用之前传入的掩码（如果有的话）
        mask = mask
    
    # 如果存在掩码，则将分数矩阵中与掩码对应位置为0的分数替换为负无穷
    # 这样在应用softmax时，这些位置的权重会接近于0
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -float("inf"))
    
    # 使用softmax函数对分数进行归一化，得到注意力权重
    weights = F.softmax(scores, dim=-1)
    
    # 计算加权后的输出，即将注意力权重与值（value）相乘
    # 这里的输出是经过注意力加权后的值向量，用于下游任务
    return torch.bmm(weights, value)


Multi-head Attention

Multi-head Attention作用 :
首先通过线性映射将 Q,K,V 序列映射到特征空间，每一组线性投影后的向量表示称为一个头 (head)，然后在每组映射后的序列上再应用 Scaled Dot-product Attention, 最后将每个头的向量拼接.


代码整合(Multi-head Attention)

代码实现 :
验证代码


前馈神经网络
The Feed-Forward Layer
没啥好写的, 就是普通的全连接 + 激活函数'><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Ais","item":"https://Pan-Binghong.github.io/daily-learning/ai/"},{"@type":"ListItem","position":2,"name":"基于Pytorch架构手撕Transformer","item":"https://Pan-Binghong.github.io/daily-learning/ai/%E5%9F%BA%E4%BA%8Epytorch%E6%9E%B6%E6%9E%84%E6%89%8B%E6%92%95transformer/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"基于Pytorch架构手撕Transformer","name":"基于Pytorch架构手撕Transformer","description":"深入学习Tranformer模型结构, 本来依赖GPT4o, 自己快复现完了, 结果在封装Decoder层的时候, 发生了mask维度匹配不对的问题, 查了很多资料最终还是没解决, 后期再搞吧.\nTransformer 的背景 什么是Transformer ? Transformer是谁提出的 ? Transformer解决了什么问题 ? Transformer核心组件有哪些 ? 注意力机制 核心公式 : 论文原图 : Scaled Dot-product Attention 代码实现 : 代码整合(Scaled Dot-product Attention) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 import torch # 导入PyTorch库 import torch.nn.functional as F # 导入PyTorch的nn.functional模块，包含了许多神经网络操作的函数 from math import sqrt # 导入math库中的sqrt函数，用于计算平方根 # 定义Scaled Dot-product Attention函数 def scaled_dot_product_attention(query, key, value, query_mask=None, key_mask=None, mask=None): # 获取查询（query）的最后一个维度大小，即键（key）的维度 dim_k = query.size(-1) # 计算查询和键的点积，并缩放，得到未归一化的注意力分数 scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k) # 如果提供了查询掩码（query_mask）和键掩码（key_mask），则计算掩码矩阵 if query_mask is not None and key_mask is not None: mask = torch.bmm(query_mask.unsqueeze(-1), key_mask.unsqueeze(1)) else: # 如果没有提供掩码，则使用之前传入的掩码（如果有的话） mask = mask # 如果存在掩码，则将分数矩阵中与掩码对应位置为0的分数替换为负无穷 # 这样在应用softmax时，这些位置的权重会接近于0 if mask is not None: scores = scores.masked_fill(mask == 0, -float(\u0026#34;inf\u0026#34;)) # 使用softmax函数对分数进行归一化，得到注意力权重 weights = F.softmax(scores, dim=-1) # 计算加权后的输出，即将注意力权重与值（value）相乘 # 这里的输出是经过注意力加权后的值向量，用于下游任务 return torch.bmm(weights, value) Multi-head Attention Multi-head Attention作用 : 首先通过线性映射将 Q,K,V 序列映射到特征空间，每一组线性投影后的向量表示称为一个头 (head)，然后在每组映射后的序列上再应用 Scaled Dot-product Attention, 最后将每个头的向量拼接. 代码整合(Multi-head Attention) 代码实现 : 验证代码 前馈神经网络 The Feed-Forward Layer 没啥好写的, 就是普通的全连接 + 激活函数\n","keywords":[],"articleBody":"深入学习Tranformer模型结构, 本来依赖GPT4o, 自己快复现完了, 结果在封装Decoder层的时候, 发生了mask维度匹配不对的问题, 查了很多资料最终还是没解决, 后期再搞吧.\nTransformer 的背景 什么是Transformer ? Transformer是谁提出的 ? Transformer解决了什么问题 ? Transformer核心组件有哪些 ? 注意力机制 核心公式 : 论文原图 : Scaled Dot-product Attention 代码实现 : 代码整合(Scaled Dot-product Attention) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 import torch # 导入PyTorch库 import torch.nn.functional as F # 导入PyTorch的nn.functional模块，包含了许多神经网络操作的函数 from math import sqrt # 导入math库中的sqrt函数，用于计算平方根 # 定义Scaled Dot-product Attention函数 def scaled_dot_product_attention(query, key, value, query_mask=None, key_mask=None, mask=None): # 获取查询（query）的最后一个维度大小，即键（key）的维度 dim_k = query.size(-1) # 计算查询和键的点积，并缩放，得到未归一化的注意力分数 scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k) # 如果提供了查询掩码（query_mask）和键掩码（key_mask），则计算掩码矩阵 if query_mask is not None and key_mask is not None: mask = torch.bmm(query_mask.unsqueeze(-1), key_mask.unsqueeze(1)) else: # 如果没有提供掩码，则使用之前传入的掩码（如果有的话） mask = mask # 如果存在掩码，则将分数矩阵中与掩码对应位置为0的分数替换为负无穷 # 这样在应用softmax时，这些位置的权重会接近于0 if mask is not None: scores = scores.masked_fill(mask == 0, -float(\"inf\")) # 使用softmax函数对分数进行归一化，得到注意力权重 weights = F.softmax(scores, dim=-1) # 计算加权后的输出，即将注意力权重与值（value）相乘 # 这里的输出是经过注意力加权后的值向量，用于下游任务 return torch.bmm(weights, value) Multi-head Attention Multi-head Attention作用 : 首先通过线性映射将 Q,K,V 序列映射到特征空间，每一组线性投影后的向量表示称为一个头 (head)，然后在每组映射后的序列上再应用 Scaled Dot-product Attention, 最后将每个头的向量拼接. 代码整合(Multi-head Attention) 代码实现 : 验证代码 前馈神经网络 The Feed-Forward Layer 没啥好写的, 就是普通的全连接 + 激活函数\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 from torch import nn # 定义FeedForward类，继承自nn.Module class FeedForward(nn.Module): # 初始化函数 def __init__(self, config): super().__init__() # 调用基类的初始化方法 # 定义第一个线性层，将输入的隐藏状态映射到中间维度 self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size) # 定义第二个线性层，将中间维度的表示映射回原始的隐藏状态维度 self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size) # 定义GELU激活函数 self.gelu = nn.GELU() # 定义Dropout层，用于防止过拟合 self.dropout = nn.Dropout(config.hidden_dropout_prob) # 前向传播函数 def forward(self, x): # 应用第一个线性层 x = self.linear_1(x) # 应用GELU激活函数 x = self.gelu(x) # 应用第二个线性层 x = self.linear_2(x) # 应用Dropout x = self.dropout(x) # 返回最终的输出 return x 与上面构建的注意力机制串联测试 :\n1 2 3 feed_forward = FeedForward(config) ff_outputs = feed_forward(attn_output) print(ff_outputs.size()) #torch.Size([1, 2, 768]) 层归一化 \u0026 残差连接 层归一化模块需要包含在残差模块内, 主要作用为 : 将输入的一批向量, 每一个都做标准化处理, 处理为 : 均值为零, 且有单位方差 残差连接主要作用为 : 是通过直接将输入绕过中间层的计算，帮助模型更容易训练深层网络，避免梯度消失问题并促进信息流动 Transformer Encoder Layer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 from torch import nn # 定义TransformerEncoderLayer类，继承自nn.Module class TransformerEncoderLayer(nn.Module): # 初始化函数 def __init__(self, config): super().__init__() # 调用基类的初始化方法 # 定义第一个层归一化，用于注意力机制之前 self.layer_norm_1 = nn.LayerNorm(config.hidden_size) # 定义第二个层归一化，用于前馈网络之前 self.layer_norm_2 = nn.LayerNorm(config.hidden_size) # 定义多头注意力机制 self.attention = MultiHeadAttention(config) # 定义前馈神经网络 self.feed_forward = FeedForward(config) # 前向传播函数 def forward(self, x, mask=None): # 应用第一个层归一化 hidden_state = self.layer_norm_1(x) # 应用注意力机制，并将结果与输入进行残差连接 # 注意力机制的输出将与输入x相加，得到更新后的x x = x + self.attention(hidden_state, hidden_state, hidden_state, mask=mask) # 应用第二个层归一化 # 注意这里的self.layer_norm_2(x)实际上是对更新后的x进行归一化 hidden_state = self.layer_norm_2(x) # 应用前馈网络，并将结果与更新后的x进行残差连接 x = x + self.feed_forward(hidden_state) # 返回最终的输出x return x 代码验证 :\n1 2 3 4 5 encoder_layer = TransformerEncoderLayer(config) print(inputs_embeds.shape) print(encoder_layer(inputs_embeds).size()) #torch.Size([1, 5, 768]) #torch.Size([1, 5, 768]) 绝对位置编码 注意力机制无法捕获词语之间的位置信息，因此 Transformer 模型还使用 Positional Embeddings 添加了词语的位置信息。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 from torch import nn, LongTensor, arange # 定义Embeddings类，继承自nn.Module class Embeddings(nn.Module): # 初始化函数 def __init__(self, config): super().__init__() # 调用基类的初始化方法 # 定义词嵌入层，将词ID映射到词向量 self.token_embeddings = nn.Embedding(config.vocab_size, config.hidden_size) # 定义位置嵌入层，为序列中的每个位置生成一个唯一的位置向量 self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size) # 定义层归一化，用于稳定训练过程 self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12) # 定义Dropout层，用于防止过拟合 self.dropout = nn.Dropout(config.hidden_dropout_prob) # 前向传播函数 def forward(self, input_ids): # 根据输入序列的长度创建位置ID seq_length = input_ids.size(1) # 获取序列长度 position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0) # 创建位置ID序列 # 创建词嵌入和位置嵌入 token_embeddings = self.token_embeddings(input_ids) # 通过词嵌入层获取词嵌入 position_embeddings = self.position_embeddings(position_ids) # 通过位置嵌入层获取位置嵌入 # 将词嵌入和位置嵌入相加，得到最终的嵌入表示 embeddings = token_embeddings + position_embeddings # 应用层归一化 embeddings = self.layer_norm(embeddings) # 应用Dropout embeddings = self.dropout(embeddings) # 返回最终的嵌入表示 return embeddings # 创建Embeddings层的实例，并使用config配置 embedding_layer = Embeddings(config) # 使用embedding_layer处理输入的词ID，并打印输出的大小 # 这里假设inputs.input_ids是之前通过tokenizer得到的词ID序列 print(embedding_layer(inputs.input_ids).size()) #torch.Size([1, 5, 768]) Transformer Encoder 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 from torch import nn # 定义TransformerEncoder类，继承自nn.Module class TransformerEncoder(nn.Module): # 初始化函数 def __init__(self, config): super().__init__() # 调用基类的初始化方法 # 创建嵌入层实例，用于将输入的词ID转换为嵌入向量 self.embeddings = Embeddings(config) # 创建一个包含多个Transformer编码器层的列表 # num_hidden_layers表示编码器中隐藏层的数量 self.layers = nn.ModuleList([TransformerEncoderLayer(config) for _ in range(config.num_hidden_layers)]) # 前向传播函数 def forward(self, x, mask=None): # 首先通过嵌入层处理输入x x = self.embeddings(x) # 然后依次通过每个编码器层 for layer in self.layers: # 将当前层的输出作为下一层的输入，并传递掩码（如果有的话） x = layer(x, mask=mask) # 返回最终的编码器输出 return x 测试代码 :\n1 2 encoder = TransformerEncoder(config) print(encoder(inputs.input_ids).size()) #torch.Size([1, 5, 768]) 完整代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 import torch from torch import nn import torch.nn.functional as F from math import sqrt class AttentionHead(nn.Module): def __init__(self, embed_dim, head_dim): super().__init__() self.q = nn.Linear(embed_dim, head_dim) self.k = nn.Linear(embed_dim, head_dim) self.v = nn.Linear(embed_dim, head_dim) def forward(self, query, key, value, mask=None): query, key, value = self.q(query), self.k(key), self.v(value) scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(query.size(-1)) if mask is not None: scores = scores.masked_fill(mask == 0, -float(\"inf\")) weights = F.softmax(scores, dim=-1) return torch.bmm(weights, value) class MultiHeadAttention(nn.Module): def __init__(self, config): super().__init__() embed_dim = config.hidden_size num_heads = config.num_attention_heads head_dim = embed_dim // num_heads self.heads = nn.ModuleList( [AttentionHead(embed_dim, head_dim) for _ in range(num_heads)] ) self.output_linear = nn.Linear(embed_dim, embed_dim) def forward(self, query, key, value, mask=None, query_mask=None, key_mask=None): if query_mask is not None and key_mask is not None: mask = torch.bmm(query_mask.unsqueeze(-1), key_mask.unsqueeze(1)) x = torch.cat([h(query, key, value, mask) for h in self.heads], dim=-1) x = self.output_linear(x) return x class FeedForward(nn.Module): def __init__(self, config): super().__init__() self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size) self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size) self.gelu = nn.GELU() self.dropout = nn.Dropout(config.hidden_dropout_prob) def forward(self, x): x = self.linear_1(x) x = self.gelu(x) x = self.linear_2(x) x = self.dropout(x) return x class TransformerEncoderLayer(nn.Module): def __init__(self, config): super().__init__() self.layer_norm_1 = nn.LayerNorm(config.hidden_size) self.layer_norm_2 = nn.LayerNorm(config.hidden_size) self.attention = MultiHeadAttention(config) self.feed_forward = FeedForward(config) def forward(self, x, mask=None): # Apply layer normalization and then copy input into query, key, value hidden_state = self.layer_norm_1(x) # Apply attention with a skip connection x = x + self.attention(hidden_state, hidden_state, hidden_state, mask=mask) # Apply feed-forward layer with a skip connection x = x + self.feed_forward(self.layer_norm_2(x)) return x class Embeddings(nn.Module): def __init__(self, config): super().__init__() self.token_embeddings = nn.Embedding(config.vocab_size, config.hidden_size) self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size) self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12) self.dropout = nn.Dropout() def forward(self, input_ids): # Create position IDs for input sequence seq_length = input_ids.size(1) position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0) # Create token and position embeddings token_embeddings = self.token_embeddings(input_ids) position_embeddings = self.position_embeddings(position_ids) # Combine token and position embeddings embeddings = token_embeddings + position_embeddings embeddings = self.layer_norm(embeddings) embeddings = self.dropout(embeddings) return embeddings class TransformerEncoder(nn.Module): def __init__(self, config): super().__init__() self.embeddings = Embeddings(config) self.layers = nn.ModuleList( [TransformerEncoderLayer(config) for _ in range(config.num_hidden_layers)] ) def forward(self, x, mask=None): x = self.embeddings(x) for layer in self.layers: x = layer(x, mask) return x if __name__ == '__main__': from transformers import AutoConfig from transformers import AutoTokenizer model_ckpt = \"bert-base-uncased\"\\ cache_dir = './pretrained_model'\\ tokenizer = AutoTokenizer.from_pretrained(model_ckpt, cache_dir=cache_dir) config = AutoConfig.from_pretrained(model_ckpt) text = \"hello world\" inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False) encoder = TransformerEncoder(config) print(encoder(inputs.input_ids).size()) [!IMPORTANT]\n绝对位置编码 使用Pytorch实现Transformer中的绝对位置编码底层代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 #!/usr/bin/env python # -*- encoding: utf-8 -*- ''' @File : Positional_Encoding.py @Time : 2024/09/26 11:23:36 @Author : pan binghong @Email : 19909442097@163.com @description : Transformer中的绝对位置编码底层代码实现 ''' import torch import torch.nn as nn import math from transformers import BertTokenizer import os file_path = os.path.abspath(__file__) dir = os.path.dirname(file_path) os.chdir(dir) class PositionalEncoding(nn.Module): def __init__(self, d_model, max_len=5000): '''' :param d_model: 模型的维度 :param max_len: 序列的最大长度 ''' super(PositionalEncoding, self).__init__() # 创建一个形状为 (max_len, d_model) 的矩阵, 用于存储位置编码 pe = torch.zeros(max_len, d_model) # 创建一个形状为 (max_len, 1) 的矩阵, 用于存储位置信息, 保存索引值 e.g.[0, 1, 2, ... , max_len-1] position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # 这段代码计算位置编码中的两个频率。具体来说，它生成一个从0到d_model（不包括d_model）的偶数序列， # 然后将这些偶数转换为浮点数，并乘以一个常数因子 (-math.log(10000.0) / d_model)。 # 这个常数因子是通过对10000取自然对数并除以d_model得到的。 # 最后，通过torch.exp函数计算这些值的指数，得到最终的div_term。 div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # 应用正弦函数 得到偶数索引位置编码 pe[:, 0::2] = torch.sin(position * div_term) # 应用余弦函数 得到奇数索引位置编码 pe[:, 1::2] = torch.cos(position * div_term) # 增加一个 batch 维度, 使其能够与输入一起使用 pe = pe.unsqueeze(0) # 将位置编码矩阵注册为一个参数, 并将其添加到模型参数列表中 self.register_buffer('pe', pe) def forward(self, x): '''' :param x: 输入的序列张量, shape为: :return: 输出的序列张量, shape为: ''' x = x + self.pe[:, :x.size(1), :] return x if __name__ == '__main__': # 初始化参数 d_model = 512 max_len = 2048 # 初始化位置编码 pos_encoder = PositionalEncoding(d_model, max_len) # 初始化 tokenizer (这里以 Bert 为例) tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', cache_dir='./cache') # 使用输入\"hello world\" input_text = \"hello world\" input_ids = torch.tensor([tokenizer.encode(input_text, add_special_tokens=True)]) # 创建一个形状为 (1, seq_len, d_model) 的零矩阵 x = torch.zeros(1, input_ids.size(1), d_model) # 应用位置编码 output = pos_encoder(x) print(\"Input Text:\", input_text) print(\"Input IDs Shape:\", input_ids.shape) print(\"Output Shape:\", output.shape) print(\"Output:\", output) 以上案例为 : 当输入hello world, 经过编码后输出其对应的信息\n1 2 3 4 5 6 7 8 9 10 11 Input Text: hello world Input IDs Shape: torch.Size([1, 4]) Output Shape: torch.Size([1, 4, 512]) Output: tensor([[[ 0.0000e+00, 1.0000e+00, 0.0000e+00, ..., 1.0000e+00, 0.0000e+00, 1.0000e+00], [ 8.4147e-01, 5.4030e-01, 8.2186e-01, ..., 1.0000e+00, 1.0366e-04, 1.0000e+00], [ 9.0930e-01, -4.1615e-01, 9.3641e-01, ..., 1.0000e+00, 2.0733e-04, 1.0000e+00], [ 1.4112e-01, -9.8999e-01, 2.4509e-01, ..., 1.0000e+00, 3.1099e-04, 1.0000e+00]]]) 多头注意力机制 使用Pytorch实现多头注意力机制的底层代码, 含例子.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 #!/usr/bin/env python # -*- encoding: utf-8 -*- ''' @File : MultiHeadAttention.py @Time : 2024/09/27 09:50:43 @Author : pan binghong @Email : 19909442097@163.com @description : ''' import os import torch import torch.nn as nn from transformers import BertTokenizer # 获取当前文件的绝对路径 file_path = os.path.abspath(__file__) # 获取当前文件所在的目录路径 dir = os.path.dirname(file_path) # 将当前工作目录更改为当前文件所在的目录 os.chdir(dir) class MultiHeadAttention(nn.Module): def __init__(self, embed_size, heads): ''' 初始化多头注意力机制 :param embed_size: 输入嵌入向量的维度 :param num_heads: 多头注意力机制的头数 ''' super(MultiHeadAttention, self).__init__() assert embed_size % heads == 0, \"嵌入维度必须能被头数整除\" self.embed_size = embed_size self.heads = heads # 每个头的维度 self.heads_dim = embed_size // heads self.value = nn.Linear(self.heads_dim, self.heads_dim, bias=False) self.key = nn.Linear(self.heads_dim, self.heads_dim, bias=False) self.query = nn.Linear(self.heads_dim, self.heads_dim, bias=False) self.fc_out = nn.Linear(heads * self.heads_dim, embed_size) def forward(self, query, keys, values, mask): N = query.shape[0] value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1] # 将输入的数据划分为多个头, 先调整shape为 (N, value_len, heads, heads_dim) values = values.reshape(N, value_len, self.heads, self.heads_dim) keys = keys.reshape(N, key_len, self.heads, self.heads_dim) query = query.reshape(N, query_len, self.heads, self.heads_dim) # 初始化Q, K, V矩阵 values = self.value(values) keys = self.key(keys) query = self.query(query) # 计算注意力分数 # 这行代码使用爱因斯坦求和约定（Einstein Summation Convention）来计算注意力分数。 # 具体来说，它通过矩阵乘法计算query和keys之间的点积，然后将其重塑为所需的形状。 # # 参数解释： # - \"nqhd,nkhd-\u003enhqk\" 是爱因斯坦求和约定的字符串表示。 # - \"n\" 表示批次大小（batch size）。 # - \"q\" 表示查询序列的长度（query length）。 # - \"h\" 表示注意力头数（number of heads）。 # - \"d\" 表示每个头的维度（dimension per head）。 # - \"k\" 表示键序列的长度（key length）。 # # 具体操作： # - query 的形状为 (N, query_len, heads, heads_dim)。 # - keys 的形状为 (N, key_len, heads, heads_dim)。 # - 通过 torch.einsum 计算 query 和 keys 的点积，结果的形状为 (N, heads, query_len, key_len)。 # # 计算过程： # - 对于每个批次（n），每个头（h），计算 query 和 keys 的点积，得到一个形状为 (query_len, key_len) 的矩阵。 # - 最终结果是一个形状为 (N, heads, query_len, key_len) 的张量，表示每个查询和每个键之间的注意力分数。 energy = torch.einsum(\"nqhd,nkhd-\u003enhqk\", [query, keys]) # 应用注意力机制 if mask is not None: energy = energy.masked_fill(mask == 0, float('-1e20')) attention = torch.softmax(energy / (self.embed_size ** (1/2)), dim=3) out = torch.einsum(\"nhql,nlhd-\u003enqhd\", [attention, values]).reshape( N, query_len, self.heads * self.heads_dim ) out = self.fc_out(out) return out if __name__ == '__main__': tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', cache_dir='./cache') input_text = \"Hello world!\" # 初始化随机keys,values, query tokens = tokenizer.tokenize(input_text) token_ids = tokenizer.convert_tokens_to_ids(tokens) token_ids = torch.tensor([token_ids]).long() batch_size,seq_length = token_ids.shape embed_size =512 heads = 8 values = torch.rand(batch_size, seq_length, embed_size) keys = torch.rand(batch_size, seq_length, embed_size) query = torch.rand(batch_size, seq_length, embed_size) mask = None attention = MultiHeadAttention(embed_size, heads) out = attention(query, keys, values, mask) print(f\"Tokens: {tokens}\") print(f\"Token IDs: {token_ids}\") print(out.shape) print(f\"Multi-head Attention Output: \\n{out}\") 运行以上代码后输出 :\n1 2 3 4 5 6 7 8 Tokens: ['hello', 'world', '!'] Token IDs: tensor([[7592, 2088, 999]]) torch.Size([1, 3, 512]) Multi-head Attention Output: tensor([[[-0.0855, -0.2076, 0.0354, ..., 0.1631, -0.0024, -0.1372], [-0.0852, -0.2079, 0.0366, ..., 0.1636, -0.0025, -0.1374], [-0.0860, -0.2076, 0.0364, ..., 0.1644, -0.0018, -0.1361]]], grad_fn=\u003cViewBackward0\u003e) 前馈神经网络 就是一个简单的全连接层…没啥好说的, 看代码 :\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 #!/usr/bin/env python # -*- encoding: utf-8 -*- ''' @File : FeedForwardNetwork.py @Time : 2024/09/27 14:46:48 @Author : pan binghong @Email : 19909442097@163.com @description : ''' import os import torch import torch.nn as nn from transformers import BertTokenizer file_path = os.path.abspath(__file__) dir = os.path.dirname(file_path) os.chdir(dir) class FeedForwardNetwork(nn.Module): def __init__(self, d_model, hidden_size, dropout=0.1): ''' :param d_model:输入的特征维度大小 :param hidden_size:隐藏层大小 :param dropout:dropout概率 ''' super(FeedForwardNetwork, self).__init__() self.liner1 = nn.Linear(d_model, hidden_size) self.relu = nn.ReLU() self.liner2 = nn.Linear(hidden_size, d_model) self.dropout = nn.Dropout(dropout) def forward(self, x): ''' :param x:输入的特征 :return:输出的特征 ''' x = self.liner1(x) x = self.relu(x) x = self.liner2(x) x = self.dropout(x) return x if __name__ == '__main__': tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',cache_dir='./cache') input_text = \"Hello world\" tokens = tokenizer.tokenize(input_text) token_ids = tokenizer.convert_tokens_to_ids(tokens) token_ids = torch.tensor([token_ids]).long() d_model = token_ids.shape[1] hidden_size = 256 ff_netword = FeedForwardNetwork(d_model, hidden_size) output = ff_netword(token_ids.float()) print(f'Input tokens: \\n{tokens}') print(f'Input token_ids: \\n{token_ids}') print(f'Output from FeedForwardNetwork: \\n{output}') print(f'Output shape: \\n{output.shape}') 输出结果 :\n1 2 3 4 5 6 7 8 Input tokens: ['hello', 'world'] Input token_ids: tensor([[7592, 2088]]) Output from FeedForwardNetwork: tensor([[2332.0767, 1194.7576]], grad_fn=\u003cMulBackward0\u003e) Output shape: torch.Size([1, 2]) 层归一化 \u0026 残差连接 Layer Normalization\n代码如下 :\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 #!/usr/bin/env python # -*- encoding: utf-8 -*- ''' @File : LayerNormalization.py @Time : 2024/09/27 15:17:48 @Author : pan binghong @Email : 19909442097@163.com @description : ''' import torch import torch.nn as nn class LayerNormalization(nn.Module): def __init__(self, features, eps=1e-6): ''' 初始化层归一化模块 :param features: 特征维度大小 :param eps: 防止除零的小常数 ''' super(LayerNormalization, self).__init__() # 调用父类nn.Module的初始化方法 self.eps = eps # 设置防止除零的小常数 self.gain = nn.Parameter(torch.ones(features)) # 初始化增益参数，形状为(features,)，初始值为1 self.bias = nn.Parameter(torch.zeros(features)) # 初始化偏置参数，形状为(features,)，初始值为0 def forward(self, x): ''' 前向传播函数 :param x: 输入张量，形状为(batch_size, seq_len, features) :return: 归一化后的输出张量 ''' mean = x.mean(-1, keepdim=True) # 计算输入张量在最后一个维度上的均值，保持维度 std = x.std(-1, keepdim=True) # 计算输入张量在最后一个维度上的标准差，保持维度 return self.gain * (x - mean) / (std + self.eps) + self.bias # 应用层归一化公式并返回结果 if __name__ == \"__main__\": batch_size = 1 seq_len = 2048 features = 4096 # 创建一个简单的输入张量 x = torch.randn(batch_size, seq_len, features) # 随机初始化输入张量 # 初始化层归一化层 ln = LayerNormalization(features) # 应用层归一化 normalized_x = ln(x) # 打印原始和归一化后的张量 print(\"原始输入张量:\") print(x) print(\"\\n归一化后的输出张量:\") print(normalized_x) print(\"\\n归一化后的维度:\") print(normalized_x.shape) 输出 :\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 原始输入张量: tensor([[[ 0.7926, 0.9379, 0.9247, ..., -0.8849, 0.4262, -0.1126], [-1.0093, -0.3612, -0.7135, ..., 1.7116, -0.1164, 0.8453], [ 1.1906, -0.3223, 0.3553, ..., -0.0293, 0.6014, 0.3705], ..., [-0.3714, -0.1437, 1.2062, ..., -0.7603, -1.2689, -0.4045], [-0.6838, -0.5469, 0.2328, ..., -0.4500, -1.1035, -0.2005], [ 0.4267, -0.5163, 1.1316, ..., -0.1339, -0.4004, 0.2841]]]) 归一化后的输出张量: tensor([[[ 0.7959, 0.9418, 0.9285, ..., -0.8873, 0.4283, -0.1123], [-0.9919, -0.3548, -0.7011, ..., 1.6830, -0.1141, 0.8314], [ 1.2045, -0.3012, 0.3732, ..., -0.0096, 0.6181, 0.3883], ..., [-0.3827, -0.1589, 1.1681, ..., -0.7650, -1.2649, -0.4152], [-0.7084, -0.5709, 0.2130, ..., -0.4735, -1.1304, -0.2226], [ 0.3982, -0.5383, 1.0983, ..., -0.1585, -0.4232, 0.2567]]], grad_fn=\u003cAddBackward0\u003e) 归一化后的维度: torch.Size([1, 2048, 4096]) Residual Connections 残差连接代码实现 :\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 #!/usr/bin/env python # -*- encoding: utf-8 -*- ''' @File : ResidualConnection.py @Time : 2024/09/27 15:33:49 @Author : pan binghong @Email : 19909442097@163.com @description : ''' import torch import torch.nn as nn from LayerNormalization import LayerNormalization class ResidualConnection(nn.Module): def __init__(self, size, dropout): super(ResidualConnection, self).__init__() # 正确调用父类构造函数 self.norm = LayerNormalization(size) # 在父类构造函数之后设置属性 self.dropout = nn.Dropout(dropout) def forward(self, x, sublayer): return x + self.dropout(sublayer(self.norm(x))) if __name__ == '__main__': size = 512 dropout = 0.1 residual_module = ResidualConnection(size, dropout) x = torch.rand(32, 10, size) sublayer = nn.Linear(size, size) output = residual_module(x, sublayer) print(f'output shape: \\n{output.shape}') print(f'out: \\n{output}') 输出内容 :\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 output shape: torch.Size([32, 10, 512]) out: tensor([[[ 0.3520, 0.6030, -0.2354, ..., 0.8682, -0.3730, 0.1524], [ 1.4186, 0.5724, 0.2079, ..., 0.4792, 1.2186, 0.6546], [ 1.0631, 1.0479, 0.4523, ..., 1.4115, 1.5870, 0.7165], ..., [ 0.0567, 2.4177, 1.6159, ..., -0.5791, 0.4186, 0.6306], [ 1.9228, 0.8467, 0.7399, ..., -0.5388, -0.1463, 1.1880], [ 0.1197, 0.0182, 0.2941, ..., 0.5807, 0.3925, -0.4700]], [[ 0.3333, 0.8267, 0.3082, ..., 1.3075, 0.2646, 0.4092], [ 0.7507, 0.9554, 0.2910, ..., 1.1357, 0.4684, 0.4244], [ 0.6514, 0.2003, -0.3597, ..., 0.3598, 0.4869, -0.1992], ..., [-0.0030, 1.0668, 0.3075, ..., 0.5691, 0.2380, -0.1247], [ 0.9135, 0.4341, 0.0337, ..., 0.3597, -0.7784, 0.8458], [-0.1205, 0.6370, 1.0110, ..., 0.0136, 0.6965, 0.2374]], [[ 1.7201, 0.9184, 1.4459, ..., 0.6506, -0.4328, 0.4222], [ 0.1091, -0.7816, 0.0389, ..., 0.4128, 0.5077, 1.2345], [ 0.3344, 0.5098, 0.1903, ..., 0.4348, 0.1655, 1.0356], ..., [ 0.6577, 1.0894, 0.3509, ..., 0.9495, -0.3284, 0.8220], [-0.3985, 1.0728, 1.2246, ..., 0.6893, 0.3443, 0.4279], [ 0.2423, 0.0113, 1.3485, ..., -0.2677, -0.0577, 0.4010]], ..., [[ 1.4007, 1.3420, 0.1977, ..., 0.8533, 0.1814, 0.2697], [ 0.6700, 0.2914, 0.7087, ..., 0.4371, -0.6651, -0.3476], [ 1.7371, -0.3646, -0.7164, ..., -0.2941, 1.5312, 0.1496], ..., [ 1.4492, 1.6843, 0.4061, ..., 0.2602, 0.3412, 0.9145], [ 0.5208, 0.7262, 1.1507, ..., 1.6124, 0.1670, -0.7637], [ 0.6195, 1.8701, 0.6011, ..., 0.7136, 0.1405, 0.7195]], [[ 1.2362, 0.3252, 1.5944, ..., -0.6239, -0.5983, 0.0794], [ 1.1429, -0.8601, 0.6993, ..., 0.1767, -0.7440, 0.6210], [ 0.1337, -0.8456, -0.4567, ..., 1.0074, 0.6997, -0.7483], ..., [-0.2421, 0.8441, 1.1355, ..., 1.2241, 0.0689, -0.8084], [ 0.1203, -0.1496, -0.3044, ..., 0.2365, 1.0541, 0.5421], [ 0.7855, 0.0565, 0.9192, ..., 0.8071, -0.9707, 1.3335]], [[ 0.6022, 1.4715, 0.2470, ..., -0.0782, -0.6734, 0.8383], [ 0.8088, 0.3382, 0.6812, ..., 1.1501, 1.0537, 0.5442], [-0.0593, 1.7771, 0.0580, ..., -0.0578, 0.7382, 1.2158], ..., [ 1.1114, 0.3564, 0.8435, ..., 0.1796, 1.2682, 0.5146], [ 1.0304, 1.2170, 0.8374, ..., 2.2357, 0.2286, 0.3899], [ 0.5022, 0.3711, 0.2397, ..., 0.9505, 0.3877, -0.2048]]], grad_fn=\u003cAddBackward0\u003e) 编码器 Encoder Layer 编码器层由之前构建的多头注意力机制, 前馈神经网络, 残差模块, 层归一化组合构成\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 #!/usr/bin/env python # -*- encoding: utf-8 -*- ''' @File : EncoderLayer.py @Time : 2024/09/27 16:51:01 @Author : pan binghong @Email : 19909442097@163.com @description : 多头注意力机制、前馈神经网络、位置编码、残差连接和层归一化结合起来，\\n 构建一个 Encoder Layer。Encoder Layer 是 Transformer 编码器的基本组成单位。 ''' import torch import torch.nn as nn from MultiHeadAttention import MultiHeadAttention from FeedForwardNetwork import FeedForwardNetwork from LayerNormalization import LayerNormalization #在残差连接模块中完成 from ResidualConnection import ResidualConnection class EncoderLayer(nn.Module): def __init__(self, d_model, num_heads, hidden_size, dropout=0.1): super(EncoderLayer, self).__init__() self.d_model = d_model self.self_attn = MultiHeadAttention(d_model, num_heads) self.pos_ffn = FeedForwardNetwork(d_model, hidden_size, dropout) self.residual = nn.ModuleList([ ResidualConnection(d_model, dropout), ResidualConnection(d_model, dropout) ]) def forward(self, x, mask=None): x = self.residual[0](x, lambda x: self.self_attn(x, x, x, mask)) x = self.residual[1](x, self.pos_ffn) return x # 示例 if __name__ == \"__main__\": # 示例输入 d_model = 512 num_heads = 8 hidden_size = 2048 dropout = 0.1 batch_size = 32 seq_len = 10 encoder_layer = EncoderLayer(d_model, num_heads, hidden_size, dropout) input_tensor = torch.randn(batch_size, seq_len, d_model) mask = torch.ones(batch_size, 1,seq_len, seq_len) # 前向传播 output_tensor = encoder_layer(input_tensor, mask) # 输出结果 print(\"Input shape:\", input_tensor.shape) print(\"Output shape:\", output_tensor.shape) 输出结果为 :\n1 2 Input shape: torch.Size([32, 10, 512]) Output shape: torch.Size([32, 10, 512]) Encoder Encoder由n个Encoder_Lyaer组成, 详细代码如下 :\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 #!/usr/bin/env python # -*- encoding: utf-8 -*- ''' @File : Encoder.py @Time : 2024/09/29 08:39:15 @Author : pan binghong @Email : 19909442097@163.com @description : ''' import os import sys import torch import torch.nn as nn from EncoderLayer import EncoderLayer from LayerNormalization import LayerNormalization class Encoder(nn.Module): def __init__(self, encoder_layer, num_layers): super(Encoder, self).__init__() self.encoder_layer = nn.ModuleList([ encoder_layer for _ in range(num_layers) ]) self.norm = LayerNormalization(encoder_layer.d_model) def forward(self, src, mask=None): for layer in self.encoder_layer: src = layer(src, mask) return self.norm(src) if __name__ == '__main__': d_model = 512 num_heades = 8 hidden_size = 2048 droupout = 0.1 num_layers = 6 encoder_layer = EncoderLayer(d_model, num_heades, hidden_size, droupout) encoder = Encoder(encoder_layer, num_layers) src = torch.rand(32, 10 , d_model) output = encoder(src) print(output.shape) 输出结果为 :\n1 2 Input shape: torch.Size([32, 10, 512]) Output shape: torch.Size([32, 10, 512]) 32个样本 每个样本有10个时间步 每个时间步的特征向量有512个维度 解码器 Decoder_Layer 编码器与解码器最大的区别就是使用了Mask Multi-Head Attention, 用于防止模型训练过程中”看到”后续的目标词, 由多个解码器层构成, 代码如下 :\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 #!/usr/bin/env python # -*- encoding: utf-8 -*- ''' @File : DecoderLayer.py @Time : 2024/09/29 09:34:04 @Author : pan binghong @Email : 19909442097@163.com @description : Transformer 解码器与编码器类似，\\n 主要区别在于解码器使用了 Masked Multi-Head Attention，\\n 用于防止模型在训练过程中“看到”后续的目标词。\\n 解码器也是由多个 Decoder Layer 堆叠组成。 ''' import torch import torch.nn as nn from MultiHeadAttention import MultiHeadAttention from FeedForwardNetwork import FeedForwardNetwork from ResidualConnection import ResidualConnection class DecoderLayer(nn.Module): def __init__(self, d_model, heads, hidden_size, dropout=0.1): super(DecoderLayer, self).__init__() self.self_attn = MultiHeadAttention(d_model, heads) self.src_attn = MultiHeadAttention(d_model, heads) self.feed_forward = FeedForwardNetwork(d_model, hidden_size, dropout) self.residuals = nn.ModuleList([ ResidualConnection(d_model, dropout), ResidualConnection(d_model, dropout), ResidualConnection(d_model, dropout) ]) def forward(self, x, memory, src_mask=None, trg_mask=None): # 自注意力机制 x = self.residuals[0](x, lambda x: self.self_attn(x, x, x, trg_mask)) # 编码器-解码器注意力机制 x = self.residuals[1](x, lambda x: self.src_attn(x, memory, memory, src_mask)) # 前馈网络 return self.residuals[2](x, self.feed_forward) if __name__ == '__main__': embedding_size = 512 heads = 8 hidden_size = 2048 batch_size = 8 decoder_layer = DecoderLayer(embedding_size, heads, hidden_size) print(decoder_layer) print(decoder_layer.parameters) x = torch.rand(batch_size, 16, embedding_size) memory = torch.rand(batch_size, 16, embedding_size) src_mask = torch.rand(batch_size, 1, 16) trg_mask = torch.rand(batch_size, 16, 16) output = decoder_layer(x, memory, src_mask, trg_mask) print(output.shape) 输出结果如下 :\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 DecoderLayer( (self_attn): MultiHeadAttention( (value): Linear(in_features=64, out_features=64, bias=False) (key): Linear(in_features=64, out_features=64, bias=False) (query): Linear(in_features=64, out_features=64, bias=False) (fc_out): Linear(in_features=512, out_features=512, bias=True) ) (src_attn): MultiHeadAttention( (value): Linear(in_features=64, out_features=64, bias=False) (key): Linear(in_features=64, out_features=64, bias=False) (query): Linear(in_features=64, out_features=64, bias=False) (fc_out): Linear(in_features=512, out_features=512, bias=True) ) (feed_forward): FeedForwardNetwork( (liner1): Linear(in_features=512, out_features=2048, bias=True) (relu): ReLU() (liner2): Linear(in_features=2048, out_features=512, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (residuals): ModuleList( (0-2): 3 x ResidualConnection( (norm): LayerNormalization() (dropout): Dropout(p=0.1, inplace=False) ) ) ) \u003cbound method Module.parameters of DecoderLayer( (self_attn): MultiHeadAttention( (value): Linear(in_features=64, out_features=64, bias=False) (key): Linear(in_features=64, out_features=64, bias=False) (query): Linear(in_features=64, out_features=64, bias=False) (fc_out): Linear(in_features=512, out_features=512, bias=True) ) (src_attn): MultiHeadAttention( (value): Linear(in_features=64, out_features=64, bias=False) (key): Linear(in_features=64, out_features=64, bias=False) (query): Linear(in_features=64, out_features=64, bias=False) (fc_out): Linear(in_features=512, out_features=512, bias=True) ) (feed_forward): FeedForwardNetwork( (liner1): Linear(in_features=512, out_features=2048, bias=True) (relu): ReLU() (liner2): Linear(in_features=2048, out_features=512, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (residuals): ModuleList( (0-2): 3 x ResidualConnection( (norm): LayerNormalization() (dropout): Dropout(p=0.1, inplace=False) ) ) )\u003e torch.Size([8, 16, 512]) Decoder 代码报错解决中…\nReferences\n","wordCount":"4018","inLanguage":"en","datePublished":"2024-10-29T01:52:00Z","dateModified":"2025-06-19T03:45:00Z","author":{"@type":"Person","name":"Pan Binghong"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://Pan-Binghong.github.io/daily-learning/ai/%E5%9F%BA%E4%BA%8Epytorch%E6%9E%B6%E6%9E%84%E6%89%8B%E6%92%95transformer/"},"publisher":{"@type":"Organization","name":"我的博客","logo":{"@type":"ImageObject","url":"https://Pan-Binghong.github.io/daily-learning/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://Pan-Binghong.github.io/daily-learning/ accesskey=h title="我的博客 (Alt + H)">我的博客</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://Pan-Binghong.github.io/daily-learning/ title=首页><span>首页</span></a></li><li><a href=https://Pan-Binghong.github.io/daily-learning/ai/ title=AI><span>AI</span></a></li><li><a href=https://Pan-Binghong.github.io/daily-learning/knowledge/ title=知识库><span>知识库</span></a></li><li><a href=https://Pan-Binghong.github.io/daily-learning/backend/ title=后端><span>后端</span></a></li><li><a href=https://Pan-Binghong.github.io/daily-learning/devops/ title=DevOps><span>DevOps</span></a></li><li><a href=https://Pan-Binghong.github.io/daily-learning/other/ title=其他><span>其他</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">基于Pytorch架构手撕Transformer</h1><div class=post-meta><span title='2024-10-29 01:52:00 +0000 UTC'>October 29, 2024</span>&nbsp;·&nbsp;<span>Pan Binghong</span></div></header><div class=post-content><p>深入学习Tranformer模型结构, 本来依赖GPT4o, 自己快复现完了, 结果在封装Decoder层的时候, 发生了mask维度匹配不对的问题, 查了很多资料最终还是没解决, 后期再搞吧.</p><p><img loading=lazy src="https://prod-files-secure.s3.us-west-2.amazonaws.com/fc187c04-cf34-444f-b5f2-bdcdfad76660/5976f44e-a761-4511-a4fc-db008f73023d/image.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466YDZSRFXJ%2F20251106%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20251106T014210Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjENL%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJGMEQCID%2BZin7Pp0HQx5xDoimtZJxsWc5GM5hYbPIXD2FkVTnmAiBQE4qISJ4P4mkXEaZP%2B%2B6Sty8Sokm4wYhzZRnzc1GiZCqIBAia%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAAaDDYzNzQyMzE4MzgwNSIMlfYn%2FF0XO5PvzIIoKtwDY5nYlJ3NnPfRXuLRnYIpk8a5P65dP7l6jJs42v%2FNZnsJFdsYLhkYIQnJkKr9WkyU07kTor21Era4xH868pw4QJNF2%2FKFxFLX1AqPZa6g4T7CTlP88C%2Bv%2F4M3CEUrk3OGDC6oNkItlZt52ZC5T%2B5eMG0cjDsHIv0r3hQhPjRRC1r3zuspr1Z6mWvUsTzjNabrjThpjLaifvn%2BuX%2B9oybp3482SUnn%2FyA5fiLkybOlISj7pPyR7dkZjl%2Fk44WY918%2Bv4GsN1uS3hKN8MrbomqisYDmGRs%2BB%2FkBrwE9x0Vw%2FpCVYo2ggKVHboe%2B0RapiZV8lxsVlc8VJvQjLOBEQ9Rk7FmVGCz9zDg9hBIzH1oPAp82bW2q6nQJZnN3fa%2BS6CwTJ3Mxm%2FE57ESpa98wmFIEd7Kz7Isx7uBw7hEBiI3NRS7TTWf%2FOv4cQfB8%2FslmMO49xFHdtLIKGOvPiGI2%2FiTCmega1vCtMhBrP2pPvFqu1TomL4%2Fs6oN3kU4%2F2tCt0SMd1fa4eVQCfWNtBy4TBidpeuaUAYywHo0kb6J2SgEFJfrqQ1uNci6c1Z%2FGDsNbJyMU6btCgLw%2BG%2FiUFR0mn0OxAA6esjW%2B7UsJx%2B0KVN8%2BYCU6GoLFeFQaO4eGLQAwrPGvyAY6pgF8uAjgo0IzHuXvO5mtsCriGL3wtcfWX3DdMEoru7mvsQtiTVpbRnWC3CgzBw9F0xo6h%2B3m6mzFPRh5WEdp1%2FdCvz1qsOttH3P2Imx%2Fm62KTV%2BUGok4f7RCendNBrwRP5OOVEhSBgaqILURmeMxndY4tMK3smJrNtwUbz3NY84wwcQ%2F%2BIi2%2FCZffd52BVR00YFZYDYhekd71Whj7HWXXYn7qCP5lObg&X-Amz-Signature=138cc04fb396e889727879eb894b96fd60cf95f074c2e0eec3a55abe54bb0ea3&X-Amz-SignedHeaders=host&x-amz-checksum-mode=ENABLED&x-id=GetObject"></p><h2 id=transformer-的背景>Transformer 的背景<a hidden class=anchor aria-hidden=true href=#transformer-的背景>#</a></h2><ul><li>什么是Transformer ?</li><li>Transformer是谁提出的 ?</li><li>Transformer解决了什么问题 ?</li><li>Transformer核心组件有哪些 ?</li></ul><hr><h2 id=注意力机制>注意力机制<a hidden class=anchor aria-hidden=true href=#注意力机制>#</a></h2><ul><li>核心公式 :
<img loading=lazy src="https://prod-files-secure.s3.us-west-2.amazonaws.com/fc187c04-cf34-444f-b5f2-bdcdfad76660/7ccda5e2-2427-4a14-a91e-cd3a1ad7fc58/image.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466YDZSRFXJ%2F20251106%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20251106T014210Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjENL%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJGMEQCID%2BZin7Pp0HQx5xDoimtZJxsWc5GM5hYbPIXD2FkVTnmAiBQE4qISJ4P4mkXEaZP%2B%2B6Sty8Sokm4wYhzZRnzc1GiZCqIBAia%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAAaDDYzNzQyMzE4MzgwNSIMlfYn%2FF0XO5PvzIIoKtwDY5nYlJ3NnPfRXuLRnYIpk8a5P65dP7l6jJs42v%2FNZnsJFdsYLhkYIQnJkKr9WkyU07kTor21Era4xH868pw4QJNF2%2FKFxFLX1AqPZa6g4T7CTlP88C%2Bv%2F4M3CEUrk3OGDC6oNkItlZt52ZC5T%2B5eMG0cjDsHIv0r3hQhPjRRC1r3zuspr1Z6mWvUsTzjNabrjThpjLaifvn%2BuX%2B9oybp3482SUnn%2FyA5fiLkybOlISj7pPyR7dkZjl%2Fk44WY918%2Bv4GsN1uS3hKN8MrbomqisYDmGRs%2BB%2FkBrwE9x0Vw%2FpCVYo2ggKVHboe%2B0RapiZV8lxsVlc8VJvQjLOBEQ9Rk7FmVGCz9zDg9hBIzH1oPAp82bW2q6nQJZnN3fa%2BS6CwTJ3Mxm%2FE57ESpa98wmFIEd7Kz7Isx7uBw7hEBiI3NRS7TTWf%2FOv4cQfB8%2FslmMO49xFHdtLIKGOvPiGI2%2FiTCmega1vCtMhBrP2pPvFqu1TomL4%2Fs6oN3kU4%2F2tCt0SMd1fa4eVQCfWNtBy4TBidpeuaUAYywHo0kb6J2SgEFJfrqQ1uNci6c1Z%2FGDsNbJyMU6btCgLw%2BG%2FiUFR0mn0OxAA6esjW%2B7UsJx%2B0KVN8%2BYCU6GoLFeFQaO4eGLQAwrPGvyAY6pgF8uAjgo0IzHuXvO5mtsCriGL3wtcfWX3DdMEoru7mvsQtiTVpbRnWC3CgzBw9F0xo6h%2B3m6mzFPRh5WEdp1%2FdCvz1qsOttH3P2Imx%2Fm62KTV%2BUGok4f7RCendNBrwRP5OOVEhSBgaqILURmeMxndY4tMK3smJrNtwUbz3NY84wwcQ%2F%2BIi2%2FCZffd52BVR00YFZYDYhekd71Whj7HWXXYn7qCP5lObg&X-Amz-Signature=310ada84c517455ec081eabdf2be362c67e2cbd67478a8b44dc37d85cf8568ba&X-Amz-SignedHeaders=host&x-amz-checksum-mode=ENABLED&x-id=GetObject"></li></ul><p><img loading=lazy src="https://prod-files-secure.s3.us-west-2.amazonaws.com/fc187c04-cf34-444f-b5f2-bdcdfad76660/cb861907-c901-48ad-8055-a0b41d47106a/image.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466YDZSRFXJ%2F20251106%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20251106T014210Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjENL%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJGMEQCID%2BZin7Pp0HQx5xDoimtZJxsWc5GM5hYbPIXD2FkVTnmAiBQE4qISJ4P4mkXEaZP%2B%2B6Sty8Sokm4wYhzZRnzc1GiZCqIBAia%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAAaDDYzNzQyMzE4MzgwNSIMlfYn%2FF0XO5PvzIIoKtwDY5nYlJ3NnPfRXuLRnYIpk8a5P65dP7l6jJs42v%2FNZnsJFdsYLhkYIQnJkKr9WkyU07kTor21Era4xH868pw4QJNF2%2FKFxFLX1AqPZa6g4T7CTlP88C%2Bv%2F4M3CEUrk3OGDC6oNkItlZt52ZC5T%2B5eMG0cjDsHIv0r3hQhPjRRC1r3zuspr1Z6mWvUsTzjNabrjThpjLaifvn%2BuX%2B9oybp3482SUnn%2FyA5fiLkybOlISj7pPyR7dkZjl%2Fk44WY918%2Bv4GsN1uS3hKN8MrbomqisYDmGRs%2BB%2FkBrwE9x0Vw%2FpCVYo2ggKVHboe%2B0RapiZV8lxsVlc8VJvQjLOBEQ9Rk7FmVGCz9zDg9hBIzH1oPAp82bW2q6nQJZnN3fa%2BS6CwTJ3Mxm%2FE57ESpa98wmFIEd7Kz7Isx7uBw7hEBiI3NRS7TTWf%2FOv4cQfB8%2FslmMO49xFHdtLIKGOvPiGI2%2FiTCmega1vCtMhBrP2pPvFqu1TomL4%2Fs6oN3kU4%2F2tCt0SMd1fa4eVQCfWNtBy4TBidpeuaUAYywHo0kb6J2SgEFJfrqQ1uNci6c1Z%2FGDsNbJyMU6btCgLw%2BG%2FiUFR0mn0OxAA6esjW%2B7UsJx%2B0KVN8%2BYCU6GoLFeFQaO4eGLQAwrPGvyAY6pgF8uAjgo0IzHuXvO5mtsCriGL3wtcfWX3DdMEoru7mvsQtiTVpbRnWC3CgzBw9F0xo6h%2B3m6mzFPRh5WEdp1%2FdCvz1qsOttH3P2Imx%2Fm62KTV%2BUGok4f7RCendNBrwRP5OOVEhSBgaqILURmeMxndY4tMK3smJrNtwUbz3NY84wwcQ%2F%2BIi2%2FCZffd52BVR00YFZYDYhekd71Whj7HWXXYn7qCP5lObg&X-Amz-Signature=2c3648476d8533b164935aa3a609c2a4f4d5fe6de796cd7a0839e1c62b68202e&X-Amz-SignedHeaders=host&x-amz-checksum-mode=ENABLED&x-id=GetObject"></p><p><img loading=lazy src="https://prod-files-secure.s3.us-west-2.amazonaws.com/fc187c04-cf34-444f-b5f2-bdcdfad76660/2c6924da-ad53-4f28-b2c4-a2f769c588f2/image.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466YDZSRFXJ%2F20251106%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20251106T014210Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjENL%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJGMEQCID%2BZin7Pp0HQx5xDoimtZJxsWc5GM5hYbPIXD2FkVTnmAiBQE4qISJ4P4mkXEaZP%2B%2B6Sty8Sokm4wYhzZRnzc1GiZCqIBAia%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAAaDDYzNzQyMzE4MzgwNSIMlfYn%2FF0XO5PvzIIoKtwDY5nYlJ3NnPfRXuLRnYIpk8a5P65dP7l6jJs42v%2FNZnsJFdsYLhkYIQnJkKr9WkyU07kTor21Era4xH868pw4QJNF2%2FKFxFLX1AqPZa6g4T7CTlP88C%2Bv%2F4M3CEUrk3OGDC6oNkItlZt52ZC5T%2B5eMG0cjDsHIv0r3hQhPjRRC1r3zuspr1Z6mWvUsTzjNabrjThpjLaifvn%2BuX%2B9oybp3482SUnn%2FyA5fiLkybOlISj7pPyR7dkZjl%2Fk44WY918%2Bv4GsN1uS3hKN8MrbomqisYDmGRs%2BB%2FkBrwE9x0Vw%2FpCVYo2ggKVHboe%2B0RapiZV8lxsVlc8VJvQjLOBEQ9Rk7FmVGCz9zDg9hBIzH1oPAp82bW2q6nQJZnN3fa%2BS6CwTJ3Mxm%2FE57ESpa98wmFIEd7Kz7Isx7uBw7hEBiI3NRS7TTWf%2FOv4cQfB8%2FslmMO49xFHdtLIKGOvPiGI2%2FiTCmega1vCtMhBrP2pPvFqu1TomL4%2Fs6oN3kU4%2F2tCt0SMd1fa4eVQCfWNtBy4TBidpeuaUAYywHo0kb6J2SgEFJfrqQ1uNci6c1Z%2FGDsNbJyMU6btCgLw%2BG%2FiUFR0mn0OxAA6esjW%2B7UsJx%2B0KVN8%2BYCU6GoLFeFQaO4eGLQAwrPGvyAY6pgF8uAjgo0IzHuXvO5mtsCriGL3wtcfWX3DdMEoru7mvsQtiTVpbRnWC3CgzBw9F0xo6h%2B3m6mzFPRh5WEdp1%2FdCvz1qsOttH3P2Imx%2Fm62KTV%2BUGok4f7RCendNBrwRP5OOVEhSBgaqILURmeMxndY4tMK3smJrNtwUbz3NY84wwcQ%2F%2BIi2%2FCZffd52BVR00YFZYDYhekd71Whj7HWXXYn7qCP5lObg&X-Amz-Signature=5f17072b87da483cc931e89cefebfd5cc2cf8e228aeb9e8401fdc0470fb2260f&X-Amz-SignedHeaders=host&x-amz-checksum-mode=ENABLED&x-id=GetObject"></p><ul><li>论文原图 :
<img loading=lazy src="https://prod-files-secure.s3.us-west-2.amazonaws.com/fc187c04-cf34-444f-b5f2-bdcdfad76660/38c27c70-67d5-4420-bae3-bfb9ec5c7f30/image.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466YDZSRFXJ%2F20251106%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20251106T014210Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjENL%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJGMEQCID%2BZin7Pp0HQx5xDoimtZJxsWc5GM5hYbPIXD2FkVTnmAiBQE4qISJ4P4mkXEaZP%2B%2B6Sty8Sokm4wYhzZRnzc1GiZCqIBAia%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAAaDDYzNzQyMzE4MzgwNSIMlfYn%2FF0XO5PvzIIoKtwDY5nYlJ3NnPfRXuLRnYIpk8a5P65dP7l6jJs42v%2FNZnsJFdsYLhkYIQnJkKr9WkyU07kTor21Era4xH868pw4QJNF2%2FKFxFLX1AqPZa6g4T7CTlP88C%2Bv%2F4M3CEUrk3OGDC6oNkItlZt52ZC5T%2B5eMG0cjDsHIv0r3hQhPjRRC1r3zuspr1Z6mWvUsTzjNabrjThpjLaifvn%2BuX%2B9oybp3482SUnn%2FyA5fiLkybOlISj7pPyR7dkZjl%2Fk44WY918%2Bv4GsN1uS3hKN8MrbomqisYDmGRs%2BB%2FkBrwE9x0Vw%2FpCVYo2ggKVHboe%2B0RapiZV8lxsVlc8VJvQjLOBEQ9Rk7FmVGCz9zDg9hBIzH1oPAp82bW2q6nQJZnN3fa%2BS6CwTJ3Mxm%2FE57ESpa98wmFIEd7Kz7Isx7uBw7hEBiI3NRS7TTWf%2FOv4cQfB8%2FslmMO49xFHdtLIKGOvPiGI2%2FiTCmega1vCtMhBrP2pPvFqu1TomL4%2Fs6oN3kU4%2F2tCt0SMd1fa4eVQCfWNtBy4TBidpeuaUAYywHo0kb6J2SgEFJfrqQ1uNci6c1Z%2FGDsNbJyMU6btCgLw%2BG%2FiUFR0mn0OxAA6esjW%2B7UsJx%2B0KVN8%2BYCU6GoLFeFQaO4eGLQAwrPGvyAY6pgF8uAjgo0IzHuXvO5mtsCriGL3wtcfWX3DdMEoru7mvsQtiTVpbRnWC3CgzBw9F0xo6h%2B3m6mzFPRh5WEdp1%2FdCvz1qsOttH3P2Imx%2Fm62KTV%2BUGok4f7RCendNBrwRP5OOVEhSBgaqILURmeMxndY4tMK3smJrNtwUbz3NY84wwcQ%2F%2BIi2%2FCZffd52BVR00YFZYDYhekd71Whj7HWXXYn7qCP5lObg&X-Amz-Signature=38a2725602e83357cb78f4f19ca6f8e665712ed927490eee65ba0245832d7424&X-Amz-SignedHeaders=host&x-amz-checksum-mode=ENABLED&x-id=GetObject"></li></ul><h3 id=scaled-dot-product-attention>Scaled Dot-product Attention<a hidden class=anchor aria-hidden=true href=#scaled-dot-product-attention>#</a></h3><p><img loading=lazy src="https://prod-files-secure.s3.us-west-2.amazonaws.com/fc187c04-cf34-444f-b5f2-bdcdfad76660/90a969cd-6a3b-4923-bfd0-b511f94fd56d/image.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466YDZSRFXJ%2F20251106%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20251106T014210Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjENL%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJGMEQCID%2BZin7Pp0HQx5xDoimtZJxsWc5GM5hYbPIXD2FkVTnmAiBQE4qISJ4P4mkXEaZP%2B%2B6Sty8Sokm4wYhzZRnzc1GiZCqIBAia%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAAaDDYzNzQyMzE4MzgwNSIMlfYn%2FF0XO5PvzIIoKtwDY5nYlJ3NnPfRXuLRnYIpk8a5P65dP7l6jJs42v%2FNZnsJFdsYLhkYIQnJkKr9WkyU07kTor21Era4xH868pw4QJNF2%2FKFxFLX1AqPZa6g4T7CTlP88C%2Bv%2F4M3CEUrk3OGDC6oNkItlZt52ZC5T%2B5eMG0cjDsHIv0r3hQhPjRRC1r3zuspr1Z6mWvUsTzjNabrjThpjLaifvn%2BuX%2B9oybp3482SUnn%2FyA5fiLkybOlISj7pPyR7dkZjl%2Fk44WY918%2Bv4GsN1uS3hKN8MrbomqisYDmGRs%2BB%2FkBrwE9x0Vw%2FpCVYo2ggKVHboe%2B0RapiZV8lxsVlc8VJvQjLOBEQ9Rk7FmVGCz9zDg9hBIzH1oPAp82bW2q6nQJZnN3fa%2BS6CwTJ3Mxm%2FE57ESpa98wmFIEd7Kz7Isx7uBw7hEBiI3NRS7TTWf%2FOv4cQfB8%2FslmMO49xFHdtLIKGOvPiGI2%2FiTCmega1vCtMhBrP2pPvFqu1TomL4%2Fs6oN3kU4%2F2tCt0SMd1fa4eVQCfWNtBy4TBidpeuaUAYywHo0kb6J2SgEFJfrqQ1uNci6c1Z%2FGDsNbJyMU6btCgLw%2BG%2FiUFR0mn0OxAA6esjW%2B7UsJx%2B0KVN8%2BYCU6GoLFeFQaO4eGLQAwrPGvyAY6pgF8uAjgo0IzHuXvO5mtsCriGL3wtcfWX3DdMEoru7mvsQtiTVpbRnWC3CgzBw9F0xo6h%2B3m6mzFPRh5WEdp1%2FdCvz1qsOttH3P2Imx%2Fm62KTV%2BUGok4f7RCendNBrwRP5OOVEhSBgaqILURmeMxndY4tMK3smJrNtwUbz3NY84wwcQ%2F%2BIi2%2FCZffd52BVR00YFZYDYhekd71Whj7HWXXYn7qCP5lObg&X-Amz-Signature=77ac689a9ae944159f4a9b1479bb9c9d9a98a2b0e5c506c564b856211ece93b0&X-Amz-SignedHeaders=host&x-amz-checksum-mode=ENABLED&x-id=GetObject"></p><ul><li>代码实现 :</li></ul><h3 id=代码整合scaled-dot-product-attention>代码整合(Scaled Dot-product Attention)<a hidden class=anchor aria-hidden=true href=#代码整合scaled-dot-product-attention>#</a></h3><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">29
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">30
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch  <span style=color:#75715e># 导入PyTorch库</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn.functional <span style=color:#66d9ef>as</span> F  <span style=color:#75715e># 导入PyTorch的nn.functional模块，包含了许多神经网络操作的函数</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> math <span style=color:#f92672>import</span> sqrt  <span style=color:#75715e># 导入math库中的sqrt函数，用于计算平方根</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 定义Scaled Dot-product Attention函数</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>scaled_dot_product_attention</span>(query, key, value, query_mask<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>, key_mask<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>, mask<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>    <span style=color:#75715e># 获取查询（query）的最后一个维度大小，即键（key）的维度</span>
</span></span><span style=display:flex><span>    dim_k <span style=color:#f92672>=</span> query<span style=color:#f92672>.</span>size(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># 计算查询和键的点积，并缩放，得到未归一化的注意力分数</span>
</span></span><span style=display:flex><span>    scores <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>bmm(query, key<span style=color:#f92672>.</span>transpose(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>)) <span style=color:#f92672>/</span> sqrt(dim_k)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># 如果提供了查询掩码（query_mask）和键掩码（key_mask），则计算掩码矩阵</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> query_mask <span style=color:#f92672>is</span> <span style=color:#f92672>not</span> <span style=color:#66d9ef>None</span> <span style=color:#f92672>and</span> key_mask <span style=color:#f92672>is</span> <span style=color:#f92672>not</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        mask <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>bmm(query_mask<span style=color:#f92672>.</span>unsqueeze(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>), key_mask<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>        <span style=color:#75715e># 如果没有提供掩码，则使用之前传入的掩码（如果有的话）</span>
</span></span><span style=display:flex><span>        mask <span style=color:#f92672>=</span> mask
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># 如果存在掩码，则将分数矩阵中与掩码对应位置为0的分数替换为负无穷</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 这样在应用softmax时，这些位置的权重会接近于0</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> mask <span style=color:#f92672>is</span> <span style=color:#f92672>not</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        scores <span style=color:#f92672>=</span> scores<span style=color:#f92672>.</span>masked_fill(mask <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>, <span style=color:#f92672>-</span>float(<span style=color:#e6db74>&#34;inf&#34;</span>))
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># 使用softmax函数对分数进行归一化，得到注意力权重</span>
</span></span><span style=display:flex><span>    weights <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>softmax(scores, dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># 计算加权后的输出，即将注意力权重与值（value）相乘</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 这里的输出是经过注意力加权后的值向量，用于下游任务</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> torch<span style=color:#f92672>.</span>bmm(weights, value)
</span></span></code></pre></td></tr></table></div></div><h3 id=multi-head-attention>Multi-head Attention<a hidden class=anchor aria-hidden=true href=#multi-head-attention>#</a></h3><ul><li>Multi-head Attention作用 :
首先通过线性映射将 Q,K,V 序列映射到特征空间，每一组线性投影后的向量表示称为一个头 (head)，然后在每组映射后的序列上再应用 Scaled Dot-product Attention, 最后将每个头的向量拼接.
<img loading=lazy src="https://prod-files-secure.s3.us-west-2.amazonaws.com/fc187c04-cf34-444f-b5f2-bdcdfad76660/ab37c920-8ea8-4dd4-b3a6-655932f8727c/image.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466YDZSRFXJ%2F20251106%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20251106T014210Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjENL%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJGMEQCID%2BZin7Pp0HQx5xDoimtZJxsWc5GM5hYbPIXD2FkVTnmAiBQE4qISJ4P4mkXEaZP%2B%2B6Sty8Sokm4wYhzZRnzc1GiZCqIBAia%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAAaDDYzNzQyMzE4MzgwNSIMlfYn%2FF0XO5PvzIIoKtwDY5nYlJ3NnPfRXuLRnYIpk8a5P65dP7l6jJs42v%2FNZnsJFdsYLhkYIQnJkKr9WkyU07kTor21Era4xH868pw4QJNF2%2FKFxFLX1AqPZa6g4T7CTlP88C%2Bv%2F4M3CEUrk3OGDC6oNkItlZt52ZC5T%2B5eMG0cjDsHIv0r3hQhPjRRC1r3zuspr1Z6mWvUsTzjNabrjThpjLaifvn%2BuX%2B9oybp3482SUnn%2FyA5fiLkybOlISj7pPyR7dkZjl%2Fk44WY918%2Bv4GsN1uS3hKN8MrbomqisYDmGRs%2BB%2FkBrwE9x0Vw%2FpCVYo2ggKVHboe%2B0RapiZV8lxsVlc8VJvQjLOBEQ9Rk7FmVGCz9zDg9hBIzH1oPAp82bW2q6nQJZnN3fa%2BS6CwTJ3Mxm%2FE57ESpa98wmFIEd7Kz7Isx7uBw7hEBiI3NRS7TTWf%2FOv4cQfB8%2FslmMO49xFHdtLIKGOvPiGI2%2FiTCmega1vCtMhBrP2pPvFqu1TomL4%2Fs6oN3kU4%2F2tCt0SMd1fa4eVQCfWNtBy4TBidpeuaUAYywHo0kb6J2SgEFJfrqQ1uNci6c1Z%2FGDsNbJyMU6btCgLw%2BG%2FiUFR0mn0OxAA6esjW%2B7UsJx%2B0KVN8%2BYCU6GoLFeFQaO4eGLQAwrPGvyAY6pgF8uAjgo0IzHuXvO5mtsCriGL3wtcfWX3DdMEoru7mvsQtiTVpbRnWC3CgzBw9F0xo6h%2B3m6mzFPRh5WEdp1%2FdCvz1qsOttH3P2Imx%2Fm62KTV%2BUGok4f7RCendNBrwRP5OOVEhSBgaqILURmeMxndY4tMK3smJrNtwUbz3NY84wwcQ%2F%2BIi2%2FCZffd52BVR00YFZYDYhekd71Whj7HWXXYn7qCP5lObg&X-Amz-Signature=917d16816dbbda7231be25f670ee03cdda28a1e0a624d2209d7315d2b0f8f3be&X-Amz-SignedHeaders=host&x-amz-checksum-mode=ENABLED&x-id=GetObject"></li></ul><h3 id=代码整合multi-head-attention>代码整合(Multi-head Attention)<a hidden class=anchor aria-hidden=true href=#代码整合multi-head-attention>#</a></h3><ul><li>代码实现 :</li><li>验证代码</li></ul><hr><h2 id=前馈神经网络>前馈神经网络<a hidden class=anchor aria-hidden=true href=#前馈神经网络>#</a></h2><h3 id=the-feed-forward-layer>The Feed-Forward Layer<a hidden class=anchor aria-hidden=true href=#the-feed-forward-layer>#</a></h3><p>没啥好写的, 就是普通的全连接 + 激活函数</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">28
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> torch <span style=color:#f92672>import</span> nn
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 定义FeedForward类，继承自nn.Module</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>FeedForward</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#75715e># 初始化函数</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, config):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span><span style=color:#a6e22e>__init__</span>()  <span style=color:#75715e># 调用基类的初始化方法</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 定义第一个线性层，将输入的隐藏状态映射到中间维度</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>linear_1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(config<span style=color:#f92672>.</span>hidden_size, config<span style=color:#f92672>.</span>intermediate_size)
</span></span><span style=display:flex><span>        <span style=color:#75715e># 定义第二个线性层，将中间维度的表示映射回原始的隐藏状态维度</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>linear_2 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(config<span style=color:#f92672>.</span>intermediate_size, config<span style=color:#f92672>.</span>hidden_size)
</span></span><span style=display:flex><span>        <span style=color:#75715e># 定义GELU激活函数</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>gelu <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>GELU()
</span></span><span style=display:flex><span>        <span style=color:#75715e># 定义Dropout层，用于防止过拟合</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>dropout <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Dropout(config<span style=color:#f92672>.</span>hidden_dropout_prob)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 前向传播函数</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        <span style=color:#75715e># 应用第一个线性层</span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>linear_1(x)
</span></span><span style=display:flex><span>        <span style=color:#75715e># 应用GELU激活函数</span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>gelu(x)
</span></span><span style=display:flex><span>        <span style=color:#75715e># 应用第二个线性层</span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>linear_2(x)
</span></span><span style=display:flex><span>        <span style=color:#75715e># 应用Dropout</span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>dropout(x)
</span></span><span style=display:flex><span>        <span style=color:#75715e># 返回最终的输出</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span></code></pre></td></tr></table></div></div><p>与上面构建的注意力机制串联测试 :</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>feed_forward <span style=color:#f92672>=</span> FeedForward(config)
</span></span><span style=display:flex><span>ff_outputs <span style=color:#f92672>=</span> feed_forward(attn_output)
</span></span><span style=display:flex><span>print(ff_outputs<span style=color:#f92672>.</span>size()) <span style=color:#75715e>#torch.Size([1, 2, 768])</span>
</span></span></code></pre></td></tr></table></div></div><hr><h2 id=层归一化--残差连接>层归一化 & 残差连接<a hidden class=anchor aria-hidden=true href=#层归一化--残差连接>#</a></h2><ul><li>层归一化模块需要包含在残差模块内, 主要作用为 : 将输入的一批向量, 每一个都做标准化处理, 处理为 : 均值为零, 且有单位方差</li><li>残差连接主要作用为 : 是通过直接将输入绕过中间层的计算，帮助模型更容易训练深层网络，避免梯度消失问题并促进信息流动</li></ul><h3 id=transformer-encoder-layer>Transformer Encoder Layer<a hidden class=anchor aria-hidden=true href=#transformer-encoder-layer>#</a></h3><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">29
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">30
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> torch <span style=color:#f92672>import</span> nn
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 定义TransformerEncoderLayer类，继承自nn.Module</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>TransformerEncoderLayer</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#75715e># 初始化函数</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, config):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span><span style=color:#a6e22e>__init__</span>()  <span style=color:#75715e># 调用基类的初始化方法</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 定义第一个层归一化，用于注意力机制之前</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>layer_norm_1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>LayerNorm(config<span style=color:#f92672>.</span>hidden_size)
</span></span><span style=display:flex><span>        <span style=color:#75715e># 定义第二个层归一化，用于前馈网络之前</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>layer_norm_2 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>LayerNorm(config<span style=color:#f92672>.</span>hidden_size)
</span></span><span style=display:flex><span>        <span style=color:#75715e># 定义多头注意力机制</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>attention <span style=color:#f92672>=</span> MultiHeadAttention(config)
</span></span><span style=display:flex><span>        <span style=color:#75715e># 定义前馈神经网络</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>feed_forward <span style=color:#f92672>=</span> FeedForward(config)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 前向传播函数</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x, mask<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>        <span style=color:#75715e># 应用第一个层归一化</span>
</span></span><span style=display:flex><span>        hidden_state <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>layer_norm_1(x)
</span></span><span style=display:flex><span>        <span style=color:#75715e># 应用注意力机制，并将结果与输入进行残差连接</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 注意力机制的输出将与输入x相加，得到更新后的x</span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> x <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>attention(hidden_state, hidden_state, hidden_state, mask<span style=color:#f92672>=</span>mask)
</span></span><span style=display:flex><span>        <span style=color:#75715e># 应用第二个层归一化</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 注意这里的self.layer_norm_2(x)实际上是对更新后的x进行归一化</span>
</span></span><span style=display:flex><span>        hidden_state <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>layer_norm_2(x)
</span></span><span style=display:flex><span>        <span style=color:#75715e># 应用前馈网络，并将结果与更新后的x进行残差连接</span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> x <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>feed_forward(hidden_state)
</span></span><span style=display:flex><span>        <span style=color:#75715e># 返回最终的输出x</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span></code></pre></td></tr></table></div></div><p>代码验证 :</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">5
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>encoder_layer <span style=color:#f92672>=</span> TransformerEncoderLayer(config)
</span></span><span style=display:flex><span>print(inputs_embeds<span style=color:#f92672>.</span>shape)
</span></span><span style=display:flex><span>print(encoder_layer(inputs_embeds)<span style=color:#f92672>.</span>size())
</span></span><span style=display:flex><span><span style=color:#75715e>#torch.Size([1, 5, 768])</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#torch.Size([1, 5, 768])</span>
</span></span></code></pre></td></tr></table></div></div><hr><h2 id=绝对位置编码>绝对位置编码<a hidden class=anchor aria-hidden=true href=#绝对位置编码>#</a></h2><p>注意力机制无法捕获词语之间的位置信息，因此 Transformer 模型还使用 Positional Embeddings 添加了词语的位置信息。</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">29
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">30
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">31
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">32
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">33
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">34
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">35
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">36
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">37
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">38
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">39
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> torch <span style=color:#f92672>import</span> nn, LongTensor, arange
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 定义Embeddings类，继承自nn.Module</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Embeddings</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#75715e># 初始化函数</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, config):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span><span style=color:#a6e22e>__init__</span>()  <span style=color:#75715e># 调用基类的初始化方法</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 定义词嵌入层，将词ID映射到词向量</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>token_embeddings <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Embedding(config<span style=color:#f92672>.</span>vocab_size, config<span style=color:#f92672>.</span>hidden_size)
</span></span><span style=display:flex><span>        <span style=color:#75715e># 定义位置嵌入层，为序列中的每个位置生成一个唯一的位置向量</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>position_embeddings <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Embedding(config<span style=color:#f92672>.</span>max_position_embeddings, config<span style=color:#f92672>.</span>hidden_size)
</span></span><span style=display:flex><span>        <span style=color:#75715e># 定义层归一化，用于稳定训练过程</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>layer_norm <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>LayerNorm(config<span style=color:#f92672>.</span>hidden_size, eps<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-12</span>)
</span></span><span style=display:flex><span>        <span style=color:#75715e># 定义Dropout层，用于防止过拟合</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>dropout <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Dropout(config<span style=color:#f92672>.</span>hidden_dropout_prob)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 前向传播函数</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, input_ids):
</span></span><span style=display:flex><span>        <span style=color:#75715e># 根据输入序列的长度创建位置ID</span>
</span></span><span style=display:flex><span>        seq_length <span style=color:#f92672>=</span> input_ids<span style=color:#f92672>.</span>size(<span style=color:#ae81ff>1</span>)  <span style=color:#75715e># 获取序列长度</span>
</span></span><span style=display:flex><span>        position_ids <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>arange(seq_length, dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>long)<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>0</span>)  <span style=color:#75715e># 创建位置ID序列</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 创建词嵌入和位置嵌入</span>
</span></span><span style=display:flex><span>        token_embeddings <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>token_embeddings(input_ids)  <span style=color:#75715e># 通过词嵌入层获取词嵌入</span>
</span></span><span style=display:flex><span>        position_embeddings <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>position_embeddings(position_ids)  <span style=color:#75715e># 通过位置嵌入层获取位置嵌入</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 将词嵌入和位置嵌入相加，得到最终的嵌入表示</span>
</span></span><span style=display:flex><span>        embeddings <span style=color:#f92672>=</span> token_embeddings <span style=color:#f92672>+</span> position_embeddings
</span></span><span style=display:flex><span>        <span style=color:#75715e># 应用层归一化</span>
</span></span><span style=display:flex><span>        embeddings <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>layer_norm(embeddings)
</span></span><span style=display:flex><span>        <span style=color:#75715e># 应用Dropout</span>
</span></span><span style=display:flex><span>        embeddings <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>dropout(embeddings)
</span></span><span style=display:flex><span>        <span style=color:#75715e># 返回最终的嵌入表示</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> embeddings
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 创建Embeddings层的实例，并使用config配置</span>
</span></span><span style=display:flex><span>embedding_layer <span style=color:#f92672>=</span> Embeddings(config)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 使用embedding_layer处理输入的词ID，并打印输出的大小</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 这里假设inputs.input_ids是之前通过tokenizer得到的词ID序列</span>
</span></span><span style=display:flex><span>print(embedding_layer(inputs<span style=color:#f92672>.</span>input_ids)<span style=color:#f92672>.</span>size()) <span style=color:#75715e>#torch.Size([1, 5, 768])</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=transformer-encoder>Transformer Encoder<a hidden class=anchor aria-hidden=true href=#transformer-encoder>#</a></h3><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">23
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> torch <span style=color:#f92672>import</span> nn
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 定义TransformerEncoder类，继承自nn.Module</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>TransformerEncoder</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#75715e># 初始化函数</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, config):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span><span style=color:#a6e22e>__init__</span>()  <span style=color:#75715e># 调用基类的初始化方法</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 创建嵌入层实例，用于将输入的词ID转换为嵌入向量</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>embeddings <span style=color:#f92672>=</span> Embeddings(config)
</span></span><span style=display:flex><span>        <span style=color:#75715e># 创建一个包含多个Transformer编码器层的列表</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># num_hidden_layers表示编码器中隐藏层的数量</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>layers <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>ModuleList([TransformerEncoderLayer(config) <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(config<span style=color:#f92672>.</span>num_hidden_layers)])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 前向传播函数</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x, mask<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>        <span style=color:#75715e># 首先通过嵌入层处理输入x</span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>embeddings(x)
</span></span><span style=display:flex><span>        <span style=color:#75715e># 然后依次通过每个编码器层</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> layer <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>layers:
</span></span><span style=display:flex><span>            <span style=color:#75715e># 将当前层的输出作为下一层的输入，并传递掩码（如果有的话）</span>
</span></span><span style=display:flex><span>            x <span style=color:#f92672>=</span> layer(x, mask<span style=color:#f92672>=</span>mask)
</span></span><span style=display:flex><span>        <span style=color:#75715e># 返回最终的编码器输出</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span></code></pre></td></tr></table></div></div><p>测试代码 :</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>encoder <span style=color:#f92672>=</span> TransformerEncoder(config)
</span></span><span style=display:flex><span>print(encoder(inputs<span style=color:#f92672>.</span>input_ids)<span style=color:#f92672>.</span>size())  <span style=color:#75715e>#torch.Size([1, 5, 768])</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=完整代码>完整代码<a hidden class=anchor aria-hidden=true href=#完整代码>#</a></h2><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">  1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">  2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">  3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">  4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">  5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">  6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">  7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">  8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">  9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 29
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 30
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 31
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 32
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 33
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 34
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 35
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 36
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 37
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 38
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 39
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 40
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 41
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 42
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 43
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 44
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 45
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 46
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 47
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 48
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 49
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 50
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 51
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 52
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 53
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 54
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 55
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 56
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 57
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 58
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 59
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 60
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 61
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 62
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 63
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 64
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 65
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 66
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 67
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 68
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 69
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 70
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 71
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 72
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 73
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 74
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 75
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 76
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 77
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 78
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 79
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 80
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 81
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 82
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 83
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 84
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 85
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 86
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 87
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 88
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 89
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 90
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 91
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 92
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 93
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 94
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 95
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 96
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 97
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 98
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 99
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">100
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">101
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">102
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">103
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">104
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">105
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">106
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">107
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">108
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">109
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">110
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">111
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">112
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">113
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">114
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">115
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">116
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">117
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">118
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">119
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">120
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">121
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torch <span style=color:#f92672>import</span> nn
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn.functional <span style=color:#66d9ef>as</span> F
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> math <span style=color:#f92672>import</span> sqrt
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>AttentionHead</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, embed_dim, head_dim):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span><span style=color:#a6e22e>__init__</span>()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>q <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(embed_dim, head_dim)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>k <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(embed_dim, head_dim)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>v <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(embed_dim, head_dim)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, query, key, value, mask<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>        query, key, value <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>q(query), self<span style=color:#f92672>.</span>k(key), self<span style=color:#f92672>.</span>v(value)
</span></span><span style=display:flex><span>        scores <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>bmm(query, key<span style=color:#f92672>.</span>transpose(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>)) <span style=color:#f92672>/</span> sqrt(query<span style=color:#f92672>.</span>size(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> mask <span style=color:#f92672>is</span> <span style=color:#f92672>not</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>            scores <span style=color:#f92672>=</span> scores<span style=color:#f92672>.</span>masked_fill(mask <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>, <span style=color:#f92672>-</span>float(<span style=color:#e6db74>&#34;inf&#34;</span>))
</span></span><span style=display:flex><span>        weights <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>softmax(scores, dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> torch<span style=color:#f92672>.</span>bmm(weights, value)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>MultiHeadAttention</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, config):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span><span style=color:#a6e22e>__init__</span>()
</span></span><span style=display:flex><span>        embed_dim <span style=color:#f92672>=</span> config<span style=color:#f92672>.</span>hidden_size
</span></span><span style=display:flex><span>        num_heads <span style=color:#f92672>=</span> config<span style=color:#f92672>.</span>num_attention_heads
</span></span><span style=display:flex><span>        head_dim <span style=color:#f92672>=</span> embed_dim <span style=color:#f92672>//</span> num_heads
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>heads <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>ModuleList(
</span></span><span style=display:flex><span>            [AttentionHead(embed_dim, head_dim) <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(num_heads)]
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>output_linear <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(embed_dim, embed_dim)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, query, key, value, mask<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>, query_mask<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>, key_mask<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> query_mask <span style=color:#f92672>is</span> <span style=color:#f92672>not</span> <span style=color:#66d9ef>None</span> <span style=color:#f92672>and</span> key_mask <span style=color:#f92672>is</span> <span style=color:#f92672>not</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>            mask <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>bmm(query_mask<span style=color:#f92672>.</span>unsqueeze(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>), key_mask<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>cat([h(query, key, value, mask) <span style=color:#66d9ef>for</span> h <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>heads], dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>output_linear(x)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>FeedForward</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, config):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span><span style=color:#a6e22e>__init__</span>()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>linear_1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(config<span style=color:#f92672>.</span>hidden_size, config<span style=color:#f92672>.</span>intermediate_size)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>linear_2 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(config<span style=color:#f92672>.</span>intermediate_size, config<span style=color:#f92672>.</span>hidden_size)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>gelu <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>GELU()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>dropout <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Dropout(config<span style=color:#f92672>.</span>hidden_dropout_prob)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>linear_1(x)
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>gelu(x)
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>linear_2(x)
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>dropout(x)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>TransformerEncoderLayer</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, config):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span><span style=color:#a6e22e>__init__</span>()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>layer_norm_1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>LayerNorm(config<span style=color:#f92672>.</span>hidden_size)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>layer_norm_2 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>LayerNorm(config<span style=color:#f92672>.</span>hidden_size)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>attention <span style=color:#f92672>=</span> MultiHeadAttention(config)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>feed_forward <span style=color:#f92672>=</span> FeedForward(config)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x, mask<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>        <span style=color:#75715e># Apply layer normalization and then copy input into query, key, value</span>
</span></span><span style=display:flex><span>        hidden_state <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>layer_norm_1(x)
</span></span><span style=display:flex><span>        <span style=color:#75715e># Apply attention with a skip connection</span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> x <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>attention(hidden_state, hidden_state, hidden_state, mask<span style=color:#f92672>=</span>mask)
</span></span><span style=display:flex><span>        <span style=color:#75715e># Apply feed-forward layer with a skip connection</span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> x <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>feed_forward(self<span style=color:#f92672>.</span>layer_norm_2(x))
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Embeddings</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, config):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span><span style=color:#a6e22e>__init__</span>()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>token_embeddings <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Embedding(config<span style=color:#f92672>.</span>vocab_size,
</span></span><span style=display:flex><span>                                             config<span style=color:#f92672>.</span>hidden_size)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>position_embeddings <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Embedding(config<span style=color:#f92672>.</span>max_position_embeddings,
</span></span><span style=display:flex><span>                                                config<span style=color:#f92672>.</span>hidden_size)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>layer_norm <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>LayerNorm(config<span style=color:#f92672>.</span>hidden_size, eps<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-12</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>dropout <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Dropout()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, input_ids):
</span></span><span style=display:flex><span>        <span style=color:#75715e># Create position IDs for input sequence</span>
</span></span><span style=display:flex><span>        seq_length <span style=color:#f92672>=</span> input_ids<span style=color:#f92672>.</span>size(<span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        position_ids <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>arange(seq_length, dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>long)<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>        <span style=color:#75715e># Create token and position embeddings</span>
</span></span><span style=display:flex><span>        token_embeddings <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>token_embeddings(input_ids)
</span></span><span style=display:flex><span>        position_embeddings <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>position_embeddings(position_ids)
</span></span><span style=display:flex><span>        <span style=color:#75715e># Combine token and position embeddings</span>
</span></span><span style=display:flex><span>        embeddings <span style=color:#f92672>=</span> token_embeddings <span style=color:#f92672>+</span> position_embeddings
</span></span><span style=display:flex><span>        embeddings <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>layer_norm(embeddings)
</span></span><span style=display:flex><span>        embeddings <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>dropout(embeddings)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> embeddings
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>TransformerEncoder</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, config):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span><span style=color:#a6e22e>__init__</span>()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>embeddings <span style=color:#f92672>=</span> Embeddings(config)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>layers <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>ModuleList(
</span></span><span style=display:flex><span>            [TransformerEncoderLayer(config) <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(config<span style=color:#f92672>.</span>num_hidden_layers)]
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x, mask<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>embeddings(x)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> layer <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>layers:
</span></span><span style=display:flex><span>            x <span style=color:#f92672>=</span> layer(x, mask)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;__main__&#39;</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> AutoConfig
</span></span><span style=display:flex><span>    <span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> AutoTokenizer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    model_ckpt <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;bert-base-uncased&#34;</span>\
</span></span><span style=display:flex><span>    cache_dir <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;./pretrained_model&#39;</span>\
</span></span><span style=display:flex><span>    tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(model_ckpt, cache_dir<span style=color:#f92672>=</span>cache_dir)
</span></span><span style=display:flex><span>    config <span style=color:#f92672>=</span> AutoConfig<span style=color:#f92672>.</span>from_pretrained(model_ckpt)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    text <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;hello world&#34;</span>
</span></span><span style=display:flex><span>    inputs <span style=color:#f92672>=</span> tokenizer(text, return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;pt&#34;</span>, add_special_tokens<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    encoder <span style=color:#f92672>=</span> TransformerEncoder(config)
</span></span><span style=display:flex><span>    print(encoder(inputs<span style=color:#f92672>.</span>input_ids)<span style=color:#f92672>.</span>size())
</span></span></code></pre></td></tr></table></div></div><hr><blockquote><p>[!IMPORTANT]</p></blockquote><h2 id=绝对位置编码-1>绝对位置编码<a hidden class=anchor aria-hidden=true href=#绝对位置编码-1>#</a></h2><p>使用Pytorch实现Transformer中的绝对位置编码底层代码</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">29
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">30
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">31
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">32
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">33
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">34
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">35
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">36
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">37
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">38
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">39
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">40
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">41
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">42
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">43
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">44
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">45
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">46
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">47
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">48
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">49
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">50
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">51
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">52
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">53
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">54
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">55
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">56
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">57
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">58
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">59
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">60
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">61
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">62
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">63
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">64
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">65
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">66
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">67
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">68
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">69
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">70
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">71
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">72
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">73
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">74
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">75
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">76
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">77
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">78
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">79
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">80
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">81
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">82
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">83
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">84
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">85
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">86
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>#!/usr/bin/env python</span>
</span></span><span style=display:flex><span><span style=color:#75715e># -*- encoding: utf-8 -*-</span>
</span></span><span style=display:flex><span><span style=color:#e6db74>&#39;&#39;&#39;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>@File    :   Positional_Encoding.py
</span></span></span><span style=display:flex><span><span style=color:#e6db74>@Time    :   2024/09/26 11:23:36
</span></span></span><span style=display:flex><span><span style=color:#e6db74>@Author  :   pan binghong 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>@Email   :   19909442097@163.com
</span></span></span><span style=display:flex><span><span style=color:#e6db74>@description   :   Transformer中的绝对位置编码底层代码实现
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&#39;&#39;&#39;</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> math
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> BertTokenizer
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>file_path <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>abspath(__file__)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>dir <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>dirname(file_path)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>os<span style=color:#f92672>.</span>chdir(dir)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>PositionalEncoding</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, d_model, max_len<span style=color:#f92672>=</span><span style=color:#ae81ff>5000</span>):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;&#39;&#39;&#39;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        :param d_model: 模型的维度
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        :param max_len: 序列的最大长度
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#39;&#39;&#39;</span>
</span></span><span style=display:flex><span>        super(PositionalEncoding, self)<span style=color:#f92672>.</span><span style=color:#a6e22e>__init__</span>()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 创建一个形状为 (max_len, d_model) 的矩阵, 用于存储位置编码</span>
</span></span><span style=display:flex><span>        pe <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>zeros(max_len, d_model)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 创建一个形状为 (max_len, 1) 的矩阵, 用于存储位置信息, 保存索引值 e.g.[0, 1, 2, ... , max_len-1]</span>
</span></span><span style=display:flex><span>        position <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>0</span>, max_len, dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>float)<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 这段代码计算位置编码中的两个频率。具体来说，它生成一个从0到d_model（不包括d_model）的偶数序列，</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 然后将这些偶数转换为浮点数，并乘以一个常数因子 (-math.log(10000.0) / d_model)。</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 这个常数因子是通过对10000取自然对数并除以d_model得到的。</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 最后，通过torch.exp函数计算这些值的指数，得到最终的div_term。</span>
</span></span><span style=display:flex><span>        div_term <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>exp(torch<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>0</span>, d_model, <span style=color:#ae81ff>2</span>)<span style=color:#f92672>.</span>float() <span style=color:#f92672>*</span> (<span style=color:#f92672>-</span>math<span style=color:#f92672>.</span>log(<span style=color:#ae81ff>10000.0</span>) <span style=color:#f92672>/</span> d_model))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 应用正弦函数 得到偶数索引位置编码</span>
</span></span><span style=display:flex><span>        pe[:, <span style=color:#ae81ff>0</span>::<span style=color:#ae81ff>2</span>] <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>sin(position <span style=color:#f92672>*</span> div_term)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 应用余弦函数 得到奇数索引位置编码</span>
</span></span><span style=display:flex><span>        pe[:, <span style=color:#ae81ff>1</span>::<span style=color:#ae81ff>2</span>] <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>cos(position <span style=color:#f92672>*</span> div_term)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 增加一个 batch 维度, 使其能够与输入一起使用</span>
</span></span><span style=display:flex><span>        pe <span style=color:#f92672>=</span> pe<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 将位置编码矩阵注册为一个参数, 并将其添加到模型参数列表中</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>register_buffer(<span style=color:#e6db74>&#39;pe&#39;</span>, pe)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;&#39;&#39;&#39;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        :param x: 输入的序列张量, shape为: &lt;batch_size, seq_len, d_model&gt;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        :return: 输出的序列张量, shape为: &lt;batch_size, seq_len, d_model&gt;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#39;&#39;&#39;</span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> x <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>pe[:, :x<span style=color:#f92672>.</span>size(<span style=color:#ae81ff>1</span>), :]
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;__main__&#39;</span>:
</span></span><span style=display:flex><span>    <span style=color:#75715e># 初始化参数</span>
</span></span><span style=display:flex><span>    d_model <span style=color:#f92672>=</span> <span style=color:#ae81ff>512</span>
</span></span><span style=display:flex><span>    max_len <span style=color:#f92672>=</span> <span style=color:#ae81ff>2048</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 初始化位置编码</span>
</span></span><span style=display:flex><span>    pos_encoder <span style=color:#f92672>=</span> PositionalEncoding(d_model, max_len)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 初始化 tokenizer (这里以 Bert 为例)</span>
</span></span><span style=display:flex><span>    tokenizer <span style=color:#f92672>=</span> BertTokenizer<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#39;bert-base-uncased&#39;</span>, cache_dir<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;./cache&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 使用输入&#34;hello world&#34;</span>
</span></span><span style=display:flex><span>    input_text <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;hello world&#34;</span>
</span></span><span style=display:flex><span>    input_ids <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([tokenizer<span style=color:#f92672>.</span>encode(input_text, add_special_tokens<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 创建一个形状为 (1, seq_len, d_model) 的零矩阵</span>
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>zeros(<span style=color:#ae81ff>1</span>, input_ids<span style=color:#f92672>.</span>size(<span style=color:#ae81ff>1</span>), d_model)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 应用位置编码</span>
</span></span><span style=display:flex><span>    output <span style=color:#f92672>=</span> pos_encoder(x)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;Input Text:&#34;</span>, input_text)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;Input IDs Shape:&#34;</span>, input_ids<span style=color:#f92672>.</span>shape)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;Output Shape:&#34;</span>, output<span style=color:#f92672>.</span>shape)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;Output:&#34;</span>, output)
</span></span></code></pre></td></tr></table></div></div><p>以上案例为 : 当输入hello world, 经过编码后输出其对应的信息</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>Input Text: hello world
</span></span><span style=display:flex><span>Input IDs Shape: torch.Size<span style=color:#f92672>([</span>1, 4<span style=color:#f92672>])</span>
</span></span><span style=display:flex><span>Output Shape: torch.Size<span style=color:#f92672>([</span>1, 4, 512<span style=color:#f92672>])</span>
</span></span><span style=display:flex><span>Output: tensor<span style=color:#f92672>([[[</span> 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,
</span></span><span style=display:flex><span>           0.0000e+00,  1.0000e+00<span style=color:#f92672>]</span>,
</span></span><span style=display:flex><span>         <span style=color:#f92672>[</span> 8.4147e-01,  5.4030e-01,  8.2186e-01,  ...,  1.0000e+00,
</span></span><span style=display:flex><span>           1.0366e-04,  1.0000e+00<span style=color:#f92672>]</span>,
</span></span><span style=display:flex><span>         <span style=color:#f92672>[</span> 9.0930e-01, -4.1615e-01,  9.3641e-01,  ...,  1.0000e+00,
</span></span><span style=display:flex><span>           2.0733e-04,  1.0000e+00<span style=color:#f92672>]</span>,
</span></span><span style=display:flex><span>         <span style=color:#f92672>[</span> 1.4112e-01, -9.8999e-01,  2.4509e-01,  ...,  1.0000e+00,
</span></span><span style=display:flex><span>           3.1099e-04,  1.0000e+00<span style=color:#f92672>]]])</span>
</span></span></code></pre></td></tr></table></div></div><hr><h2 id=多头注意力机制>多头注意力机制<a hidden class=anchor aria-hidden=true href=#多头注意力机制>#</a></h2><p>使用Pytorch实现多头注意力机制的底层代码, 含例子.</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">  1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">  2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">  3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">  4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">  5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">  6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">  7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">  8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">  9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 29
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 30
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 31
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 32
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 33
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 34
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 35
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 36
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 37
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 38
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 39
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 40
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 41
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 42
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 43
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 44
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 45
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 46
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 47
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 48
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 49
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 50
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 51
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 52
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 53
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 54
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 55
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 56
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 57
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 58
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 59
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 60
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 61
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 62
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 63
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 64
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 65
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 66
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 67
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 68
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 69
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 70
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 71
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 72
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 73
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 74
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 75
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 76
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 77
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 78
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 79
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 80
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 81
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 82
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 83
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 84
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 85
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 86
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 87
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 88
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 89
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 90
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 91
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 92
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 93
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 94
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 95
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 96
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 97
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 98
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 99
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">100
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">101
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">102
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">103
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">104
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">105
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">106
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">107
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">108
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">109
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">110
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">111
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">112
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">113
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">114
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">115
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">116
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">117
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">118
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">119
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">120
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">121
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">122
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">123
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">124
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>#!/usr/bin/env python</span>
</span></span><span style=display:flex><span><span style=color:#75715e># -*- encoding: utf-8 -*-</span>
</span></span><span style=display:flex><span><span style=color:#e6db74>&#39;&#39;&#39;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>@File    :   MultiHeadAttention.py
</span></span></span><span style=display:flex><span><span style=color:#e6db74>@Time    :   2024/09/27 09:50:43
</span></span></span><span style=display:flex><span><span style=color:#e6db74>@Author  :   pan binghong 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>@Email   :   19909442097@163.com
</span></span></span><span style=display:flex><span><span style=color:#e6db74>@description   :   
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&#39;&#39;&#39;</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> BertTokenizer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 获取当前文件的绝对路径</span>
</span></span><span style=display:flex><span>file_path <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>abspath(__file__)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 获取当前文件所在的目录路径</span>
</span></span><span style=display:flex><span>dir <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>dirname(file_path)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 将当前工作目录更改为当前文件所在的目录</span>
</span></span><span style=display:flex><span>os<span style=color:#f92672>.</span>chdir(dir)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>MultiHeadAttention</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, embed_size, heads):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;&#39;&#39;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        初始化多头注意力机制
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        :param embed_size: 输入嵌入向量的维度
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        :param num_heads: 多头注意力机制的头数
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#39;&#39;&#39;</span>
</span></span><span style=display:flex><span>        super(MultiHeadAttention, self)<span style=color:#f92672>.</span><span style=color:#a6e22e>__init__</span>()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>assert</span> embed_size <span style=color:#f92672>%</span> heads <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>, <span style=color:#e6db74>&#34;嵌入维度必须能被头数整除&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>embed_size <span style=color:#f92672>=</span> embed_size
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>heads <span style=color:#f92672>=</span> heads
</span></span><span style=display:flex><span>        <span style=color:#75715e># 每个头的维度</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>heads_dim <span style=color:#f92672>=</span> embed_size <span style=color:#f92672>//</span> heads
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>value <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(self<span style=color:#f92672>.</span>heads_dim, self<span style=color:#f92672>.</span>heads_dim, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>key <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(self<span style=color:#f92672>.</span>heads_dim, self<span style=color:#f92672>.</span>heads_dim, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>query <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(self<span style=color:#f92672>.</span>heads_dim, self<span style=color:#f92672>.</span>heads_dim, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>fc_out <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(heads <span style=color:#f92672>*</span> self<span style=color:#f92672>.</span>heads_dim, embed_size)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, query, keys, values, mask):
</span></span><span style=display:flex><span>        N <span style=color:#f92672>=</span> query<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>        value_len, key_len, query_len <span style=color:#f92672>=</span> values<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>], keys<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>], query<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 将输入的数据划分为多个头, 先调整shape为 (N, value_len, heads, heads_dim)</span>
</span></span><span style=display:flex><span>        values <span style=color:#f92672>=</span> values<span style=color:#f92672>.</span>reshape(N, value_len, self<span style=color:#f92672>.</span>heads, self<span style=color:#f92672>.</span>heads_dim)
</span></span><span style=display:flex><span>        keys <span style=color:#f92672>=</span> keys<span style=color:#f92672>.</span>reshape(N, key_len, self<span style=color:#f92672>.</span>heads, self<span style=color:#f92672>.</span>heads_dim)
</span></span><span style=display:flex><span>        query <span style=color:#f92672>=</span> query<span style=color:#f92672>.</span>reshape(N, query_len, self<span style=color:#f92672>.</span>heads, self<span style=color:#f92672>.</span>heads_dim)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 初始化Q, K, V矩阵</span>
</span></span><span style=display:flex><span>        values <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>value(values)
</span></span><span style=display:flex><span>        keys <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>key(keys)
</span></span><span style=display:flex><span>        query <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>query(query)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 计算注意力分数</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 这行代码使用爱因斯坦求和约定（Einstein Summation Convention）来计算注意力分数。</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 具体来说，它通过矩阵乘法计算query和keys之间的点积，然后将其重塑为所需的形状。</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># </span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 参数解释：</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># - &#34;nqhd,nkhd-&gt;nhqk&#34; 是爱因斯坦求和约定的字符串表示。</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e>#   - &#34;n&#34; 表示批次大小（batch size）。</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e>#   - &#34;q&#34; 表示查询序列的长度（query length）。</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e>#   - &#34;h&#34; 表示注意力头数（number of heads）。</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e>#   - &#34;d&#34; 表示每个头的维度（dimension per head）。</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e>#   - &#34;k&#34; 表示键序列的长度（key length）。</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># </span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 具体操作：</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># - query 的形状为 (N, query_len, heads, heads_dim)。</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># - keys 的形状为 (N, key_len, heads, heads_dim)。</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># - 通过 torch.einsum 计算 query 和 keys 的点积，结果的形状为 (N, heads, query_len, key_len)。</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># </span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 计算过程：</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># - 对于每个批次（n），每个头（h），计算 query 和 keys 的点积，得到一个形状为 (query_len, key_len) 的矩阵。</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># - 最终结果是一个形状为 (N, heads, query_len, key_len) 的张量，表示每个查询和每个键之间的注意力分数。</span>
</span></span><span style=display:flex><span>        energy <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>einsum(<span style=color:#e6db74>&#34;nqhd,nkhd-&gt;nhqk&#34;</span>, [query, keys])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 应用注意力机制</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> mask <span style=color:#f92672>is</span> <span style=color:#f92672>not</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>            energy <span style=color:#f92672>=</span> energy<span style=color:#f92672>.</span>masked_fill(mask <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>, float(<span style=color:#e6db74>&#39;-1e20&#39;</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        attention <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>softmax(energy <span style=color:#f92672>/</span> (self<span style=color:#f92672>.</span>embed_size <span style=color:#f92672>**</span> (<span style=color:#ae81ff>1</span><span style=color:#f92672>/</span><span style=color:#ae81ff>2</span>)), dim<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        out <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>einsum(<span style=color:#e6db74>&#34;nhql,nlhd-&gt;nqhd&#34;</span>, [attention, values])<span style=color:#f92672>.</span>reshape(
</span></span><span style=display:flex><span>            N, query_len, self<span style=color:#f92672>.</span>heads <span style=color:#f92672>*</span> self<span style=color:#f92672>.</span>heads_dim
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        out <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>fc_out(out)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> out
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;__main__&#39;</span>:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    tokenizer <span style=color:#f92672>=</span> BertTokenizer<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#39;bert-base-uncased&#39;</span>, cache_dir<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;./cache&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    input_text <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;Hello world!&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 初始化随机keys,values, query</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    tokens <span style=color:#f92672>=</span> tokenizer<span style=color:#f92672>.</span>tokenize(input_text)
</span></span><span style=display:flex><span>    token_ids <span style=color:#f92672>=</span> tokenizer<span style=color:#f92672>.</span>convert_tokens_to_ids(tokens)
</span></span><span style=display:flex><span>    token_ids <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([token_ids])<span style=color:#f92672>.</span>long()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    batch_size,seq_length <span style=color:#f92672>=</span> token_ids<span style=color:#f92672>.</span>shape
</span></span><span style=display:flex><span>    embed_size <span style=color:#f92672>=</span><span style=color:#ae81ff>512</span>
</span></span><span style=display:flex><span>    heads <span style=color:#f92672>=</span> <span style=color:#ae81ff>8</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    values <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>rand(batch_size, seq_length, embed_size)
</span></span><span style=display:flex><span>    keys <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>rand(batch_size, seq_length, embed_size)
</span></span><span style=display:flex><span>    query <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>rand(batch_size, seq_length, embed_size)
</span></span><span style=display:flex><span>    mask <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    attention <span style=color:#f92672>=</span> MultiHeadAttention(embed_size, heads)
</span></span><span style=display:flex><span>    out <span style=color:#f92672>=</span> attention(query, keys, values, mask)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Tokens: </span><span style=color:#e6db74>{</span>tokens<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Token IDs: </span><span style=color:#e6db74>{</span>token_ids<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    print(out<span style=color:#f92672>.</span>shape)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Multi-head Attention Output: </span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>{</span>out<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></td></tr></table></div></div><p>运行以上代码后输出 :</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">8
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>Tokens: [<span style=color:#e6db74>&#39;hello&#39;</span>, <span style=color:#e6db74>&#39;world&#39;</span>, <span style=color:#e6db74>&#39;!&#39;</span>]
</span></span><span style=display:flex><span>Token IDs: tensor([[<span style=color:#ae81ff>7592</span>, <span style=color:#ae81ff>2088</span>,  <span style=color:#ae81ff>999</span>]])
</span></span><span style=display:flex><span>torch<span style=color:#f92672>.</span>Size([<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>512</span>])
</span></span><span style=display:flex><span>Multi<span style=color:#f92672>-</span>head Attention Output:
</span></span><span style=display:flex><span>tensor([[[<span style=color:#f92672>-</span><span style=color:#ae81ff>0.0855</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.2076</span>,  <span style=color:#ae81ff>0.0354</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.1631</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.0024</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.1372</span>],
</span></span><span style=display:flex><span>         [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.0852</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.2079</span>,  <span style=color:#ae81ff>0.0366</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.1636</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.0025</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.1374</span>],
</span></span><span style=display:flex><span>         [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.0860</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.2076</span>,  <span style=color:#ae81ff>0.0364</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.1644</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.0018</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.1361</span>]]],
</span></span><span style=display:flex><span>       grad_fn<span style=color:#f92672>=&lt;</span>ViewBackward0<span style=color:#f92672>&gt;</span>)
</span></span></code></pre></td></tr></table></div></div><hr><h2 id=前馈神经网络-1>前馈神经网络<a hidden class=anchor aria-hidden=true href=#前馈神经网络-1>#</a></h2><p>就是一个简单的全连接层…没啥好说的, 看代码 :</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">29
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">30
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">31
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">32
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">33
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">34
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">35
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">36
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">37
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">38
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">39
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">40
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">41
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">42
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">43
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">44
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">45
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">46
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">47
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">48
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">49
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">50
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">51
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">52
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">53
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">54
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">55
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">56
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">57
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">58
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">59
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">60
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>#!/usr/bin/env python</span>
</span></span><span style=display:flex><span><span style=color:#75715e># -*- encoding: utf-8 -*-</span>
</span></span><span style=display:flex><span><span style=color:#e6db74>&#39;&#39;&#39;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>@File    :   FeedForwardNetwork.py
</span></span></span><span style=display:flex><span><span style=color:#e6db74>@Time    :   2024/09/27 14:46:48
</span></span></span><span style=display:flex><span><span style=color:#e6db74>@Author  :   pan binghong 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>@Email   :   19909442097@163.com
</span></span></span><span style=display:flex><span><span style=color:#e6db74>@description   :   
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&#39;&#39;&#39;</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> BertTokenizer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>file_path <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>abspath(__file__)
</span></span><span style=display:flex><span>dir <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>dirname(file_path)
</span></span><span style=display:flex><span>os<span style=color:#f92672>.</span>chdir(dir)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>FeedForwardNetwork</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, d_model, hidden_size, dropout<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;&#39;&#39;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        :param d_model:输入的特征维度大小
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        :param hidden_size:隐藏层大小
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        :param dropout:dropout概率
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#39;&#39;&#39;</span>
</span></span><span style=display:flex><span>        super(FeedForwardNetwork, self)<span style=color:#f92672>.</span><span style=color:#a6e22e>__init__</span>()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>liner1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(d_model, hidden_size)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>relu <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>ReLU()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>liner2 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(hidden_size, d_model)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>dropout <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Dropout(dropout)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;&#39;&#39;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        :param x:输入的特征
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        :return:输出的特征
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#39;&#39;&#39;</span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>liner1(x)
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>relu(x)
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>liner2(x)
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>dropout(x)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;__main__&#39;</span>:
</span></span><span style=display:flex><span>    tokenizer <span style=color:#f92672>=</span> BertTokenizer<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#39;bert-base-uncased&#39;</span>,cache_dir<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;./cache&#39;</span>)
</span></span><span style=display:flex><span>    input_text <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;Hello world&#34;</span>
</span></span><span style=display:flex><span>    tokens <span style=color:#f92672>=</span> tokenizer<span style=color:#f92672>.</span>tokenize(input_text)
</span></span><span style=display:flex><span>    token_ids <span style=color:#f92672>=</span> tokenizer<span style=color:#f92672>.</span>convert_tokens_to_ids(tokens)
</span></span><span style=display:flex><span>    token_ids <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([token_ids])<span style=color:#f92672>.</span>long()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    d_model <span style=color:#f92672>=</span> token_ids<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>    hidden_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>256</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    ff_netword <span style=color:#f92672>=</span> FeedForwardNetwork(d_model, hidden_size)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    output <span style=color:#f92672>=</span> ff_netword(token_ids<span style=color:#f92672>.</span>float())
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;Input tokens: </span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>{</span>tokens<span style=color:#e6db74>}</span><span style=color:#e6db74>&#39;</span>)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;Input token_ids: </span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>{</span>token_ids<span style=color:#e6db74>}</span><span style=color:#e6db74>&#39;</span>)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;Output from FeedForwardNetwork: </span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>{</span>output<span style=color:#e6db74>}</span><span style=color:#e6db74>&#39;</span>)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;Output shape: </span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>{</span>output<span style=color:#f92672>.</span>shape<span style=color:#e6db74>}</span><span style=color:#e6db74>&#39;</span>)
</span></span></code></pre></td></tr></table></div></div><p>输出结果 :</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">8
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>Input tokens:
</span></span><span style=display:flex><span>[<span style=color:#e6db74>&#39;hello&#39;</span>, <span style=color:#e6db74>&#39;world&#39;</span>]
</span></span><span style=display:flex><span>Input token_ids:
</span></span><span style=display:flex><span>tensor([[<span style=color:#ae81ff>7592</span>, <span style=color:#ae81ff>2088</span>]])
</span></span><span style=display:flex><span>Output <span style=color:#f92672>from</span> FeedForwardNetwork:
</span></span><span style=display:flex><span>tensor([[<span style=color:#ae81ff>2332.0767</span>, <span style=color:#ae81ff>1194.7576</span>]], grad_fn<span style=color:#f92672>=&lt;</span>MulBackward0<span style=color:#f92672>&gt;</span>)
</span></span><span style=display:flex><span>Output shape:
</span></span><span style=display:flex><span>torch<span style=color:#f92672>.</span>Size([<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>])
</span></span></code></pre></td></tr></table></div></div><h1 id=层归一化--残差连接-1>层归一化 & 残差连接<a hidden class=anchor aria-hidden=true href=#层归一化--残差连接-1>#</a></h1><hr><p>Layer Normalization</p><hr><p>代码如下 :</p><hr><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">29
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">30
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">31
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">32
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">33
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">34
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">35
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">36
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">37
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">38
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">39
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">40
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">41
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">42
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">43
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">44
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">45
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">46
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">47
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">48
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">49
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">50
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">51
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">52
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">53
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">54
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">55
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-javascript data-lang=javascript><span style=display:flex><span><span style=color:#75715e>#!/usr/bin/env python
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#960050;background-color:#1e0010>#</span> <span style=color:#f92672>-*-</span> <span style=color:#a6e22e>encoding</span><span style=color:#f92672>:</span> <span style=color:#a6e22e>utf</span><span style=color:#f92672>-</span><span style=color:#ae81ff>8</span> <span style=color:#f92672>-*-</span>
</span></span><span style=display:flex><span><span style=color:#e6db74>&#39;&#39;&#39;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>@File    :   LayerNormalization.py
</span></span></span><span style=display:flex><span><span style=color:#e6db74>@Time    :   2024/09/27 15:17:48
</span></span></span><span style=display:flex><span><span style=color:#e6db74>@Author  :   pan binghong 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>@Email   :   19909442097@163.com
</span></span></span><span style=display:flex><span><span style=color:#e6db74>@description   :   
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&#39;&#39;&#39;</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>import</span> <span style=color:#a6e22e>torch</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>import</span> <span style=color:#a6e22e>torch</span>.<span style=color:#a6e22e>nn</span> <span style=color:#a6e22e>as</span> <span style=color:#a6e22e>nn</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>LayerNormalization</span>(<span style=color:#a6e22e>nn</span>.<span style=color:#a6e22e>Module</span>)<span style=color:#f92672>:</span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>def</span> <span style=color:#a6e22e>__init__</span>(<span style=color:#a6e22e>self</span>, <span style=color:#a6e22e>features</span>, <span style=color:#a6e22e>eps</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1e-6</span>)<span style=color:#f92672>:</span>
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;&#39;&#39;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        初始化层归一化模块
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        :param features: 特征维度大小
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        :param eps: 防止除零的小常数
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#39;&#39;&#39;</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>super</span>(<span style=color:#a6e22e>LayerNormalization</span>, <span style=color:#a6e22e>self</span>).<span style=color:#a6e22e>__init__</span>()  <span style=color:#960050;background-color:#1e0010>#</span> <span style=color:#a6e22e>调用父类nn</span>.<span style=color:#a6e22e>Module的初始化方法</span>
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>self</span>.<span style=color:#a6e22e>eps</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>eps</span>  <span style=color:#960050;background-color:#1e0010>#</span> <span style=color:#a6e22e>设置防止除零的小常数</span>
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>self</span>.<span style=color:#a6e22e>gain</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>nn</span>.<span style=color:#a6e22e>Parameter</span>(<span style=color:#a6e22e>torch</span>.<span style=color:#a6e22e>ones</span>(<span style=color:#a6e22e>features</span>))  <span style=color:#960050;background-color:#1e0010>#</span> <span style=color:#a6e22e>初始化增益参数</span><span style=color:#960050;background-color:#1e0010>，</span><span style=color:#a6e22e>形状为</span>(<span style=color:#a6e22e>features</span>,)<span style=color:#960050;background-color:#1e0010>，</span><span style=color:#a6e22e>初始值为1</span>
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>self</span>.<span style=color:#a6e22e>bias</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>nn</span>.<span style=color:#a6e22e>Parameter</span>(<span style=color:#a6e22e>torch</span>.<span style=color:#a6e22e>zeros</span>(<span style=color:#a6e22e>features</span>))  <span style=color:#960050;background-color:#1e0010>#</span> <span style=color:#a6e22e>初始化偏置参数</span><span style=color:#960050;background-color:#1e0010>，</span><span style=color:#a6e22e>形状为</span>(<span style=color:#a6e22e>features</span>,)<span style=color:#960050;background-color:#1e0010>，</span><span style=color:#a6e22e>初始值为0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>def</span> <span style=color:#a6e22e>forward</span>(<span style=color:#a6e22e>self</span>, <span style=color:#a6e22e>x</span>)<span style=color:#f92672>:</span>
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;&#39;&#39;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        前向传播函数
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        :param x: 输入张量，形状为(batch_size, seq_len, features)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        :return: 归一化后的输出张量
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#39;&#39;&#39;</span>
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>mean</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>x</span>.<span style=color:#a6e22e>mean</span>(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#a6e22e>keepdim</span><span style=color:#f92672>=</span><span style=color:#a6e22e>True</span>)  <span style=color:#960050;background-color:#1e0010>#</span> <span style=color:#a6e22e>计算输入张量在最后一个维度上的均值</span><span style=color:#960050;background-color:#1e0010>，</span><span style=color:#a6e22e>保持维度</span>
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>std</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>x</span>.<span style=color:#a6e22e>std</span>(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#a6e22e>keepdim</span><span style=color:#f92672>=</span><span style=color:#a6e22e>True</span>)  <span style=color:#960050;background-color:#1e0010>#</span> <span style=color:#a6e22e>计算输入张量在最后一个维度上的标准差</span><span style=color:#960050;background-color:#1e0010>，</span><span style=color:#a6e22e>保持维度</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> <span style=color:#a6e22e>self</span>.<span style=color:#a6e22e>gain</span> <span style=color:#f92672>*</span> (<span style=color:#a6e22e>x</span> <span style=color:#f92672>-</span> <span style=color:#a6e22e>mean</span>) <span style=color:#f92672>/</span> (<span style=color:#a6e22e>std</span> <span style=color:#f92672>+</span> <span style=color:#a6e22e>self</span>.<span style=color:#a6e22e>eps</span>) <span style=color:#f92672>+</span> <span style=color:#a6e22e>self</span>.<span style=color:#a6e22e>bias</span>  <span style=color:#960050;background-color:#1e0010>#</span> <span style=color:#a6e22e>应用层归一化公式并返回结果</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> <span style=color:#a6e22e>__name__</span> <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;__main__&#34;</span><span style=color:#f92672>:</span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>batch_size</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>seq_len</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>2048</span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>features</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>4096</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#960050;background-color:#1e0010>#</span> <span style=color:#a6e22e>创建一个简单的输入张量</span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>x</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>torch</span>.<span style=color:#a6e22e>randn</span>(<span style=color:#a6e22e>batch_size</span>, <span style=color:#a6e22e>seq_len</span>, <span style=color:#a6e22e>features</span>)  <span style=color:#960050;background-color:#1e0010>#</span> <span style=color:#a6e22e>随机初始化输入张量</span>
</span></span><span style=display:flex><span>    <span style=color:#960050;background-color:#1e0010>#</span> <span style=color:#a6e22e>初始化层归一化层</span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>ln</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>LayerNormalization</span>(<span style=color:#a6e22e>features</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#960050;background-color:#1e0010>#</span> <span style=color:#a6e22e>应用层归一化</span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>normalized_x</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>ln</span>(<span style=color:#a6e22e>x</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#960050;background-color:#1e0010>#</span> <span style=color:#a6e22e>打印原始和归一化后的张量</span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>print</span>(<span style=color:#e6db74>&#34;原始输入张量:&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>print</span>(<span style=color:#a6e22e>x</span>)
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>print</span>(<span style=color:#e6db74>&#34;\n归一化后的输出张量:&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>print</span>(<span style=color:#a6e22e>normalized_x</span>)
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>print</span>(<span style=color:#e6db74>&#34;\n归一化后的维度:&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>print</span>(<span style=color:#a6e22e>normalized_x</span>.<span style=color:#a6e22e>shape</span>)
</span></span></code></pre></td></tr></table></div></div><hr><p>输出 :</p><hr><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">21
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-javascript data-lang=javascript><span style=display:flex><span><span style=color:#a6e22e>原始输入张量</span><span style=color:#f92672>:</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>tensor</span>([[[ <span style=color:#ae81ff>0.7926</span>,  <span style=color:#ae81ff>0.9379</span>,  <span style=color:#ae81ff>0.9247</span>,  ..., <span style=color:#f92672>-</span><span style=color:#ae81ff>0.8849</span>,  <span style=color:#ae81ff>0.4262</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.1126</span>],
</span></span><span style=display:flex><span>         [<span style=color:#f92672>-</span><span style=color:#ae81ff>1.0093</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.3612</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.7135</span>,  ...,  <span style=color:#ae81ff>1.7116</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.1164</span>,  <span style=color:#ae81ff>0.8453</span>],
</span></span><span style=display:flex><span>         [ <span style=color:#ae81ff>1.1906</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.3223</span>,  <span style=color:#ae81ff>0.3553</span>,  ..., <span style=color:#f92672>-</span><span style=color:#ae81ff>0.0293</span>,  <span style=color:#ae81ff>0.6014</span>,  <span style=color:#ae81ff>0.3705</span>],
</span></span><span style=display:flex><span>         ...,
</span></span><span style=display:flex><span>         [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.3714</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.1437</span>,  <span style=color:#ae81ff>1.2062</span>,  ..., <span style=color:#f92672>-</span><span style=color:#ae81ff>0.7603</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1.2689</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.4045</span>],
</span></span><span style=display:flex><span>         [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.6838</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.5469</span>,  <span style=color:#ae81ff>0.2328</span>,  ..., <span style=color:#f92672>-</span><span style=color:#ae81ff>0.4500</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1.1035</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.2005</span>],
</span></span><span style=display:flex><span>         [ <span style=color:#ae81ff>0.4267</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.5163</span>,  <span style=color:#ae81ff>1.1316</span>,  ..., <span style=color:#f92672>-</span><span style=color:#ae81ff>0.1339</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.4004</span>,  <span style=color:#ae81ff>0.2841</span>]]])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>归一化后的输出张量</span><span style=color:#f92672>:</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>tensor</span>([[[ <span style=color:#ae81ff>0.7959</span>,  <span style=color:#ae81ff>0.9418</span>,  <span style=color:#ae81ff>0.9285</span>,  ..., <span style=color:#f92672>-</span><span style=color:#ae81ff>0.8873</span>,  <span style=color:#ae81ff>0.4283</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.1123</span>],
</span></span><span style=display:flex><span>         [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.9919</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.3548</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.7011</span>,  ...,  <span style=color:#ae81ff>1.6830</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.1141</span>,  <span style=color:#ae81ff>0.8314</span>],
</span></span><span style=display:flex><span>         [ <span style=color:#ae81ff>1.2045</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.3012</span>,  <span style=color:#ae81ff>0.3732</span>,  ..., <span style=color:#f92672>-</span><span style=color:#ae81ff>0.0096</span>,  <span style=color:#ae81ff>0.6181</span>,  <span style=color:#ae81ff>0.3883</span>],
</span></span><span style=display:flex><span>         ...,
</span></span><span style=display:flex><span>         [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.3827</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.1589</span>,  <span style=color:#ae81ff>1.1681</span>,  ..., <span style=color:#f92672>-</span><span style=color:#ae81ff>0.7650</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1.2649</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.4152</span>],
</span></span><span style=display:flex><span>         [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.7084</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.5709</span>,  <span style=color:#ae81ff>0.2130</span>,  ..., <span style=color:#f92672>-</span><span style=color:#ae81ff>0.4735</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1.1304</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.2226</span>],
</span></span><span style=display:flex><span>         [ <span style=color:#ae81ff>0.3982</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.5383</span>,  <span style=color:#ae81ff>1.0983</span>,  ..., <span style=color:#f92672>-</span><span style=color:#ae81ff>0.1585</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.4232</span>,  <span style=color:#ae81ff>0.2567</span>]]],
</span></span><span style=display:flex><span>       <span style=color:#a6e22e>grad_fn</span><span style=color:#f92672>=&lt;</span><span style=color:#a6e22e>AddBackward0</span><span style=color:#f92672>&gt;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>归一化后的维度</span><span style=color:#f92672>:</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>torch</span>.<span style=color:#a6e22e>Size</span>([<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2048</span>, <span style=color:#ae81ff>4096</span>])
</span></span></code></pre></td></tr></table></div></div><hr><h3 id=residual-connections>Residual Connections<a hidden class=anchor aria-hidden=true href=#residual-connections>#</a></h3><p>残差连接代码实现 :</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">29
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">30
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">31
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">32
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">33
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">34
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">35
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">36
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>#!/usr/bin/env python</span>
</span></span><span style=display:flex><span><span style=color:#75715e># -*- encoding: utf-8 -*-</span>
</span></span><span style=display:flex><span><span style=color:#e6db74>&#39;&#39;&#39;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>@File    :   ResidualConnection.py
</span></span></span><span style=display:flex><span><span style=color:#e6db74>@Time    :   2024/09/27 15:33:49
</span></span></span><span style=display:flex><span><span style=color:#e6db74>@Author  :   pan binghong 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>@Email   :   19909442097@163.com
</span></span></span><span style=display:flex><span><span style=color:#e6db74>@description   :   
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&#39;&#39;&#39;</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> LayerNormalization <span style=color:#f92672>import</span> LayerNormalization
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>ResidualConnection</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, size, dropout):
</span></span><span style=display:flex><span>        super(ResidualConnection, self)<span style=color:#f92672>.</span><span style=color:#a6e22e>__init__</span>()  <span style=color:#75715e># 正确调用父类构造函数</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>norm <span style=color:#f92672>=</span> LayerNormalization(size)  <span style=color:#75715e># 在父类构造函数之后设置属性</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>dropout <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Dropout(dropout)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x, sublayer):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>dropout(sublayer(self<span style=color:#f92672>.</span>norm(x)))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;__main__&#39;</span>:
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    size <span style=color:#f92672>=</span> <span style=color:#ae81ff>512</span>
</span></span><span style=display:flex><span>    dropout <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.1</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    residual_module <span style=color:#f92672>=</span> ResidualConnection(size, dropout)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>rand(<span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>10</span>, size)
</span></span><span style=display:flex><span>    sublayer <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(size, size)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    output <span style=color:#f92672>=</span> residual_module(x, sublayer)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;output shape: </span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>{</span>output<span style=color:#f92672>.</span>shape<span style=color:#e6db74>}</span><span style=color:#e6db74>&#39;</span>)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;out: </span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>{</span>output<span style=color:#e6db74>}</span><span style=color:#e6db74>&#39;</span>)
</span></span></code></pre></td></tr></table></div></div><p>输出内容 :</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">29
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">30
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">31
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">32
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">33
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">34
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">35
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">36
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">37
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">38
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">39
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">40
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">41
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">42
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">43
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">44
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">45
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">46
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">47
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">48
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">49
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">50
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">51
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">52
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">53
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>output shape:
</span></span><span style=display:flex><span>torch<span style=color:#f92672>.</span>Size([<span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>512</span>])
</span></span><span style=display:flex><span>out:
</span></span><span style=display:flex><span>tensor([[[ <span style=color:#ae81ff>0.3520</span>,  <span style=color:#ae81ff>0.6030</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.2354</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.8682</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.3730</span>,  <span style=color:#ae81ff>0.1524</span>],
</span></span><span style=display:flex><span>         [ <span style=color:#ae81ff>1.4186</span>,  <span style=color:#ae81ff>0.5724</span>,  <span style=color:#ae81ff>0.2079</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.4792</span>,  <span style=color:#ae81ff>1.2186</span>,  <span style=color:#ae81ff>0.6546</span>],
</span></span><span style=display:flex><span>         [ <span style=color:#ae81ff>1.0631</span>,  <span style=color:#ae81ff>1.0479</span>,  <span style=color:#ae81ff>0.4523</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>1.4115</span>,  <span style=color:#ae81ff>1.5870</span>,  <span style=color:#ae81ff>0.7165</span>],
</span></span><span style=display:flex><span>         <span style=color:#f92672>...</span>,
</span></span><span style=display:flex><span>         [ <span style=color:#ae81ff>0.0567</span>,  <span style=color:#ae81ff>2.4177</span>,  <span style=color:#ae81ff>1.6159</span>,  <span style=color:#f92672>...</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.5791</span>,  <span style=color:#ae81ff>0.4186</span>,  <span style=color:#ae81ff>0.6306</span>],
</span></span><span style=display:flex><span>         [ <span style=color:#ae81ff>1.9228</span>,  <span style=color:#ae81ff>0.8467</span>,  <span style=color:#ae81ff>0.7399</span>,  <span style=color:#f92672>...</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.5388</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.1463</span>,  <span style=color:#ae81ff>1.1880</span>],
</span></span><span style=display:flex><span>         [ <span style=color:#ae81ff>0.1197</span>,  <span style=color:#ae81ff>0.0182</span>,  <span style=color:#ae81ff>0.2941</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.5807</span>,  <span style=color:#ae81ff>0.3925</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.4700</span>]],
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        [[ <span style=color:#ae81ff>0.3333</span>,  <span style=color:#ae81ff>0.8267</span>,  <span style=color:#ae81ff>0.3082</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>1.3075</span>,  <span style=color:#ae81ff>0.2646</span>,  <span style=color:#ae81ff>0.4092</span>],
</span></span><span style=display:flex><span>         [ <span style=color:#ae81ff>0.7507</span>,  <span style=color:#ae81ff>0.9554</span>,  <span style=color:#ae81ff>0.2910</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>1.1357</span>,  <span style=color:#ae81ff>0.4684</span>,  <span style=color:#ae81ff>0.4244</span>],
</span></span><span style=display:flex><span>         [ <span style=color:#ae81ff>0.6514</span>,  <span style=color:#ae81ff>0.2003</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.3597</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.3598</span>,  <span style=color:#ae81ff>0.4869</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.1992</span>],
</span></span><span style=display:flex><span>         <span style=color:#f92672>...</span>,
</span></span><span style=display:flex><span>         [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.0030</span>,  <span style=color:#ae81ff>1.0668</span>,  <span style=color:#ae81ff>0.3075</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.5691</span>,  <span style=color:#ae81ff>0.2380</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.1247</span>],
</span></span><span style=display:flex><span>         [ <span style=color:#ae81ff>0.9135</span>,  <span style=color:#ae81ff>0.4341</span>,  <span style=color:#ae81ff>0.0337</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.3597</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.7784</span>,  <span style=color:#ae81ff>0.8458</span>],
</span></span><span style=display:flex><span>         [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.1205</span>,  <span style=color:#ae81ff>0.6370</span>,  <span style=color:#ae81ff>1.0110</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.0136</span>,  <span style=color:#ae81ff>0.6965</span>,  <span style=color:#ae81ff>0.2374</span>]],
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        [[ <span style=color:#ae81ff>1.7201</span>,  <span style=color:#ae81ff>0.9184</span>,  <span style=color:#ae81ff>1.4459</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.6506</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.4328</span>,  <span style=color:#ae81ff>0.4222</span>],
</span></span><span style=display:flex><span>         [ <span style=color:#ae81ff>0.1091</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.7816</span>,  <span style=color:#ae81ff>0.0389</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.4128</span>,  <span style=color:#ae81ff>0.5077</span>,  <span style=color:#ae81ff>1.2345</span>],
</span></span><span style=display:flex><span>         [ <span style=color:#ae81ff>0.3344</span>,  <span style=color:#ae81ff>0.5098</span>,  <span style=color:#ae81ff>0.1903</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.4348</span>,  <span style=color:#ae81ff>0.1655</span>,  <span style=color:#ae81ff>1.0356</span>],
</span></span><span style=display:flex><span>         <span style=color:#f92672>...</span>,
</span></span><span style=display:flex><span>         [ <span style=color:#ae81ff>0.6577</span>,  <span style=color:#ae81ff>1.0894</span>,  <span style=color:#ae81ff>0.3509</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.9495</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.3284</span>,  <span style=color:#ae81ff>0.8220</span>],
</span></span><span style=display:flex><span>         [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.3985</span>,  <span style=color:#ae81ff>1.0728</span>,  <span style=color:#ae81ff>1.2246</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.6893</span>,  <span style=color:#ae81ff>0.3443</span>,  <span style=color:#ae81ff>0.4279</span>],
</span></span><span style=display:flex><span>         [ <span style=color:#ae81ff>0.2423</span>,  <span style=color:#ae81ff>0.0113</span>,  <span style=color:#ae81ff>1.3485</span>,  <span style=color:#f92672>...</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.2677</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.0577</span>,  <span style=color:#ae81ff>0.4010</span>]],
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>...</span>,
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        [[ <span style=color:#ae81ff>1.4007</span>,  <span style=color:#ae81ff>1.3420</span>,  <span style=color:#ae81ff>0.1977</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.8533</span>,  <span style=color:#ae81ff>0.1814</span>,  <span style=color:#ae81ff>0.2697</span>],
</span></span><span style=display:flex><span>         [ <span style=color:#ae81ff>0.6700</span>,  <span style=color:#ae81ff>0.2914</span>,  <span style=color:#ae81ff>0.7087</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.4371</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.6651</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.3476</span>],
</span></span><span style=display:flex><span>         [ <span style=color:#ae81ff>1.7371</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.3646</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.7164</span>,  <span style=color:#f92672>...</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.2941</span>,  <span style=color:#ae81ff>1.5312</span>,  <span style=color:#ae81ff>0.1496</span>],
</span></span><span style=display:flex><span>         <span style=color:#f92672>...</span>,
</span></span><span style=display:flex><span>         [ <span style=color:#ae81ff>1.4492</span>,  <span style=color:#ae81ff>1.6843</span>,  <span style=color:#ae81ff>0.4061</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.2602</span>,  <span style=color:#ae81ff>0.3412</span>,  <span style=color:#ae81ff>0.9145</span>],
</span></span><span style=display:flex><span>         [ <span style=color:#ae81ff>0.5208</span>,  <span style=color:#ae81ff>0.7262</span>,  <span style=color:#ae81ff>1.1507</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>1.6124</span>,  <span style=color:#ae81ff>0.1670</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.7637</span>],
</span></span><span style=display:flex><span>         [ <span style=color:#ae81ff>0.6195</span>,  <span style=color:#ae81ff>1.8701</span>,  <span style=color:#ae81ff>0.6011</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.7136</span>,  <span style=color:#ae81ff>0.1405</span>,  <span style=color:#ae81ff>0.7195</span>]],
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        [[ <span style=color:#ae81ff>1.2362</span>,  <span style=color:#ae81ff>0.3252</span>,  <span style=color:#ae81ff>1.5944</span>,  <span style=color:#f92672>...</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.6239</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.5983</span>,  <span style=color:#ae81ff>0.0794</span>],
</span></span><span style=display:flex><span>         [ <span style=color:#ae81ff>1.1429</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.8601</span>,  <span style=color:#ae81ff>0.6993</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.1767</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.7440</span>,  <span style=color:#ae81ff>0.6210</span>],
</span></span><span style=display:flex><span>         [ <span style=color:#ae81ff>0.1337</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.8456</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.4567</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>1.0074</span>,  <span style=color:#ae81ff>0.6997</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.7483</span>],
</span></span><span style=display:flex><span>         <span style=color:#f92672>...</span>,
</span></span><span style=display:flex><span>         [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.2421</span>,  <span style=color:#ae81ff>0.8441</span>,  <span style=color:#ae81ff>1.1355</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>1.2241</span>,  <span style=color:#ae81ff>0.0689</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.8084</span>],
</span></span><span style=display:flex><span>         [ <span style=color:#ae81ff>0.1203</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.1496</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.3044</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.2365</span>,  <span style=color:#ae81ff>1.0541</span>,  <span style=color:#ae81ff>0.5421</span>],
</span></span><span style=display:flex><span>         [ <span style=color:#ae81ff>0.7855</span>,  <span style=color:#ae81ff>0.0565</span>,  <span style=color:#ae81ff>0.9192</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.8071</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.9707</span>,  <span style=color:#ae81ff>1.3335</span>]],
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        [[ <span style=color:#ae81ff>0.6022</span>,  <span style=color:#ae81ff>1.4715</span>,  <span style=color:#ae81ff>0.2470</span>,  <span style=color:#f92672>...</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.0782</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.6734</span>,  <span style=color:#ae81ff>0.8383</span>],
</span></span><span style=display:flex><span>         [ <span style=color:#ae81ff>0.8088</span>,  <span style=color:#ae81ff>0.3382</span>,  <span style=color:#ae81ff>0.6812</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>1.1501</span>,  <span style=color:#ae81ff>1.0537</span>,  <span style=color:#ae81ff>0.5442</span>],
</span></span><span style=display:flex><span>         [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.0593</span>,  <span style=color:#ae81ff>1.7771</span>,  <span style=color:#ae81ff>0.0580</span>,  <span style=color:#f92672>...</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.0578</span>,  <span style=color:#ae81ff>0.7382</span>,  <span style=color:#ae81ff>1.2158</span>],
</span></span><span style=display:flex><span>         <span style=color:#f92672>...</span>,
</span></span><span style=display:flex><span>         [ <span style=color:#ae81ff>1.1114</span>,  <span style=color:#ae81ff>0.3564</span>,  <span style=color:#ae81ff>0.8435</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.1796</span>,  <span style=color:#ae81ff>1.2682</span>,  <span style=color:#ae81ff>0.5146</span>],
</span></span><span style=display:flex><span>         [ <span style=color:#ae81ff>1.0304</span>,  <span style=color:#ae81ff>1.2170</span>,  <span style=color:#ae81ff>0.8374</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>2.2357</span>,  <span style=color:#ae81ff>0.2286</span>,  <span style=color:#ae81ff>0.3899</span>],
</span></span><span style=display:flex><span>         [ <span style=color:#ae81ff>0.5022</span>,  <span style=color:#ae81ff>0.3711</span>,  <span style=color:#ae81ff>0.2397</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.9505</span>,  <span style=color:#ae81ff>0.3877</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.2048</span>]]],
</span></span><span style=display:flex><span>       grad_fn<span style=color:#f92672>=&lt;</span>AddBackward0<span style=color:#f92672>&gt;</span>)
</span></span></code></pre></td></tr></table></div></div><h2 id=编码器>编码器<a hidden class=anchor aria-hidden=true href=#编码器>#</a></h2><h3 id=encoder-layer>Encoder Layer<a hidden class=anchor aria-hidden=true href=#encoder-layer>#</a></h3><p>编码器层由之前构建的多头注意力机制, 前馈神经网络, 残差模块, 层归一化组合构成</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">29
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">30
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">31
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">32
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">33
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">34
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">35
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">36
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">37
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">38
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">39
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">40
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">41
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">42
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">43
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">44
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">45
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">46
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">47
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">48
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">49
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">50
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">51
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">52
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">53
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">54
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>#!/usr/bin/env python</span>
</span></span><span style=display:flex><span><span style=color:#75715e># -*- encoding: utf-8 -*-</span>
</span></span><span style=display:flex><span><span style=color:#e6db74>&#39;&#39;&#39;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>@File    :   EncoderLayer.py
</span></span></span><span style=display:flex><span><span style=color:#e6db74>@Time    :   2024/09/27 16:51:01
</span></span></span><span style=display:flex><span><span style=color:#e6db74>@Author  :   pan binghong 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>@Email   :   19909442097@163.com
</span></span></span><span style=display:flex><span><span style=color:#e6db74>@description   :   多头注意力机制、前馈神经网络、位置编码、残差连接和层归一化结合起来，</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>                   构建一个 Encoder Layer。Encoder Layer 是 Transformer 编码器的基本组成单位。
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&#39;&#39;&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> MultiHeadAttention <span style=color:#f92672>import</span> MultiHeadAttention
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> FeedForwardNetwork <span style=color:#f92672>import</span> FeedForwardNetwork
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> LayerNormalization <span style=color:#f92672>import</span> LayerNormalization <span style=color:#75715e>#在残差连接模块中完成</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> ResidualConnection <span style=color:#f92672>import</span> ResidualConnection
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>EncoderLayer</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, d_model, num_heads, hidden_size, dropout<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>):
</span></span><span style=display:flex><span>        super(EncoderLayer, self)<span style=color:#f92672>.</span><span style=color:#a6e22e>__init__</span>()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>d_model <span style=color:#f92672>=</span> d_model
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>self_attn <span style=color:#f92672>=</span> MultiHeadAttention(d_model, num_heads)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>pos_ffn <span style=color:#f92672>=</span> FeedForwardNetwork(d_model, hidden_size, dropout)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>residual <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>ModuleList([
</span></span><span style=display:flex><span>            ResidualConnection(d_model, dropout),
</span></span><span style=display:flex><span>            ResidualConnection(d_model, dropout)
</span></span><span style=display:flex><span>        ])
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x, mask<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>residual[<span style=color:#ae81ff>0</span>](x, <span style=color:#66d9ef>lambda</span> x: self<span style=color:#f92672>.</span>self_attn(x, x, x, mask))
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>residual[<span style=color:#ae81ff>1</span>](x, self<span style=color:#f92672>.</span>pos_ffn)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span><span style=color:#75715e># 示例</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    <span style=color:#75715e># 示例输入</span>
</span></span><span style=display:flex><span>    d_model <span style=color:#f92672>=</span> <span style=color:#ae81ff>512</span>
</span></span><span style=display:flex><span>    num_heads <span style=color:#f92672>=</span> <span style=color:#ae81ff>8</span>
</span></span><span style=display:flex><span>    hidden_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>2048</span>
</span></span><span style=display:flex><span>    dropout <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.1</span>
</span></span><span style=display:flex><span>    batch_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>32</span>
</span></span><span style=display:flex><span>    seq_len <span style=color:#f92672>=</span> <span style=color:#ae81ff>10</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    encoder_layer <span style=color:#f92672>=</span> EncoderLayer(d_model, num_heads, hidden_size, dropout)
</span></span><span style=display:flex><span>    input_tensor <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(batch_size, seq_len, d_model)
</span></span><span style=display:flex><span>    mask <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>ones(batch_size, <span style=color:#ae81ff>1</span>,seq_len, seq_len)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 前向传播</span>
</span></span><span style=display:flex><span>    output_tensor <span style=color:#f92672>=</span> encoder_layer(input_tensor, mask)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 输出结果</span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;Input shape:&#34;</span>, input_tensor<span style=color:#f92672>.</span>shape)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;Output shape:&#34;</span>, output_tensor<span style=color:#f92672>.</span>shape)
</span></span></code></pre></td></tr></table></div></div><p>输出结果为 :</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>Input shape: torch<span style=color:#f92672>.</span>Size([<span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>512</span>])
</span></span><span style=display:flex><span>Output shape: torch<span style=color:#f92672>.</span>Size([<span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>512</span>])
</span></span></code></pre></td></tr></table></div></div><h3 id=encoder>Encoder<a hidden class=anchor aria-hidden=true href=#encoder>#</a></h3><p>Encoder由n个Encoder_Lyaer组成, 详细代码如下 :</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">29
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">30
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">31
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">32
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">33
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">34
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">35
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">36
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">37
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">38
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">39
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">40
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">41
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">42
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">43
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">44
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>#!/usr/bin/env python</span>
</span></span><span style=display:flex><span><span style=color:#75715e># -*- encoding: utf-8 -*-</span>
</span></span><span style=display:flex><span><span style=color:#e6db74>&#39;&#39;&#39;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>@File    :   Encoder.py
</span></span></span><span style=display:flex><span><span style=color:#e6db74>@Time    :   2024/09/29 08:39:15
</span></span></span><span style=display:flex><span><span style=color:#e6db74>@Author  :   pan binghong 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>@Email   :   19909442097@163.com
</span></span></span><span style=display:flex><span><span style=color:#e6db74>@description   :   
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&#39;&#39;&#39;</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> sys
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> EncoderLayer <span style=color:#f92672>import</span> EncoderLayer
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> LayerNormalization <span style=color:#f92672>import</span> LayerNormalization
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Encoder</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, encoder_layer, num_layers):
</span></span><span style=display:flex><span>        super(Encoder, self)<span style=color:#f92672>.</span><span style=color:#a6e22e>__init__</span>()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>encoder_layer <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>ModuleList([
</span></span><span style=display:flex><span>            encoder_layer <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(num_layers)
</span></span><span style=display:flex><span>        ])
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>norm <span style=color:#f92672>=</span> LayerNormalization(encoder_layer<span style=color:#f92672>.</span>d_model)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, src, mask<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> layer <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>encoder_layer:
</span></span><span style=display:flex><span>            src <span style=color:#f92672>=</span> layer(src, mask)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>norm(src)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;__main__&#39;</span>:
</span></span><span style=display:flex><span>    d_model <span style=color:#f92672>=</span> <span style=color:#ae81ff>512</span>
</span></span><span style=display:flex><span>    num_heades <span style=color:#f92672>=</span> <span style=color:#ae81ff>8</span>
</span></span><span style=display:flex><span>    hidden_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>2048</span>
</span></span><span style=display:flex><span>    droupout <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.1</span>
</span></span><span style=display:flex><span>    num_layers <span style=color:#f92672>=</span> <span style=color:#ae81ff>6</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    encoder_layer <span style=color:#f92672>=</span> EncoderLayer(d_model, num_heades, hidden_size, droupout)
</span></span><span style=display:flex><span>    encoder <span style=color:#f92672>=</span> Encoder(encoder_layer, num_layers)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    src <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>rand(<span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>10</span> , d_model)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    output <span style=color:#f92672>=</span> encoder(src)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    print(output<span style=color:#f92672>.</span>shape)
</span></span></code></pre></td></tr></table></div></div><p>输出结果为 :</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>Input shape: torch.Size<span style=color:#f92672>([</span>32, 10, 512<span style=color:#f92672>])</span>
</span></span><span style=display:flex><span>Output shape: torch.Size<span style=color:#f92672>([</span>32, 10, 512<span style=color:#f92672>])</span>
</span></span></code></pre></td></tr></table></div></div><ul><li>32个样本</li><li>每个样本有10个时间步</li><li>每个时间步的特征向量有512个维度</li></ul><hr><h2 id=解码器>解码器<a hidden class=anchor aria-hidden=true href=#解码器>#</a></h2><h3 id=decoder_layer>Decoder_Layer<a hidden class=anchor aria-hidden=true href=#decoder_layer>#</a></h3><p>编码器与解码器最大的区别就是使用了Mask Multi-Head Attention, 用于防止模型训练过程中”看到”后续的目标词, 由多个解码器层构成, 代码如下 :</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">29
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">30
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">31
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">32
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">33
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">34
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">35
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">36
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">37
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">38
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">39
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">40
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">41
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">42
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">43
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">44
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">45
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">46
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">47
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">48
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">49
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">50
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">51
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">52
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">53
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">54
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">55
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">56
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>#!/usr/bin/env python</span>
</span></span><span style=display:flex><span><span style=color:#75715e># -*- encoding: utf-8 -*-</span>
</span></span><span style=display:flex><span><span style=color:#e6db74>&#39;&#39;&#39;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>@File    :   DecoderLayer.py
</span></span></span><span style=display:flex><span><span style=color:#e6db74>@Time    :   2024/09/29 09:34:04
</span></span></span><span style=display:flex><span><span style=color:#e6db74>@Author  :   pan binghong 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>@Email   :   19909442097@163.com
</span></span></span><span style=display:flex><span><span style=color:#e6db74>@description   :   Transformer 解码器与编码器类似，</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>主要区别在于解码器使用了 Masked Multi-Head Attention，</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>用于防止模型在训练过程中“看到”后续的目标词。</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>解码器也是由多个 Decoder Layer 堆叠组成。
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&#39;&#39;&#39;</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> MultiHeadAttention <span style=color:#f92672>import</span> MultiHeadAttention
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> FeedForwardNetwork <span style=color:#f92672>import</span> FeedForwardNetwork
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> ResidualConnection <span style=color:#f92672>import</span> ResidualConnection
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>DecoderLayer</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, d_model, heads, hidden_size, dropout<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>):
</span></span><span style=display:flex><span>        super(DecoderLayer, self)<span style=color:#f92672>.</span><span style=color:#a6e22e>__init__</span>()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>self_attn <span style=color:#f92672>=</span> MultiHeadAttention(d_model, heads)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>src_attn <span style=color:#f92672>=</span> MultiHeadAttention(d_model, heads)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>feed_forward <span style=color:#f92672>=</span> FeedForwardNetwork(d_model, hidden_size, dropout)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>residuals <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>ModuleList([
</span></span><span style=display:flex><span>            ResidualConnection(d_model, dropout),
</span></span><span style=display:flex><span>            ResidualConnection(d_model, dropout),            
</span></span><span style=display:flex><span>            ResidualConnection(d_model, dropout)
</span></span><span style=display:flex><span>        ])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x, memory, src_mask<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>, trg_mask<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>        <span style=color:#75715e># 自注意力机制</span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>residuals[<span style=color:#ae81ff>0</span>](x, <span style=color:#66d9ef>lambda</span> x: self<span style=color:#f92672>.</span>self_attn(x, x, x, trg_mask))
</span></span><span style=display:flex><span>        <span style=color:#75715e># 编码器-解码器注意力机制</span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>residuals[<span style=color:#ae81ff>1</span>](x, <span style=color:#66d9ef>lambda</span> x: self<span style=color:#f92672>.</span>src_attn(x, memory, memory, src_mask))
</span></span><span style=display:flex><span>        <span style=color:#75715e># 前馈网络</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>residuals[<span style=color:#ae81ff>2</span>](x, self<span style=color:#f92672>.</span>feed_forward)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;__main__&#39;</span>:
</span></span><span style=display:flex><span>    embedding_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>512</span>
</span></span><span style=display:flex><span>    heads <span style=color:#f92672>=</span> <span style=color:#ae81ff>8</span>
</span></span><span style=display:flex><span>    hidden_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>2048</span>
</span></span><span style=display:flex><span>    batch_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>8</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    decoder_layer <span style=color:#f92672>=</span> DecoderLayer(embedding_size, heads, hidden_size)
</span></span><span style=display:flex><span>    print(decoder_layer)
</span></span><span style=display:flex><span>    print(decoder_layer<span style=color:#f92672>.</span>parameters)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>rand(batch_size, <span style=color:#ae81ff>16</span>, embedding_size)
</span></span><span style=display:flex><span>    memory <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>rand(batch_size, <span style=color:#ae81ff>16</span>, embedding_size)
</span></span><span style=display:flex><span>    src_mask <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>rand(batch_size, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>16</span>)
</span></span><span style=display:flex><span>    trg_mask <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>rand(batch_size, <span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>16</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    output <span style=color:#f92672>=</span> decoder_layer(x, memory, src_mask, trg_mask)
</span></span><span style=display:flex><span>    print(output<span style=color:#f92672>.</span>shape)
</span></span></code></pre></td></tr></table></div></div><p>输出结果如下 :</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">29
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">30
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">31
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">32
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">33
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">34
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">35
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">36
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">37
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">38
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">39
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">40
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">41
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">42
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">43
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">44
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">45
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">46
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">47
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">48
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">49
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">50
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">51
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">52
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">53
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>DecoderLayer(
</span></span><span style=display:flex><span>  (self_attn): MultiHeadAttention(
</span></span><span style=display:flex><span>    (value): Linear(in_features<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>, out_features<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>    (key): Linear(in_features<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>, out_features<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>    (query): Linear(in_features<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>, out_features<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>    (fc_out): Linear(in_features<span style=color:#f92672>=</span><span style=color:#ae81ff>512</span>, out_features<span style=color:#f92672>=</span><span style=color:#ae81ff>512</span>, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>  )
</span></span><span style=display:flex><span>  (src_attn): MultiHeadAttention(
</span></span><span style=display:flex><span>    (value): Linear(in_features<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>, out_features<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>    (key): Linear(in_features<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>, out_features<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>    (query): Linear(in_features<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>, out_features<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>    (fc_out): Linear(in_features<span style=color:#f92672>=</span><span style=color:#ae81ff>512</span>, out_features<span style=color:#f92672>=</span><span style=color:#ae81ff>512</span>, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>  )
</span></span><span style=display:flex><span>  (feed_forward): FeedForwardNetwork(
</span></span><span style=display:flex><span>    (liner1): Linear(in_features<span style=color:#f92672>=</span><span style=color:#ae81ff>512</span>, out_features<span style=color:#f92672>=</span><span style=color:#ae81ff>2048</span>, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    (relu): ReLU()
</span></span><span style=display:flex><span>    (liner2): Linear(in_features<span style=color:#f92672>=</span><span style=color:#ae81ff>2048</span>, out_features<span style=color:#f92672>=</span><span style=color:#ae81ff>512</span>, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    (dropout): Dropout(p<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>, inplace<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>  )
</span></span><span style=display:flex><span>  (residuals): ModuleList(
</span></span><span style=display:flex><span>    (<span style=color:#ae81ff>0</span><span style=color:#f92672>-</span><span style=color:#ae81ff>2</span>): <span style=color:#ae81ff>3</span> x ResidualConnection(
</span></span><span style=display:flex><span>      (norm): LayerNormalization()
</span></span><span style=display:flex><span>      (dropout): Dropout(p<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>, inplace<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>  )
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span><span style=color:#f92672>&lt;</span>bound method Module<span style=color:#f92672>.</span>parameters of DecoderLayer(
</span></span><span style=display:flex><span>  (self_attn): MultiHeadAttention(
</span></span><span style=display:flex><span>    (value): Linear(in_features<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>, out_features<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>    (key): Linear(in_features<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>, out_features<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>    (query): Linear(in_features<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>, out_features<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>    (fc_out): Linear(in_features<span style=color:#f92672>=</span><span style=color:#ae81ff>512</span>, out_features<span style=color:#f92672>=</span><span style=color:#ae81ff>512</span>, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>  )
</span></span><span style=display:flex><span>  (src_attn): MultiHeadAttention(
</span></span><span style=display:flex><span>    (value): Linear(in_features<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>, out_features<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>    (key): Linear(in_features<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>, out_features<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>    (query): Linear(in_features<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>, out_features<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>    (fc_out): Linear(in_features<span style=color:#f92672>=</span><span style=color:#ae81ff>512</span>, out_features<span style=color:#f92672>=</span><span style=color:#ae81ff>512</span>, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>  )
</span></span><span style=display:flex><span>  (feed_forward): FeedForwardNetwork(
</span></span><span style=display:flex><span>    (liner1): Linear(in_features<span style=color:#f92672>=</span><span style=color:#ae81ff>512</span>, out_features<span style=color:#f92672>=</span><span style=color:#ae81ff>2048</span>, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    (relu): ReLU()
</span></span><span style=display:flex><span>    (liner2): Linear(in_features<span style=color:#f92672>=</span><span style=color:#ae81ff>2048</span>, out_features<span style=color:#f92672>=</span><span style=color:#ae81ff>512</span>, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    (dropout): Dropout(p<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>, inplace<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>  )
</span></span><span style=display:flex><span>  (residuals): ModuleList(
</span></span><span style=display:flex><span>    (<span style=color:#ae81ff>0</span><span style=color:#f92672>-</span><span style=color:#ae81ff>2</span>): <span style=color:#ae81ff>3</span> x ResidualConnection(
</span></span><span style=display:flex><span>      (norm): LayerNormalization()
</span></span><span style=display:flex><span>      (dropout): Dropout(p<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>, inplace<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>  )
</span></span><span style=display:flex><span>)<span style=color:#f92672>&gt;</span>
</span></span><span style=display:flex><span>torch<span style=color:#f92672>.</span>Size([<span style=color:#ae81ff>8</span>, <span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>512</span>])
</span></span></code></pre></td></tr></table></div></div><h3 id=decoder>Decoder<a hidden class=anchor aria-hidden=true href=#decoder>#</a></h3><p>代码报错解决中…</p><hr><blockquote><p>References</p></blockquote></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://Pan-Binghong.github.io/daily-learning/>我的博客</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script></body></html>