<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>基于Pytorch架构手撕Transformer | Pan Binghong's Tech Blog</title><meta name=keywords content><meta name=description content='深入学习Tranformer模型结构, 本来依赖GPT4o, 自己快复现完了, 结果在封装Decoder层的时候, 发生了mask维度匹配不对的问题, 查了很多资料最终还是没解决, 后期再搞吧.

Transformer 的背景

什么是Transformer ?
Transformer是谁提出的 ?
Transformer解决了什么问题 ?
Transformer核心组件有哪些 ?


注意力机制

核心公式 :





论文原图 :


Scaled Dot-product Attention


代码实现 :

代码整合(Scaled Dot-product Attention)


 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30


import torch  # 导入PyTorch库
import torch.nn.functional as F  # 导入PyTorch的nn.functional模块，包含了许多神经网络操作的函数
from math import sqrt  # 导入math库中的sqrt函数，用于计算平方根

# 定义Scaled Dot-product Attention函数
def scaled_dot_product_attention(query, key, value, query_mask=None, key_mask=None, mask=None):
    # 获取查询（query）的最后一个维度大小，即键（key）的维度
    dim_k = query.size(-1)
    
    # 计算查询和键的点积，并缩放，得到未归一化的注意力分数
    scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)
    
    # 如果提供了查询掩码（query_mask）和键掩码（key_mask），则计算掩码矩阵
    if query_mask is not None and key_mask is not None:
        mask = torch.bmm(query_mask.unsqueeze(-1), key_mask.unsqueeze(1))
    else:
        # 如果没有提供掩码，则使用之前传入的掩码（如果有的话）
        mask = mask
    
    # 如果存在掩码，则将分数矩阵中与掩码对应位置为0的分数替换为负无穷
    # 这样在应用softmax时，这些位置的权重会接近于0
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -float("inf"))
    
    # 使用softmax函数对分数进行归一化，得到注意力权重
    weights = F.softmax(scores, dim=-1)
    
    # 计算加权后的输出，即将注意力权重与值（value）相乘
    # 这里的输出是经过注意力加权后的值向量，用于下游任务
    return torch.bmm(weights, value)


Multi-head Attention

Multi-head Attention作用 :
首先通过线性映射将 Q,K,V 序列映射到特征空间，每一组线性投影后的向量表示称为一个头 (head)，然后在每组映射后的序列上再应用 Scaled Dot-product Attention, 最后将每个头的向量拼接.


代码整合(Multi-head Attention)

代码实现 :
验证代码


前馈神经网络
The Feed-Forward Layer
没啥好写的, 就是普通的全连接 + 激活函数'><meta name=author content="Pan Binghong"><link rel=canonical href=https://Pan-Binghong.github.io/daily-learning/ai/%E5%9F%BA%E4%BA%8Epytorch%E6%9E%B6%E6%9E%84%E6%89%8B%E6%92%95transformer/><link crossorigin=anonymous href=/daily-learning/assets/css/stylesheet.8c4274f6592e00e67f9d9e38f6a29695c6a6ce5eded446ed371ff6565dbb189f.css integrity="sha256-jEJ09lkuAOZ/nZ449qKWlcamzl7e1EbtNx/2Vl27GJ8=" rel="preload stylesheet" as=style><link rel=icon href=https://Pan-Binghong.github.io/daily-learning/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://Pan-Binghong.github.io/daily-learning/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://Pan-Binghong.github.io/daily-learning/favicon-32x32.png><link rel=apple-touch-icon href=https://Pan-Binghong.github.io/daily-learning/apple-touch-icon.png><link rel=mask-icon href=https://Pan-Binghong.github.io/daily-learning/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://Pan-Binghong.github.io/daily-learning/ai/%E5%9F%BA%E4%BA%8Epytorch%E6%9E%B6%E6%9E%84%E6%89%8B%E6%92%95transformer/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><link rel=stylesheet href=https://unpkg.com/github-calendar@latest/dist/github-calendar-responsive.css><style>.reading-progress-bar{position:fixed;top:0;left:0;height:3px;background:linear-gradient(90deg,var(--primary),#00d4ff);z-index:9999;transition:width .1s ease;box-shadow:0 0 10px rgba(0,212,255,.5)}</style><script>document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();const t=document.querySelector(this.getAttribute("href"));t&&t.scrollIntoView({behavior:"smooth",block:"start"})})})})</script><meta property="og:url" content="https://Pan-Binghong.github.io/daily-learning/ai/%E5%9F%BA%E4%BA%8Epytorch%E6%9E%B6%E6%9E%84%E6%89%8B%E6%92%95transformer/"><meta property="og:site_name" content="Pan Binghong's Tech Blog"><meta property="og:title" content="基于Pytorch架构手撕Transformer"><meta property="og:description" content='深入学习Tranformer模型结构, 本来依赖GPT4o, 自己快复现完了, 结果在封装Decoder层的时候, 发生了mask维度匹配不对的问题, 查了很多资料最终还是没解决, 后期再搞吧.
Transformer 的背景 什么是Transformer ? Transformer是谁提出的 ? Transformer解决了什么问题 ? Transformer核心组件有哪些 ? 注意力机制 核心公式 : 论文原图 : Scaled Dot-product Attention 代码实现 : 代码整合(Scaled Dot-product Attention) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 import torch # 导入PyTorch库 import torch.nn.functional as F # 导入PyTorch的nn.functional模块，包含了许多神经网络操作的函数 from math import sqrt # 导入math库中的sqrt函数，用于计算平方根 # 定义Scaled Dot-product Attention函数 def scaled_dot_product_attention(query, key, value, query_mask=None, key_mask=None, mask=None): # 获取查询（query）的最后一个维度大小，即键（key）的维度 dim_k = query.size(-1) # 计算查询和键的点积，并缩放，得到未归一化的注意力分数 scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k) # 如果提供了查询掩码（query_mask）和键掩码（key_mask），则计算掩码矩阵 if query_mask is not None and key_mask is not None: mask = torch.bmm(query_mask.unsqueeze(-1), key_mask.unsqueeze(1)) else: # 如果没有提供掩码，则使用之前传入的掩码（如果有的话） mask = mask # 如果存在掩码，则将分数矩阵中与掩码对应位置为0的分数替换为负无穷 # 这样在应用softmax时，这些位置的权重会接近于0 if mask is not None: scores = scores.masked_fill(mask == 0, -float("inf")) # 使用softmax函数对分数进行归一化，得到注意力权重 weights = F.softmax(scores, dim=-1) # 计算加权后的输出，即将注意力权重与值（value）相乘 # 这里的输出是经过注意力加权后的值向量，用于下游任务 return torch.bmm(weights, value) Multi-head Attention Multi-head Attention作用 : 首先通过线性映射将 Q,K,V 序列映射到特征空间，每一组线性投影后的向量表示称为一个头 (head)，然后在每组映射后的序列上再应用 Scaled Dot-product Attention, 最后将每个头的向量拼接. 代码整合(Multi-head Attention) 代码实现 : 验证代码 前馈神经网络 The Feed-Forward Layer 没啥好写的, 就是普通的全连接 + 激活函数'><meta property="og:locale" content="zh-CN"><meta property="og:type" content="article"><meta property="article:section" content="ai"><meta property="article:published_time" content="2024-10-29T01:52:00+00:00"><meta property="article:modified_time" content="2025-06-19T03:45:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="基于Pytorch架构手撕Transformer"><meta name=twitter:description content='深入学习Tranformer模型结构, 本来依赖GPT4o, 自己快复现完了, 结果在封装Decoder层的时候, 发生了mask维度匹配不对的问题, 查了很多资料最终还是没解决, 后期再搞吧.

Transformer 的背景

什么是Transformer ?
Transformer是谁提出的 ?
Transformer解决了什么问题 ?
Transformer核心组件有哪些 ?


注意力机制

核心公式 :





论文原图 :


Scaled Dot-product Attention


代码实现 :

代码整合(Scaled Dot-product Attention)


 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30


import torch  # 导入PyTorch库
import torch.nn.functional as F  # 导入PyTorch的nn.functional模块，包含了许多神经网络操作的函数
from math import sqrt  # 导入math库中的sqrt函数，用于计算平方根

# 定义Scaled Dot-product Attention函数
def scaled_dot_product_attention(query, key, value, query_mask=None, key_mask=None, mask=None):
    # 获取查询（query）的最后一个维度大小，即键（key）的维度
    dim_k = query.size(-1)
    
    # 计算查询和键的点积，并缩放，得到未归一化的注意力分数
    scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)
    
    # 如果提供了查询掩码（query_mask）和键掩码（key_mask），则计算掩码矩阵
    if query_mask is not None and key_mask is not None:
        mask = torch.bmm(query_mask.unsqueeze(-1), key_mask.unsqueeze(1))
    else:
        # 如果没有提供掩码，则使用之前传入的掩码（如果有的话）
        mask = mask
    
    # 如果存在掩码，则将分数矩阵中与掩码对应位置为0的分数替换为负无穷
    # 这样在应用softmax时，这些位置的权重会接近于0
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -float("inf"))
    
    # 使用softmax函数对分数进行归一化，得到注意力权重
    weights = F.softmax(scores, dim=-1)
    
    # 计算加权后的输出，即将注意力权重与值（value）相乘
    # 这里的输出是经过注意力加权后的值向量，用于下游任务
    return torch.bmm(weights, value)


Multi-head Attention

Multi-head Attention作用 :
首先通过线性映射将 Q,K,V 序列映射到特征空间，每一组线性投影后的向量表示称为一个头 (head)，然后在每组映射后的序列上再应用 Scaled Dot-product Attention, 最后将每个头的向量拼接.


代码整合(Multi-head Attention)

代码实现 :
验证代码


前馈神经网络
The Feed-Forward Layer
没啥好写的, 就是普通的全连接 + 激活函数'><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Ais","item":"https://Pan-Binghong.github.io/daily-learning/ai/"},{"@type":"ListItem","position":2,"name":"基于Pytorch架构手撕Transformer","item":"https://Pan-Binghong.github.io/daily-learning/ai/%E5%9F%BA%E4%BA%8Epytorch%E6%9E%B6%E6%9E%84%E6%89%8B%E6%92%95transformer/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"基于Pytorch架构手撕Transformer","name":"基于Pytorch架构手撕Transformer","description":"深入学习Tranformer模型结构, 本来依赖GPT4o, 自己快复现完了, 结果在封装Decoder层的时候, 发生了mask维度匹配不对的问题, 查了很多资料最终还是没解决, 后期再搞吧.\nTransformer 的背景 什么是Transformer ? Transformer是谁提出的 ? Transformer解决了什么问题 ? Transformer核心组件有哪些 ? 注意力机制 核心公式 : 论文原图 : Scaled Dot-product Attention 代码实现 : 代码整合(Scaled Dot-product Attention) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 import torch # 导入PyTorch库 import torch.nn.functional as F # 导入PyTorch的nn.functional模块，包含了许多神经网络操作的函数 from math import sqrt # 导入math库中的sqrt函数，用于计算平方根 # 定义Scaled Dot-product Attention函数 def scaled_dot_product_attention(query, key, value, query_mask=None, key_mask=None, mask=None): # 获取查询（query）的最后一个维度大小，即键（key）的维度 dim_k = query.size(-1) # 计算查询和键的点积，并缩放，得到未归一化的注意力分数 scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k) # 如果提供了查询掩码（query_mask）和键掩码（key_mask），则计算掩码矩阵 if query_mask is not None and key_mask is not None: mask = torch.bmm(query_mask.unsqueeze(-1), key_mask.unsqueeze(1)) else: # 如果没有提供掩码，则使用之前传入的掩码（如果有的话） mask = mask # 如果存在掩码，则将分数矩阵中与掩码对应位置为0的分数替换为负无穷 # 这样在应用softmax时，这些位置的权重会接近于0 if mask is not None: scores = scores.masked_fill(mask == 0, -float(\u0026#34;inf\u0026#34;)) # 使用softmax函数对分数进行归一化，得到注意力权重 weights = F.softmax(scores, dim=-1) # 计算加权后的输出，即将注意力权重与值（value）相乘 # 这里的输出是经过注意力加权后的值向量，用于下游任务 return torch.bmm(weights, value) Multi-head Attention Multi-head Attention作用 : 首先通过线性映射将 Q,K,V 序列映射到特征空间，每一组线性投影后的向量表示称为一个头 (head)，然后在每组映射后的序列上再应用 Scaled Dot-product Attention, 最后将每个头的向量拼接. 代码整合(Multi-head Attention) 代码实现 : 验证代码 前馈神经网络 The Feed-Forward Layer 没啥好写的, 就是普通的全连接 + 激活函数\n","keywords":[],"articleBody":"深入学习Tranformer模型结构, 本来依赖GPT4o, 自己快复现完了, 结果在封装Decoder层的时候, 发生了mask维度匹配不对的问题, 查了很多资料最终还是没解决, 后期再搞吧.\nTransformer 的背景 什么是Transformer ? Transformer是谁提出的 ? Transformer解决了什么问题 ? Transformer核心组件有哪些 ? 注意力机制 核心公式 : 论文原图 : Scaled Dot-product Attention 代码实现 : 代码整合(Scaled Dot-product Attention) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 import torch # 导入PyTorch库 import torch.nn.functional as F # 导入PyTorch的nn.functional模块，包含了许多神经网络操作的函数 from math import sqrt # 导入math库中的sqrt函数，用于计算平方根 # 定义Scaled Dot-product Attention函数 def scaled_dot_product_attention(query, key, value, query_mask=None, key_mask=None, mask=None): # 获取查询（query）的最后一个维度大小，即键（key）的维度 dim_k = query.size(-1) # 计算查询和键的点积，并缩放，得到未归一化的注意力分数 scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k) # 如果提供了查询掩码（query_mask）和键掩码（key_mask），则计算掩码矩阵 if query_mask is not None and key_mask is not None: mask = torch.bmm(query_mask.unsqueeze(-1), key_mask.unsqueeze(1)) else: # 如果没有提供掩码，则使用之前传入的掩码（如果有的话） mask = mask # 如果存在掩码，则将分数矩阵中与掩码对应位置为0的分数替换为负无穷 # 这样在应用softmax时，这些位置的权重会接近于0 if mask is not None: scores = scores.masked_fill(mask == 0, -float(\"inf\")) # 使用softmax函数对分数进行归一化，得到注意力权重 weights = F.softmax(scores, dim=-1) # 计算加权后的输出，即将注意力权重与值（value）相乘 # 这里的输出是经过注意力加权后的值向量，用于下游任务 return torch.bmm(weights, value) Multi-head Attention Multi-head Attention作用 : 首先通过线性映射将 Q,K,V 序列映射到特征空间，每一组线性投影后的向量表示称为一个头 (head)，然后在每组映射后的序列上再应用 Scaled Dot-product Attention, 最后将每个头的向量拼接. 代码整合(Multi-head Attention) 代码实现 : 验证代码 前馈神经网络 The Feed-Forward Layer 没啥好写的, 就是普通的全连接 + 激活函数\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 from torch import nn # 定义FeedForward类，继承自nn.Module class FeedForward(nn.Module): # 初始化函数 def __init__(self, config): super().__init__() # 调用基类的初始化方法 # 定义第一个线性层，将输入的隐藏状态映射到中间维度 self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size) # 定义第二个线性层，将中间维度的表示映射回原始的隐藏状态维度 self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size) # 定义GELU激活函数 self.gelu = nn.GELU() # 定义Dropout层，用于防止过拟合 self.dropout = nn.Dropout(config.hidden_dropout_prob) # 前向传播函数 def forward(self, x): # 应用第一个线性层 x = self.linear_1(x) # 应用GELU激活函数 x = self.gelu(x) # 应用第二个线性层 x = self.linear_2(x) # 应用Dropout x = self.dropout(x) # 返回最终的输出 return x 与上面构建的注意力机制串联测试 :\n1 2 3 feed_forward = FeedForward(config) ff_outputs = feed_forward(attn_output) print(ff_outputs.size()) #torch.Size([1, 2, 768]) 层归一化 \u0026 残差连接 层归一化模块需要包含在残差模块内, 主要作用为 : 将输入的一批向量, 每一个都做标准化处理, 处理为 : 均值为零, 且有单位方差 残差连接主要作用为 : 是通过直接将输入绕过中间层的计算，帮助模型更容易训练深层网络，避免梯度消失问题并促进信息流动 Transformer Encoder Layer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 from torch import nn # 定义TransformerEncoderLayer类，继承自nn.Module class TransformerEncoderLayer(nn.Module): # 初始化函数 def __init__(self, config): super().__init__() # 调用基类的初始化方法 # 定义第一个层归一化，用于注意力机制之前 self.layer_norm_1 = nn.LayerNorm(config.hidden_size) # 定义第二个层归一化，用于前馈网络之前 self.layer_norm_2 = nn.LayerNorm(config.hidden_size) # 定义多头注意力机制 self.attention = MultiHeadAttention(config) # 定义前馈神经网络 self.feed_forward = FeedForward(config) # 前向传播函数 def forward(self, x, mask=None): # 应用第一个层归一化 hidden_state = self.layer_norm_1(x) # 应用注意力机制，并将结果与输入进行残差连接 # 注意力机制的输出将与输入x相加，得到更新后的x x = x + self.attention(hidden_state, hidden_state, hidden_state, mask=mask) # 应用第二个层归一化 # 注意这里的self.layer_norm_2(x)实际上是对更新后的x进行归一化 hidden_state = self.layer_norm_2(x) # 应用前馈网络，并将结果与更新后的x进行残差连接 x = x + self.feed_forward(hidden_state) # 返回最终的输出x return x 代码验证 :\n1 2 3 4 5 encoder_layer = TransformerEncoderLayer(config) print(inputs_embeds.shape) print(encoder_layer(inputs_embeds).size()) #torch.Size([1, 5, 768]) #torch.Size([1, 5, 768]) 绝对位置编码 注意力机制无法捕获词语之间的位置信息，因此 Transformer 模型还使用 Positional Embeddings 添加了词语的位置信息。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 from torch import nn, LongTensor, arange # 定义Embeddings类，继承自nn.Module class Embeddings(nn.Module): # 初始化函数 def __init__(self, config): super().__init__() # 调用基类的初始化方法 # 定义词嵌入层，将词ID映射到词向量 self.token_embeddings = nn.Embedding(config.vocab_size, config.hidden_size) # 定义位置嵌入层，为序列中的每个位置生成一个唯一的位置向量 self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size) # 定义层归一化，用于稳定训练过程 self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12) # 定义Dropout层，用于防止过拟合 self.dropout = nn.Dropout(config.hidden_dropout_prob) # 前向传播函数 def forward(self, input_ids): # 根据输入序列的长度创建位置ID seq_length = input_ids.size(1) # 获取序列长度 position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0) # 创建位置ID序列 # 创建词嵌入和位置嵌入 token_embeddings = self.token_embeddings(input_ids) # 通过词嵌入层获取词嵌入 position_embeddings = self.position_embeddings(position_ids) # 通过位置嵌入层获取位置嵌入 # 将词嵌入和位置嵌入相加，得到最终的嵌入表示 embeddings = token_embeddings + position_embeddings # 应用层归一化 embeddings = self.layer_norm(embeddings) # 应用Dropout embeddings = self.dropout(embeddings) # 返回最终的嵌入表示 return embeddings # 创建Embeddings层的实例，并使用config配置 embedding_layer = Embeddings(config) # 使用embedding_layer处理输入的词ID，并打印输出的大小 # 这里假设inputs.input_ids是之前通过tokenizer得到的词ID序列 print(embedding_layer(inputs.input_ids).size()) #torch.Size([1, 5, 768]) Transformer Encoder 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 from torch import nn # 定义TransformerEncoder类，继承自nn.Module class TransformerEncoder(nn.Module): # 初始化函数 def __init__(self, config): super().__init__() # 调用基类的初始化方法 # 创建嵌入层实例，用于将输入的词ID转换为嵌入向量 self.embeddings = Embeddings(config) # 创建一个包含多个Transformer编码器层的列表 # num_hidden_layers表示编码器中隐藏层的数量 self.layers = nn.ModuleList([TransformerEncoderLayer(config) for _ in range(config.num_hidden_layers)]) # 前向传播函数 def forward(self, x, mask=None): # 首先通过嵌入层处理输入x x = self.embeddings(x) # 然后依次通过每个编码器层 for layer in self.layers: # 将当前层的输出作为下一层的输入，并传递掩码（如果有的话） x = layer(x, mask=mask) # 返回最终的编码器输出 return x 测试代码 :\n1 2 encoder = TransformerEncoder(config) print(encoder(inputs.input_ids).size()) #torch.Size([1, 5, 768]) 完整代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 import torch from torch import nn import torch.nn.functional as F from math import sqrt class AttentionHead(nn.Module): def __init__(self, embed_dim, head_dim): super().__init__() self.q = nn.Linear(embed_dim, head_dim) self.k = nn.Linear(embed_dim, head_dim) self.v = nn.Linear(embed_dim, head_dim) def forward(self, query, key, value, mask=None): query, key, value = self.q(query), self.k(key), self.v(value) scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(query.size(-1)) if mask is not None: scores = scores.masked_fill(mask == 0, -float(\"inf\")) weights = F.softmax(scores, dim=-1) return torch.bmm(weights, value) class MultiHeadAttention(nn.Module): def __init__(self, config): super().__init__() embed_dim = config.hidden_size num_heads = config.num_attention_heads head_dim = embed_dim // num_heads self.heads = nn.ModuleList( [AttentionHead(embed_dim, head_dim) for _ in range(num_heads)] ) self.output_linear = nn.Linear(embed_dim, embed_dim) def forward(self, query, key, value, mask=None, query_mask=None, key_mask=None): if query_mask is not None and key_mask is not None: mask = torch.bmm(query_mask.unsqueeze(-1), key_mask.unsqueeze(1)) x = torch.cat([h(query, key, value, mask) for h in self.heads], dim=-1) x = self.output_linear(x) return x class FeedForward(nn.Module): def __init__(self, config): super().__init__() self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size) self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size) self.gelu = nn.GELU() self.dropout = nn.Dropout(config.hidden_dropout_prob) def forward(self, x): x = self.linear_1(x) x = self.gelu(x) x = self.linear_2(x) x = self.dropout(x) return x class TransformerEncoderLayer(nn.Module): def __init__(self, config): super().__init__() self.layer_norm_1 = nn.LayerNorm(config.hidden_size) self.layer_norm_2 = nn.LayerNorm(config.hidden_size) self.attention = MultiHeadAttention(config) self.feed_forward = FeedForward(config) def forward(self, x, mask=None): # Apply layer normalization and then copy input into query, key, value hidden_state = self.layer_norm_1(x) # Apply attention with a skip connection x = x + self.attention(hidden_state, hidden_state, hidden_state, mask=mask) # Apply feed-forward layer with a skip connection x = x + self.feed_forward(self.layer_norm_2(x)) return x class Embeddings(nn.Module): def __init__(self, config): super().__init__() self.token_embeddings = nn.Embedding(config.vocab_size, config.hidden_size) self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size) self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12) self.dropout = nn.Dropout() def forward(self, input_ids): # Create position IDs for input sequence seq_length = input_ids.size(1) position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0) # Create token and position embeddings token_embeddings = self.token_embeddings(input_ids) position_embeddings = self.position_embeddings(position_ids) # Combine token and position embeddings embeddings = token_embeddings + position_embeddings embeddings = self.layer_norm(embeddings) embeddings = self.dropout(embeddings) return embeddings class TransformerEncoder(nn.Module): def __init__(self, config): super().__init__() self.embeddings = Embeddings(config) self.layers = nn.ModuleList( [TransformerEncoderLayer(config) for _ in range(config.num_hidden_layers)] ) def forward(self, x, mask=None): x = self.embeddings(x) for layer in self.layers: x = layer(x, mask) return x if __name__ == '__main__': from transformers import AutoConfig from transformers import AutoTokenizer model_ckpt = \"bert-base-uncased\"\\ cache_dir = './pretrained_model'\\ tokenizer = AutoTokenizer.from_pretrained(model_ckpt, cache_dir=cache_dir) config = AutoConfig.from_pretrained(model_ckpt) text = \"hello world\" inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False) encoder = TransformerEncoder(config) print(encoder(inputs.input_ids).size()) [!IMPORTANT]\n绝对位置编码 使用Pytorch实现Transformer中的绝对位置编码底层代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 #!/usr/bin/env python # -*- encoding: utf-8 -*- ''' @File : Positional_Encoding.py @Time : 2024/09/26 11:23:36 @Author : pan binghong @Email : 19909442097@163.com @description : Transformer中的绝对位置编码底层代码实现 ''' import torch import torch.nn as nn import math from transformers import BertTokenizer import os file_path = os.path.abspath(__file__) dir = os.path.dirname(file_path) os.chdir(dir) class PositionalEncoding(nn.Module): def __init__(self, d_model, max_len=5000): '''' :param d_model: 模型的维度 :param max_len: 序列的最大长度 ''' super(PositionalEncoding, self).__init__() # 创建一个形状为 (max_len, d_model) 的矩阵, 用于存储位置编码 pe = torch.zeros(max_len, d_model) # 创建一个形状为 (max_len, 1) 的矩阵, 用于存储位置信息, 保存索引值 e.g.[0, 1, 2, ... , max_len-1] position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # 这段代码计算位置编码中的两个频率。具体来说，它生成一个从0到d_model（不包括d_model）的偶数序列， # 然后将这些偶数转换为浮点数，并乘以一个常数因子 (-math.log(10000.0) / d_model)。 # 这个常数因子是通过对10000取自然对数并除以d_model得到的。 # 最后，通过torch.exp函数计算这些值的指数，得到最终的div_term。 div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # 应用正弦函数 得到偶数索引位置编码 pe[:, 0::2] = torch.sin(position * div_term) # 应用余弦函数 得到奇数索引位置编码 pe[:, 1::2] = torch.cos(position * div_term) # 增加一个 batch 维度, 使其能够与输入一起使用 pe = pe.unsqueeze(0) # 将位置编码矩阵注册为一个参数, 并将其添加到模型参数列表中 self.register_buffer('pe', pe) def forward(self, x): '''' :param x: 输入的序列张量, shape为: :return: 输出的序列张量, shape为: ''' x = x + self.pe[:, :x.size(1), :] return x if __name__ == '__main__': # 初始化参数 d_model = 512 max_len = 2048 # 初始化位置编码 pos_encoder = PositionalEncoding(d_model, max_len) # 初始化 tokenizer (这里以 Bert 为例) tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', cache_dir='./cache') # 使用输入\"hello world\" input_text = \"hello world\" input_ids = torch.tensor([tokenizer.encode(input_text, add_special_tokens=True)]) # 创建一个形状为 (1, seq_len, d_model) 的零矩阵 x = torch.zeros(1, input_ids.size(1), d_model) # 应用位置编码 output = pos_encoder(x) print(\"Input Text:\", input_text) print(\"Input IDs Shape:\", input_ids.shape) print(\"Output Shape:\", output.shape) print(\"Output:\", output) 以上案例为 : 当输入hello world, 经过编码后输出其对应的信息\n1 2 3 4 5 6 7 8 9 10 11 Input Text: hello world Input IDs Shape: torch.Size([1, 4]) Output Shape: torch.Size([1, 4, 512]) Output: tensor([[[ 0.0000e+00, 1.0000e+00, 0.0000e+00, ..., 1.0000e+00, 0.0000e+00, 1.0000e+00], [ 8.4147e-01, 5.4030e-01, 8.2186e-01, ..., 1.0000e+00, 1.0366e-04, 1.0000e+00], [ 9.0930e-01, -4.1615e-01, 9.3641e-01, ..., 1.0000e+00, 2.0733e-04, 1.0000e+00], [ 1.4112e-01, -9.8999e-01, 2.4509e-01, ..., 1.0000e+00, 3.1099e-04, 1.0000e+00]]]) 多头注意力机制 使用Pytorch实现多头注意力机制的底层代码, 含例子.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 #!/usr/bin/env python # -*- encoding: utf-8 -*- ''' @File : MultiHeadAttention.py @Time : 2024/09/27 09:50:43 @Author : pan binghong @Email : 19909442097@163.com @description : ''' import os import torch import torch.nn as nn from transformers import BertTokenizer # 获取当前文件的绝对路径 file_path = os.path.abspath(__file__) # 获取当前文件所在的目录路径 dir = os.path.dirname(file_path) # 将当前工作目录更改为当前文件所在的目录 os.chdir(dir) class MultiHeadAttention(nn.Module): def __init__(self, embed_size, heads): ''' 初始化多头注意力机制 :param embed_size: 输入嵌入向量的维度 :param num_heads: 多头注意力机制的头数 ''' super(MultiHeadAttention, self).__init__() assert embed_size % heads == 0, \"嵌入维度必须能被头数整除\" self.embed_size = embed_size self.heads = heads # 每个头的维度 self.heads_dim = embed_size // heads self.value = nn.Linear(self.heads_dim, self.heads_dim, bias=False) self.key = nn.Linear(self.heads_dim, self.heads_dim, bias=False) self.query = nn.Linear(self.heads_dim, self.heads_dim, bias=False) self.fc_out = nn.Linear(heads * self.heads_dim, embed_size) def forward(self, query, keys, values, mask): N = query.shape[0] value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1] # 将输入的数据划分为多个头, 先调整shape为 (N, value_len, heads, heads_dim) values = values.reshape(N, value_len, self.heads, self.heads_dim) keys = keys.reshape(N, key_len, self.heads, self.heads_dim) query = query.reshape(N, query_len, self.heads, self.heads_dim) # 初始化Q, K, V矩阵 values = self.value(values) keys = self.key(keys) query = self.query(query) # 计算注意力分数 # 这行代码使用爱因斯坦求和约定（Einstein Summation Convention）来计算注意力分数。 # 具体来说，它通过矩阵乘法计算query和keys之间的点积，然后将其重塑为所需的形状。 # # 参数解释： # - \"nqhd,nkhd-\u003enhqk\" 是爱因斯坦求和约定的字符串表示。 # - \"n\" 表示批次大小（batch size）。 # - \"q\" 表示查询序列的长度（query length）。 # - \"h\" 表示注意力头数（number of heads）。 # - \"d\" 表示每个头的维度（dimension per head）。 # - \"k\" 表示键序列的长度（key length）。 # # 具体操作： # - query 的形状为 (N, query_len, heads, heads_dim)。 # - keys 的形状为 (N, key_len, heads, heads_dim)。 # - 通过 torch.einsum 计算 query 和 keys 的点积，结果的形状为 (N, heads, query_len, key_len)。 # # 计算过程： # - 对于每个批次（n），每个头（h），计算 query 和 keys 的点积，得到一个形状为 (query_len, key_len) 的矩阵。 # - 最终结果是一个形状为 (N, heads, query_len, key_len) 的张量，表示每个查询和每个键之间的注意力分数。 energy = torch.einsum(\"nqhd,nkhd-\u003enhqk\", [query, keys]) # 应用注意力机制 if mask is not None: energy = energy.masked_fill(mask == 0, float('-1e20')) attention = torch.softmax(energy / (self.embed_size ** (1/2)), dim=3) out = torch.einsum(\"nhql,nlhd-\u003enqhd\", [attention, values]).reshape( N, query_len, self.heads * self.heads_dim ) out = self.fc_out(out) return out if __name__ == '__main__': tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', cache_dir='./cache') input_text = \"Hello world!\" # 初始化随机keys,values, query tokens = tokenizer.tokenize(input_text) token_ids = tokenizer.convert_tokens_to_ids(tokens) token_ids = torch.tensor([token_ids]).long() batch_size,seq_length = token_ids.shape embed_size =512 heads = 8 values = torch.rand(batch_size, seq_length, embed_size) keys = torch.rand(batch_size, seq_length, embed_size) query = torch.rand(batch_size, seq_length, embed_size) mask = None attention = MultiHeadAttention(embed_size, heads) out = attention(query, keys, values, mask) print(f\"Tokens: {tokens}\") print(f\"Token IDs: {token_ids}\") print(out.shape) print(f\"Multi-head Attention Output: \\n{out}\") 运行以上代码后输出 :\n1 2 3 4 5 6 7 8 Tokens: ['hello', 'world', '!'] Token IDs: tensor([[7592, 2088, 999]]) torch.Size([1, 3, 512]) Multi-head Attention Output: tensor([[[-0.0855, -0.2076, 0.0354, ..., 0.1631, -0.0024, -0.1372], [-0.0852, -0.2079, 0.0366, ..., 0.1636, -0.0025, -0.1374], [-0.0860, -0.2076, 0.0364, ..., 0.1644, -0.0018, -0.1361]]], grad_fn=\u003cViewBackward0\u003e) 前馈神经网络 就是一个简单的全连接层…没啥好说的, 看代码 :\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 #!/usr/bin/env python # -*- encoding: utf-8 -*- ''' @File : FeedForwardNetwork.py @Time : 2024/09/27 14:46:48 @Author : pan binghong @Email : 19909442097@163.com @description : ''' import os import torch import torch.nn as nn from transformers import BertTokenizer file_path = os.path.abspath(__file__) dir = os.path.dirname(file_path) os.chdir(dir) class FeedForwardNetwork(nn.Module): def __init__(self, d_model, hidden_size, dropout=0.1): ''' :param d_model:输入的特征维度大小 :param hidden_size:隐藏层大小 :param dropout:dropout概率 ''' super(FeedForwardNetwork, self).__init__() self.liner1 = nn.Linear(d_model, hidden_size) self.relu = nn.ReLU() self.liner2 = nn.Linear(hidden_size, d_model) self.dropout = nn.Dropout(dropout) def forward(self, x): ''' :param x:输入的特征 :return:输出的特征 ''' x = self.liner1(x) x = self.relu(x) x = self.liner2(x) x = self.dropout(x) return x if __name__ == '__main__': tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',cache_dir='./cache') input_text = \"Hello world\" tokens = tokenizer.tokenize(input_text) token_ids = tokenizer.convert_tokens_to_ids(tokens) token_ids = torch.tensor([token_ids]).long() d_model = token_ids.shape[1] hidden_size = 256 ff_netword = FeedForwardNetwork(d_model, hidden_size) output = ff_netword(token_ids.float()) print(f'Input tokens: \\n{tokens}') print(f'Input token_ids: \\n{token_ids}') print(f'Output from FeedForwardNetwork: \\n{output}') print(f'Output shape: \\n{output.shape}') 输出结果 :\n1 2 3 4 5 6 7 8 Input tokens: ['hello', 'world'] Input token_ids: tensor([[7592, 2088]]) Output from FeedForwardNetwork: tensor([[2332.0767, 1194.7576]], grad_fn=\u003cMulBackward0\u003e) Output shape: torch.Size([1, 2]) 层归一化 \u0026 残差连接 Layer Normalization\n代码如下 :\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 #!/usr/bin/env python # -*- encoding: utf-8 -*- ''' @File : LayerNormalization.py @Time : 2024/09/27 15:17:48 @Author : pan binghong @Email : 19909442097@163.com @description : ''' import torch import torch.nn as nn class LayerNormalization(nn.Module): def __init__(self, features, eps=1e-6): ''' 初始化层归一化模块 :param features: 特征维度大小 :param eps: 防止除零的小常数 ''' super(LayerNormalization, self).__init__() # 调用父类nn.Module的初始化方法 self.eps = eps # 设置防止除零的小常数 self.gain = nn.Parameter(torch.ones(features)) # 初始化增益参数，形状为(features,)，初始值为1 self.bias = nn.Parameter(torch.zeros(features)) # 初始化偏置参数，形状为(features,)，初始值为0 def forward(self, x): ''' 前向传播函数 :param x: 输入张量，形状为(batch_size, seq_len, features) :return: 归一化后的输出张量 ''' mean = x.mean(-1, keepdim=True) # 计算输入张量在最后一个维度上的均值，保持维度 std = x.std(-1, keepdim=True) # 计算输入张量在最后一个维度上的标准差，保持维度 return self.gain * (x - mean) / (std + self.eps) + self.bias # 应用层归一化公式并返回结果 if __name__ == \"__main__\": batch_size = 1 seq_len = 2048 features = 4096 # 创建一个简单的输入张量 x = torch.randn(batch_size, seq_len, features) # 随机初始化输入张量 # 初始化层归一化层 ln = LayerNormalization(features) # 应用层归一化 normalized_x = ln(x) # 打印原始和归一化后的张量 print(\"原始输入张量:\") print(x) print(\"\\n归一化后的输出张量:\") print(normalized_x) print(\"\\n归一化后的维度:\") print(normalized_x.shape) 输出 :\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 原始输入张量: tensor([[[ 0.7926, 0.9379, 0.9247, ..., -0.8849, 0.4262, -0.1126], [-1.0093, -0.3612, -0.7135, ..., 1.7116, -0.1164, 0.8453], [ 1.1906, -0.3223, 0.3553, ..., -0.0293, 0.6014, 0.3705], ..., [-0.3714, -0.1437, 1.2062, ..., -0.7603, -1.2689, -0.4045], [-0.6838, -0.5469, 0.2328, ..., -0.4500, -1.1035, -0.2005], [ 0.4267, -0.5163, 1.1316, ..., -0.1339, -0.4004, 0.2841]]]) 归一化后的输出张量: tensor([[[ 0.7959, 0.9418, 0.9285, ..., -0.8873, 0.4283, -0.1123], [-0.9919, -0.3548, -0.7011, ..., 1.6830, -0.1141, 0.8314], [ 1.2045, -0.3012, 0.3732, ..., -0.0096, 0.6181, 0.3883], ..., [-0.3827, -0.1589, 1.1681, ..., -0.7650, -1.2649, -0.4152], [-0.7084, -0.5709, 0.2130, ..., -0.4735, -1.1304, -0.2226], [ 0.3982, -0.5383, 1.0983, ..., -0.1585, -0.4232, 0.2567]]], grad_fn=\u003cAddBackward0\u003e) 归一化后的维度: torch.Size([1, 2048, 4096]) Residual Connections 残差连接代码实现 :\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 #!/usr/bin/env python # -*- encoding: utf-8 -*- ''' @File : ResidualConnection.py @Time : 2024/09/27 15:33:49 @Author : pan binghong @Email : 19909442097@163.com @description : ''' import torch import torch.nn as nn from LayerNormalization import LayerNormalization class ResidualConnection(nn.Module): def __init__(self, size, dropout): super(ResidualConnection, self).__init__() # 正确调用父类构造函数 self.norm = LayerNormalization(size) # 在父类构造函数之后设置属性 self.dropout = nn.Dropout(dropout) def forward(self, x, sublayer): return x + self.dropout(sublayer(self.norm(x))) if __name__ == '__main__': size = 512 dropout = 0.1 residual_module = ResidualConnection(size, dropout) x = torch.rand(32, 10, size) sublayer = nn.Linear(size, size) output = residual_module(x, sublayer) print(f'output shape: \\n{output.shape}') print(f'out: \\n{output}') 输出内容 :\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 output shape: torch.Size([32, 10, 512]) out: tensor([[[ 0.3520, 0.6030, -0.2354, ..., 0.8682, -0.3730, 0.1524], [ 1.4186, 0.5724, 0.2079, ..., 0.4792, 1.2186, 0.6546], [ 1.0631, 1.0479, 0.4523, ..., 1.4115, 1.5870, 0.7165], ..., [ 0.0567, 2.4177, 1.6159, ..., -0.5791, 0.4186, 0.6306], [ 1.9228, 0.8467, 0.7399, ..., -0.5388, -0.1463, 1.1880], [ 0.1197, 0.0182, 0.2941, ..., 0.5807, 0.3925, -0.4700]], [[ 0.3333, 0.8267, 0.3082, ..., 1.3075, 0.2646, 0.4092], [ 0.7507, 0.9554, 0.2910, ..., 1.1357, 0.4684, 0.4244], [ 0.6514, 0.2003, -0.3597, ..., 0.3598, 0.4869, -0.1992], ..., [-0.0030, 1.0668, 0.3075, ..., 0.5691, 0.2380, -0.1247], [ 0.9135, 0.4341, 0.0337, ..., 0.3597, -0.7784, 0.8458], [-0.1205, 0.6370, 1.0110, ..., 0.0136, 0.6965, 0.2374]], [[ 1.7201, 0.9184, 1.4459, ..., 0.6506, -0.4328, 0.4222], [ 0.1091, -0.7816, 0.0389, ..., 0.4128, 0.5077, 1.2345], [ 0.3344, 0.5098, 0.1903, ..., 0.4348, 0.1655, 1.0356], ..., [ 0.6577, 1.0894, 0.3509, ..., 0.9495, -0.3284, 0.8220], [-0.3985, 1.0728, 1.2246, ..., 0.6893, 0.3443, 0.4279], [ 0.2423, 0.0113, 1.3485, ..., -0.2677, -0.0577, 0.4010]], ..., [[ 1.4007, 1.3420, 0.1977, ..., 0.8533, 0.1814, 0.2697], [ 0.6700, 0.2914, 0.7087, ..., 0.4371, -0.6651, -0.3476], [ 1.7371, -0.3646, -0.7164, ..., -0.2941, 1.5312, 0.1496], ..., [ 1.4492, 1.6843, 0.4061, ..., 0.2602, 0.3412, 0.9145], [ 0.5208, 0.7262, 1.1507, ..., 1.6124, 0.1670, -0.7637], [ 0.6195, 1.8701, 0.6011, ..., 0.7136, 0.1405, 0.7195]], [[ 1.2362, 0.3252, 1.5944, ..., -0.6239, -0.5983, 0.0794], [ 1.1429, -0.8601, 0.6993, ..., 0.1767, -0.7440, 0.6210], [ 0.1337, -0.8456, -0.4567, ..., 1.0074, 0.6997, -0.7483], ..., [-0.2421, 0.8441, 1.1355, ..., 1.2241, 0.0689, -0.8084], [ 0.1203, -0.1496, -0.3044, ..., 0.2365, 1.0541, 0.5421], [ 0.7855, 0.0565, 0.9192, ..., 0.8071, -0.9707, 1.3335]], [[ 0.6022, 1.4715, 0.2470, ..., -0.0782, -0.6734, 0.8383], [ 0.8088, 0.3382, 0.6812, ..., 1.1501, 1.0537, 0.5442], [-0.0593, 1.7771, 0.0580, ..., -0.0578, 0.7382, 1.2158], ..., [ 1.1114, 0.3564, 0.8435, ..., 0.1796, 1.2682, 0.5146], [ 1.0304, 1.2170, 0.8374, ..., 2.2357, 0.2286, 0.3899], [ 0.5022, 0.3711, 0.2397, ..., 0.9505, 0.3877, -0.2048]]], grad_fn=\u003cAddBackward0\u003e) 编码器 Encoder Layer 编码器层由之前构建的多头注意力机制, 前馈神经网络, 残差模块, 层归一化组合构成\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 #!/usr/bin/env python # -*- encoding: utf-8 -*- ''' @File : EncoderLayer.py @Time : 2024/09/27 16:51:01 @Author : pan binghong @Email : 19909442097@163.com @description : 多头注意力机制、前馈神经网络、位置编码、残差连接和层归一化结合起来，\\n 构建一个 Encoder Layer。Encoder Layer 是 Transformer 编码器的基本组成单位。 ''' import torch import torch.nn as nn from MultiHeadAttention import MultiHeadAttention from FeedForwardNetwork import FeedForwardNetwork from LayerNormalization import LayerNormalization #在残差连接模块中完成 from ResidualConnection import ResidualConnection class EncoderLayer(nn.Module): def __init__(self, d_model, num_heads, hidden_size, dropout=0.1): super(EncoderLayer, self).__init__() self.d_model = d_model self.self_attn = MultiHeadAttention(d_model, num_heads) self.pos_ffn = FeedForwardNetwork(d_model, hidden_size, dropout) self.residual = nn.ModuleList([ ResidualConnection(d_model, dropout), ResidualConnection(d_model, dropout) ]) def forward(self, x, mask=None): x = self.residual[0](x, lambda x: self.self_attn(x, x, x, mask)) x = self.residual[1](x, self.pos_ffn) return x # 示例 if __name__ == \"__main__\": # 示例输入 d_model = 512 num_heads = 8 hidden_size = 2048 dropout = 0.1 batch_size = 32 seq_len = 10 encoder_layer = EncoderLayer(d_model, num_heads, hidden_size, dropout) input_tensor = torch.randn(batch_size, seq_len, d_model) mask = torch.ones(batch_size, 1,seq_len, seq_len) # 前向传播 output_tensor = encoder_layer(input_tensor, mask) # 输出结果 print(\"Input shape:\", input_tensor.shape) print(\"Output shape:\", output_tensor.shape) 输出结果为 :\n1 2 Input shape: torch.Size([32, 10, 512]) Output shape: torch.Size([32, 10, 512]) Encoder Encoder由n个Encoder_Lyaer组成, 详细代码如下 :\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 #!/usr/bin/env python # -*- encoding: utf-8 -*- ''' @File : Encoder.py @Time : 2024/09/29 08:39:15 @Author : pan binghong @Email : 19909442097@163.com @description : ''' import os import sys import torch import torch.nn as nn from EncoderLayer import EncoderLayer from LayerNormalization import LayerNormalization class Encoder(nn.Module): def __init__(self, encoder_layer, num_layers): super(Encoder, self).__init__() self.encoder_layer = nn.ModuleList([ encoder_layer for _ in range(num_layers) ]) self.norm = LayerNormalization(encoder_layer.d_model) def forward(self, src, mask=None): for layer in self.encoder_layer: src = layer(src, mask) return self.norm(src) if __name__ == '__main__': d_model = 512 num_heades = 8 hidden_size = 2048 droupout = 0.1 num_layers = 6 encoder_layer = EncoderLayer(d_model, num_heades, hidden_size, droupout) encoder = Encoder(encoder_layer, num_layers) src = torch.rand(32, 10 , d_model) output = encoder(src) print(output.shape) 输出结果为 :\n1 2 Input shape: torch.Size([32, 10, 512]) Output shape: torch.Size([32, 10, 512]) 32个样本 每个样本有10个时间步 每个时间步的特征向量有512个维度 解码器 Decoder_Layer 编码器与解码器最大的区别就是使用了Mask Multi-Head Attention, 用于防止模型训练过程中”看到”后续的目标词, 由多个解码器层构成, 代码如下 :\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 #!/usr/bin/env python # -*- encoding: utf-8 -*- ''' @File : DecoderLayer.py @Time : 2024/09/29 09:34:04 @Author : pan binghong @Email : 19909442097@163.com @description : Transformer 解码器与编码器类似，\\n 主要区别在于解码器使用了 Masked Multi-Head Attention，\\n 用于防止模型在训练过程中“看到”后续的目标词。\\n 解码器也是由多个 Decoder Layer 堆叠组成。 ''' import torch import torch.nn as nn from MultiHeadAttention import MultiHeadAttention from FeedForwardNetwork import FeedForwardNetwork from ResidualConnection import ResidualConnection class DecoderLayer(nn.Module): def __init__(self, d_model, heads, hidden_size, dropout=0.1): super(DecoderLayer, self).__init__() self.self_attn = MultiHeadAttention(d_model, heads) self.src_attn = MultiHeadAttention(d_model, heads) self.feed_forward = FeedForwardNetwork(d_model, hidden_size, dropout) self.residuals = nn.ModuleList([ ResidualConnection(d_model, dropout), ResidualConnection(d_model, dropout), ResidualConnection(d_model, dropout) ]) def forward(self, x, memory, src_mask=None, trg_mask=None): # 自注意力机制 x = self.residuals[0](x, lambda x: self.self_attn(x, x, x, trg_mask)) # 编码器-解码器注意力机制 x = self.residuals[1](x, lambda x: self.src_attn(x, memory, memory, src_mask)) # 前馈网络 return self.residuals[2](x, self.feed_forward) if __name__ == '__main__': embedding_size = 512 heads = 8 hidden_size = 2048 batch_size = 8 decoder_layer = DecoderLayer(embedding_size, heads, hidden_size) print(decoder_layer) print(decoder_layer.parameters) x = torch.rand(batch_size, 16, embedding_size) memory = torch.rand(batch_size, 16, embedding_size) src_mask = torch.rand(batch_size, 1, 16) trg_mask = torch.rand(batch_size, 16, 16) output = decoder_layer(x, memory, src_mask, trg_mask) print(output.shape) 输出结果如下 :\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 DecoderLayer( (self_attn): MultiHeadAttention( (value): Linear(in_features=64, out_features=64, bias=False) (key): Linear(in_features=64, out_features=64, bias=False) (query): Linear(in_features=64, out_features=64, bias=False) (fc_out): Linear(in_features=512, out_features=512, bias=True) ) (src_attn): MultiHeadAttention( (value): Linear(in_features=64, out_features=64, bias=False) (key): Linear(in_features=64, out_features=64, bias=False) (query): Linear(in_features=64, out_features=64, bias=False) (fc_out): Linear(in_features=512, out_features=512, bias=True) ) (feed_forward): FeedForwardNetwork( (liner1): Linear(in_features=512, out_features=2048, bias=True) (relu): ReLU() (liner2): Linear(in_features=2048, out_features=512, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (residuals): ModuleList( (0-2): 3 x ResidualConnection( (norm): LayerNormalization() (dropout): Dropout(p=0.1, inplace=False) ) ) ) \u003cbound method Module.parameters of DecoderLayer( (self_attn): MultiHeadAttention( (value): Linear(in_features=64, out_features=64, bias=False) (key): Linear(in_features=64, out_features=64, bias=False) (query): Linear(in_features=64, out_features=64, bias=False) (fc_out): Linear(in_features=512, out_features=512, bias=True) ) (src_attn): MultiHeadAttention( (value): Linear(in_features=64, out_features=64, bias=False) (key): Linear(in_features=64, out_features=64, bias=False) (query): Linear(in_features=64, out_features=64, bias=False) (fc_out): Linear(in_features=512, out_features=512, bias=True) ) (feed_forward): FeedForwardNetwork( (liner1): Linear(in_features=512, out_features=2048, bias=True) (relu): ReLU() (liner2): Linear(in_features=2048, out_features=512, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (residuals): ModuleList( (0-2): 3 x ResidualConnection( (norm): LayerNormalization() (dropout): Dropout(p=0.1, inplace=False) ) ) )\u003e torch.Size([8, 16, 512]) Decoder 代码报错解决中…\nReferences\n","wordCount":"4018","inLanguage":"en","datePublished":"2024-10-29T01:52:00Z","dateModified":"2025-06-19T03:45:00Z","author":{"@type":"Person","name":"Pan Binghong"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://Pan-Binghong.github.io/daily-learning/ai/%E5%9F%BA%E4%BA%8Epytorch%E6%9E%B6%E6%9E%84%E6%89%8B%E6%92%95transformer/"},"publisher":{"@type":"Organization","name":"Pan Binghong's Tech Blog","logo":{"@type":"ImageObject","url":"https://Pan-Binghong.github.io/daily-learning/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://Pan-Binghong.github.io/daily-learning/ accesskey=h title="Pan Binghong's Tech Blog (Alt + H)">Pan Binghong's Tech Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://Pan-Binghong.github.io/daily-learning/ title="🏠 首页"><span>🏠 首页</span></a></li><li><a href=https://Pan-Binghong.github.io/daily-learning/ai/ title="🤖 AI"><span>🤖 AI</span></a></li><li><a href=https://Pan-Binghong.github.io/daily-learning/knowledge/ title="📚 知识库"><span>📚 知识库</span></a></li><li><a href=https://Pan-Binghong.github.io/daily-learning/backend/ title="⚙️ 后端"><span>⚙️ 后端</span></a></li><li><a href=https://Pan-Binghong.github.io/daily-learning/devops/ title="🔧 DevOps"><span>🔧 DevOps</span></a></li><li><a href=https://Pan-Binghong.github.io/daily-learning/other/ title="📝 其他"><span>📝 其他</span></a></li><li><a href=https://Pan-Binghong.github.io/daily-learning/search/ title="🔍 搜索"><span>🔍 搜索</span></a></li><li><a href=https://Pan-Binghong.github.io/daily-learning/tags/ title="🏷️ 标签"><span>🏷️ 标签</span></a></li><li><a href=https://Pan-Binghong.github.io/daily-learning/archives/ title="📂 归档"><span>📂 归档</span></a></li><li><a href=https://Pan-Binghong.github.io/daily-learning/about/ title="👤 关于"><span>👤 关于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://Pan-Binghong.github.io/daily-learning/>Home</a>&nbsp;»&nbsp;<a href=https://Pan-Binghong.github.io/daily-learning/ai/>Ais</a></div><h1 class="post-title entry-hint-parent">基于Pytorch架构手撕Transformer</h1><div class=post-meta><span title='2024-10-29 01:52:00 +0000 UTC'>October 29, 2024</span>&nbsp;·&nbsp;<span>19 min</span>&nbsp;·&nbsp;<span>4018 words</span>&nbsp;·&nbsp;<span>Pan Binghong</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><ul><li><a href=#transformer-%e7%9a%84%e8%83%8c%e6%99%af aria-label="Transformer 的背景">Transformer 的背景</a></li><li><a href=#%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6 aria-label=注意力机制>注意力机制</a><ul><li><a href=#scaled-dot-product-attention aria-label="Scaled Dot-product Attention">Scaled Dot-product Attention</a></li><li><a href=#%e4%bb%a3%e7%a0%81%e6%95%b4%e5%90%88scaled-dot-product-attention aria-label="代码整合(Scaled Dot-product Attention)">代码整合(Scaled Dot-product Attention)</a></li><li><a href=#multi-head-attention aria-label="Multi-head Attention">Multi-head Attention</a></li><li><a href=#%e4%bb%a3%e7%a0%81%e6%95%b4%e5%90%88multi-head-attention aria-label="代码整合(Multi-head Attention)">代码整合(Multi-head Attention)</a></li></ul></li><li><a href=#%e5%89%8d%e9%a6%88%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c aria-label=前馈神经网络>前馈神经网络</a><ul><li><a href=#the-feed-forward-layer aria-label="The Feed-Forward Layer">The Feed-Forward Layer</a></li></ul></li><li><a href=#%e5%b1%82%e5%bd%92%e4%b8%80%e5%8c%96--%e6%ae%8b%e5%b7%ae%e8%bf%9e%e6%8e%a5 aria-label="层归一化 & 残差连接">层归一化 & 残差连接</a><ul><li><a href=#transformer-encoder-layer aria-label="Transformer Encoder Layer">Transformer Encoder Layer</a></li></ul></li><li><a href=#%e7%bb%9d%e5%af%b9%e4%bd%8d%e7%bd%ae%e7%bc%96%e7%a0%81 aria-label=绝对位置编码>绝对位置编码</a><ul><li><a href=#transformer-encoder aria-label="Transformer Encoder">Transformer Encoder</a></li></ul></li><li><a href=#%e5%ae%8c%e6%95%b4%e4%bb%a3%e7%a0%81 aria-label=完整代码>完整代码</a></li><li><a href=#%e7%bb%9d%e5%af%b9%e4%bd%8d%e7%bd%ae%e7%bc%96%e7%a0%81-1 aria-label=绝对位置编码>绝对位置编码</a></li><li><a href=#%e5%a4%9a%e5%a4%b4%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6 aria-label=多头注意力机制>多头注意力机制</a></li><li><a href=#%e5%89%8d%e9%a6%88%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c-1 aria-label=前馈神经网络>前馈神经网络</a></li></ul><li><a href=#%e5%b1%82%e5%bd%92%e4%b8%80%e5%8c%96--%e6%ae%8b%e5%b7%ae%e8%bf%9e%e6%8e%a5-1 aria-label="层归一化 & 残差连接">层归一化 & 残差连接</a><ul><ul><li><a href=#residual-connections aria-label="Residual Connections">Residual Connections</a></li></ul><li><a href=#%e7%bc%96%e7%a0%81%e5%99%a8 aria-label=编码器>编码器</a><ul><li><a href=#encoder-layer aria-label="Encoder Layer">Encoder Layer</a></li><li><a href=#encoder aria-label=Encoder>Encoder</a></li></ul></li><li><a href=#%e8%a7%a3%e7%a0%81%e5%99%a8 aria-label=解码器>解码器</a><ul><li><a href=#decoder_layer aria-label=Decoder_Layer>Decoder_Layer</a></li><li><a href=#decoder aria-label=Decoder>Decoder</a></li></ul></li></ul></li></ul></div></details></div><div class=post-content><p>深入学习Tranformer模型结构, 本来依赖GPT4o, 自己快复现完了, 结果在封装Decoder层的时候, 发生了mask维度匹配不对的问题, 查了很多资料最终还是没解决, 后期再搞吧.</p><p><img loading=lazy src="https://prod-files-secure.s3.us-west-2.amazonaws.com/fc187c04-cf34-444f-b5f2-bdcdfad76660/5976f44e-a761-4511-a4fc-db008f73023d/image.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466V6O3YB3J%2F20251204%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20251204T024937Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEHIaCXVzLXdlc3QtMiJHMEUCIQCC5AphwKV1HQJLtDr7F8hluRKyjvHYMG%2BkwXTt%2BYH0VAIgBbM%2F0PxyE%2FKy7rSHX5jyT7b8lazGrMHrXJMUQA4bN0kq%2FwMIOxAAGgw2Mzc0MjMxODM4MDUiDMym73mXcm2RNETDtircA1dFFxrThyCSAcWXAZnYT6pR9puHfc2ArjOVukDxoGINEHP3TnAUVyUz5oOpHRnT6o4BOD5dUHWIYbeanJZ%2BlIqt8Cmx%2BLGZNHyENSerO7HSI%2FBtV%2BFr0rrbJ0x5EN%2B%2Byr694VwfDIZDhtrsFs3%2BfX95Rk%2FEDAdlEqpZJhoE9S4rWS0FuZRDQs9%2Fd4FAUFhouSjwAawanxVNnM58i6om%2BGsI0kcgaxAfUaXp%2B4mSGEnHwcK285%2FZhETLjqrDgGCzWKnZzQpiqpouXRRYuoGx9W2F%2FvfBbIq9u%2BP9cyb6bgFJ7%2F8S%2FxMmWRhu5Ag6yUaiY76PfW6xgeIF0HEESuBdip3FQZn8g%2FO4FP3WG%2FEFx80NQvt782WFDchpBKIjFzEwpGthaufFcu%2BHHgDIuqzqcP%2Ff0WkXCY3Wp4UICI5O%2F9moq6g4EPqKwDIUrXH2JlSZ0ky7%2BkJEUVz45j9l55Asi8DbhTtmojXXArvV%2F%2B79AVMvxWfoQIc6SH1TyXTYW%2BBx0HeMGBjJROxmy9PAl4jUpfa%2FHaJceYRqFJ4yYFNE60LoEzoRHkPGkCgdKKvv1FYU8pT1dFYexl6P7Dqypmrcyq0EfCwaq55Q1zh%2BsCFAss3LO15brxClWSdP18zoMPnUw8kGOqUBOtbYKbK7Kku%2BYH4BRwQw0IRKcVA%2FE%2FOuSunP5Nh9P9C9G0RevLBOMmU%2FThR0HcYknP9PbZal%2Fz%2FL5UXACkNNlryKbdiiRY2AlvgcnUPS3nFs%2BbeP21Tf6rfBeSxXWtX1aBwh84visckIQC%2FBeNZ6DY24yEoRKNegz98zSsUzpDpXlJY%2FSZXUlE3q%2BNIJZDdpSXY1Agez6iJPV5wpCV1%2FYs7OKe5J&X-Amz-Signature=296e9f41e2bee80cdb0484f05920e5c0b0ef9b2329d852e4eb83d7a9bab36395&X-Amz-SignedHeaders=host&x-amz-checksum-mode=ENABLED&x-id=GetObject"></p><h2 id=transformer-的背景>Transformer 的背景<a hidden class=anchor aria-hidden=true href=#transformer-的背景>#</a></h2><ul><li>什么是Transformer ?</li><li>Transformer是谁提出的 ?</li><li>Transformer解决了什么问题 ?</li><li>Transformer核心组件有哪些 ?</li></ul><hr><h2 id=注意力机制>注意力机制<a hidden class=anchor aria-hidden=true href=#注意力机制>#</a></h2><ul><li>核心公式 :
<img loading=lazy src="https://prod-files-secure.s3.us-west-2.amazonaws.com/fc187c04-cf34-444f-b5f2-bdcdfad76660/7ccda5e2-2427-4a14-a91e-cd3a1ad7fc58/image.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466V6O3YB3J%2F20251204%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20251204T024937Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEHIaCXVzLXdlc3QtMiJHMEUCIQCC5AphwKV1HQJLtDr7F8hluRKyjvHYMG%2BkwXTt%2BYH0VAIgBbM%2F0PxyE%2FKy7rSHX5jyT7b8lazGrMHrXJMUQA4bN0kq%2FwMIOxAAGgw2Mzc0MjMxODM4MDUiDMym73mXcm2RNETDtircA1dFFxrThyCSAcWXAZnYT6pR9puHfc2ArjOVukDxoGINEHP3TnAUVyUz5oOpHRnT6o4BOD5dUHWIYbeanJZ%2BlIqt8Cmx%2BLGZNHyENSerO7HSI%2FBtV%2BFr0rrbJ0x5EN%2B%2Byr694VwfDIZDhtrsFs3%2BfX95Rk%2FEDAdlEqpZJhoE9S4rWS0FuZRDQs9%2Fd4FAUFhouSjwAawanxVNnM58i6om%2BGsI0kcgaxAfUaXp%2B4mSGEnHwcK285%2FZhETLjqrDgGCzWKnZzQpiqpouXRRYuoGx9W2F%2FvfBbIq9u%2BP9cyb6bgFJ7%2F8S%2FxMmWRhu5Ag6yUaiY76PfW6xgeIF0HEESuBdip3FQZn8g%2FO4FP3WG%2FEFx80NQvt782WFDchpBKIjFzEwpGthaufFcu%2BHHgDIuqzqcP%2Ff0WkXCY3Wp4UICI5O%2F9moq6g4EPqKwDIUrXH2JlSZ0ky7%2BkJEUVz45j9l55Asi8DbhTtmojXXArvV%2F%2B79AVMvxWfoQIc6SH1TyXTYW%2BBx0HeMGBjJROxmy9PAl4jUpfa%2FHaJceYRqFJ4yYFNE60LoEzoRHkPGkCgdKKvv1FYU8pT1dFYexl6P7Dqypmrcyq0EfCwaq55Q1zh%2BsCFAss3LO15brxClWSdP18zoMPnUw8kGOqUBOtbYKbK7Kku%2BYH4BRwQw0IRKcVA%2FE%2FOuSunP5Nh9P9C9G0RevLBOMmU%2FThR0HcYknP9PbZal%2Fz%2FL5UXACkNNlryKbdiiRY2AlvgcnUPS3nFs%2BbeP21Tf6rfBeSxXWtX1aBwh84visckIQC%2FBeNZ6DY24yEoRKNegz98zSsUzpDpXlJY%2FSZXUlE3q%2BNIJZDdpSXY1Agez6iJPV5wpCV1%2FYs7OKe5J&X-Amz-Signature=e2b9d6ab07c88f622fe6ec7ddeff2b4900d4e56d436786be8ef0c23b39b1da6c&X-Amz-SignedHeaders=host&x-amz-checksum-mode=ENABLED&x-id=GetObject"></li></ul><p><img loading=lazy src="https://prod-files-secure.s3.us-west-2.amazonaws.com/fc187c04-cf34-444f-b5f2-bdcdfad76660/cb861907-c901-48ad-8055-a0b41d47106a/image.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466V6O3YB3J%2F20251204%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20251204T024937Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEHIaCXVzLXdlc3QtMiJHMEUCIQCC5AphwKV1HQJLtDr7F8hluRKyjvHYMG%2BkwXTt%2BYH0VAIgBbM%2F0PxyE%2FKy7rSHX5jyT7b8lazGrMHrXJMUQA4bN0kq%2FwMIOxAAGgw2Mzc0MjMxODM4MDUiDMym73mXcm2RNETDtircA1dFFxrThyCSAcWXAZnYT6pR9puHfc2ArjOVukDxoGINEHP3TnAUVyUz5oOpHRnT6o4BOD5dUHWIYbeanJZ%2BlIqt8Cmx%2BLGZNHyENSerO7HSI%2FBtV%2BFr0rrbJ0x5EN%2B%2Byr694VwfDIZDhtrsFs3%2BfX95Rk%2FEDAdlEqpZJhoE9S4rWS0FuZRDQs9%2Fd4FAUFhouSjwAawanxVNnM58i6om%2BGsI0kcgaxAfUaXp%2B4mSGEnHwcK285%2FZhETLjqrDgGCzWKnZzQpiqpouXRRYuoGx9W2F%2FvfBbIq9u%2BP9cyb6bgFJ7%2F8S%2FxMmWRhu5Ag6yUaiY76PfW6xgeIF0HEESuBdip3FQZn8g%2FO4FP3WG%2FEFx80NQvt782WFDchpBKIjFzEwpGthaufFcu%2BHHgDIuqzqcP%2Ff0WkXCY3Wp4UICI5O%2F9moq6g4EPqKwDIUrXH2JlSZ0ky7%2BkJEUVz45j9l55Asi8DbhTtmojXXArvV%2F%2B79AVMvxWfoQIc6SH1TyXTYW%2BBx0HeMGBjJROxmy9PAl4jUpfa%2FHaJceYRqFJ4yYFNE60LoEzoRHkPGkCgdKKvv1FYU8pT1dFYexl6P7Dqypmrcyq0EfCwaq55Q1zh%2BsCFAss3LO15brxClWSdP18zoMPnUw8kGOqUBOtbYKbK7Kku%2BYH4BRwQw0IRKcVA%2FE%2FOuSunP5Nh9P9C9G0RevLBOMmU%2FThR0HcYknP9PbZal%2Fz%2FL5UXACkNNlryKbdiiRY2AlvgcnUPS3nFs%2BbeP21Tf6rfBeSxXWtX1aBwh84visckIQC%2FBeNZ6DY24yEoRKNegz98zSsUzpDpXlJY%2FSZXUlE3q%2BNIJZDdpSXY1Agez6iJPV5wpCV1%2FYs7OKe5J&X-Amz-Signature=ec0d09a694bf1768ce1f166f7c83c1f57b47c0ffafc8414d32fe5e28840227da&X-Amz-SignedHeaders=host&x-amz-checksum-mode=ENABLED&x-id=GetObject"></p><p><img loading=lazy src="https://prod-files-secure.s3.us-west-2.amazonaws.com/fc187c04-cf34-444f-b5f2-bdcdfad76660/2c6924da-ad53-4f28-b2c4-a2f769c588f2/image.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466V6O3YB3J%2F20251204%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20251204T024937Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEHIaCXVzLXdlc3QtMiJHMEUCIQCC5AphwKV1HQJLtDr7F8hluRKyjvHYMG%2BkwXTt%2BYH0VAIgBbM%2F0PxyE%2FKy7rSHX5jyT7b8lazGrMHrXJMUQA4bN0kq%2FwMIOxAAGgw2Mzc0MjMxODM4MDUiDMym73mXcm2RNETDtircA1dFFxrThyCSAcWXAZnYT6pR9puHfc2ArjOVukDxoGINEHP3TnAUVyUz5oOpHRnT6o4BOD5dUHWIYbeanJZ%2BlIqt8Cmx%2BLGZNHyENSerO7HSI%2FBtV%2BFr0rrbJ0x5EN%2B%2Byr694VwfDIZDhtrsFs3%2BfX95Rk%2FEDAdlEqpZJhoE9S4rWS0FuZRDQs9%2Fd4FAUFhouSjwAawanxVNnM58i6om%2BGsI0kcgaxAfUaXp%2B4mSGEnHwcK285%2FZhETLjqrDgGCzWKnZzQpiqpouXRRYuoGx9W2F%2FvfBbIq9u%2BP9cyb6bgFJ7%2F8S%2FxMmWRhu5Ag6yUaiY76PfW6xgeIF0HEESuBdip3FQZn8g%2FO4FP3WG%2FEFx80NQvt782WFDchpBKIjFzEwpGthaufFcu%2BHHgDIuqzqcP%2Ff0WkXCY3Wp4UICI5O%2F9moq6g4EPqKwDIUrXH2JlSZ0ky7%2BkJEUVz45j9l55Asi8DbhTtmojXXArvV%2F%2B79AVMvxWfoQIc6SH1TyXTYW%2BBx0HeMGBjJROxmy9PAl4jUpfa%2FHaJceYRqFJ4yYFNE60LoEzoRHkPGkCgdKKvv1FYU8pT1dFYexl6P7Dqypmrcyq0EfCwaq55Q1zh%2BsCFAss3LO15brxClWSdP18zoMPnUw8kGOqUBOtbYKbK7Kku%2BYH4BRwQw0IRKcVA%2FE%2FOuSunP5Nh9P9C9G0RevLBOMmU%2FThR0HcYknP9PbZal%2Fz%2FL5UXACkNNlryKbdiiRY2AlvgcnUPS3nFs%2BbeP21Tf6rfBeSxXWtX1aBwh84visckIQC%2FBeNZ6DY24yEoRKNegz98zSsUzpDpXlJY%2FSZXUlE3q%2BNIJZDdpSXY1Agez6iJPV5wpCV1%2FYs7OKe5J&X-Amz-Signature=4e35d8a81931f8bb4724ba274a77bb7198af34c644317407414ad79d73e37ea2&X-Amz-SignedHeaders=host&x-amz-checksum-mode=ENABLED&x-id=GetObject"></p><ul><li>论文原图 :
<img loading=lazy src="https://prod-files-secure.s3.us-west-2.amazonaws.com/fc187c04-cf34-444f-b5f2-bdcdfad76660/38c27c70-67d5-4420-bae3-bfb9ec5c7f30/image.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466V6O3YB3J%2F20251204%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20251204T024937Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEHIaCXVzLXdlc3QtMiJHMEUCIQCC5AphwKV1HQJLtDr7F8hluRKyjvHYMG%2BkwXTt%2BYH0VAIgBbM%2F0PxyE%2FKy7rSHX5jyT7b8lazGrMHrXJMUQA4bN0kq%2FwMIOxAAGgw2Mzc0MjMxODM4MDUiDMym73mXcm2RNETDtircA1dFFxrThyCSAcWXAZnYT6pR9puHfc2ArjOVukDxoGINEHP3TnAUVyUz5oOpHRnT6o4BOD5dUHWIYbeanJZ%2BlIqt8Cmx%2BLGZNHyENSerO7HSI%2FBtV%2BFr0rrbJ0x5EN%2B%2Byr694VwfDIZDhtrsFs3%2BfX95Rk%2FEDAdlEqpZJhoE9S4rWS0FuZRDQs9%2Fd4FAUFhouSjwAawanxVNnM58i6om%2BGsI0kcgaxAfUaXp%2B4mSGEnHwcK285%2FZhETLjqrDgGCzWKnZzQpiqpouXRRYuoGx9W2F%2FvfBbIq9u%2BP9cyb6bgFJ7%2F8S%2FxMmWRhu5Ag6yUaiY76PfW6xgeIF0HEESuBdip3FQZn8g%2FO4FP3WG%2FEFx80NQvt782WFDchpBKIjFzEwpGthaufFcu%2BHHgDIuqzqcP%2Ff0WkXCY3Wp4UICI5O%2F9moq6g4EPqKwDIUrXH2JlSZ0ky7%2BkJEUVz45j9l55Asi8DbhTtmojXXArvV%2F%2B79AVMvxWfoQIc6SH1TyXTYW%2BBx0HeMGBjJROxmy9PAl4jUpfa%2FHaJceYRqFJ4yYFNE60LoEzoRHkPGkCgdKKvv1FYU8pT1dFYexl6P7Dqypmrcyq0EfCwaq55Q1zh%2BsCFAss3LO15brxClWSdP18zoMPnUw8kGOqUBOtbYKbK7Kku%2BYH4BRwQw0IRKcVA%2FE%2FOuSunP5Nh9P9C9G0RevLBOMmU%2FThR0HcYknP9PbZal%2Fz%2FL5UXACkNNlryKbdiiRY2AlvgcnUPS3nFs%2BbeP21Tf6rfBeSxXWtX1aBwh84visckIQC%2FBeNZ6DY24yEoRKNegz98zSsUzpDpXlJY%2FSZXUlE3q%2BNIJZDdpSXY1Agez6iJPV5wpCV1%2FYs7OKe5J&X-Amz-Signature=7d0b425a2a630acae1d95dfb195706b864b47484a4f7b869c9a68f5b33373fd4&X-Amz-SignedHeaders=host&x-amz-checksum-mode=ENABLED&x-id=GetObject"></li></ul><h3 id=scaled-dot-product-attention>Scaled Dot-product Attention<a hidden class=anchor aria-hidden=true href=#scaled-dot-product-attention>#</a></h3><p><img loading=lazy src="https://prod-files-secure.s3.us-west-2.amazonaws.com/fc187c04-cf34-444f-b5f2-bdcdfad76660/90a969cd-6a3b-4923-bfd0-b511f94fd56d/image.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466V6O3YB3J%2F20251204%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20251204T024937Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEHIaCXVzLXdlc3QtMiJHMEUCIQCC5AphwKV1HQJLtDr7F8hluRKyjvHYMG%2BkwXTt%2BYH0VAIgBbM%2F0PxyE%2FKy7rSHX5jyT7b8lazGrMHrXJMUQA4bN0kq%2FwMIOxAAGgw2Mzc0MjMxODM4MDUiDMym73mXcm2RNETDtircA1dFFxrThyCSAcWXAZnYT6pR9puHfc2ArjOVukDxoGINEHP3TnAUVyUz5oOpHRnT6o4BOD5dUHWIYbeanJZ%2BlIqt8Cmx%2BLGZNHyENSerO7HSI%2FBtV%2BFr0rrbJ0x5EN%2B%2Byr694VwfDIZDhtrsFs3%2BfX95Rk%2FEDAdlEqpZJhoE9S4rWS0FuZRDQs9%2Fd4FAUFhouSjwAawanxVNnM58i6om%2BGsI0kcgaxAfUaXp%2B4mSGEnHwcK285%2FZhETLjqrDgGCzWKnZzQpiqpouXRRYuoGx9W2F%2FvfBbIq9u%2BP9cyb6bgFJ7%2F8S%2FxMmWRhu5Ag6yUaiY76PfW6xgeIF0HEESuBdip3FQZn8g%2FO4FP3WG%2FEFx80NQvt782WFDchpBKIjFzEwpGthaufFcu%2BHHgDIuqzqcP%2Ff0WkXCY3Wp4UICI5O%2F9moq6g4EPqKwDIUrXH2JlSZ0ky7%2BkJEUVz45j9l55Asi8DbhTtmojXXArvV%2F%2B79AVMvxWfoQIc6SH1TyXTYW%2BBx0HeMGBjJROxmy9PAl4jUpfa%2FHaJceYRqFJ4yYFNE60LoEzoRHkPGkCgdKKvv1FYU8pT1dFYexl6P7Dqypmrcyq0EfCwaq55Q1zh%2BsCFAss3LO15brxClWSdP18zoMPnUw8kGOqUBOtbYKbK7Kku%2BYH4BRwQw0IRKcVA%2FE%2FOuSunP5Nh9P9C9G0RevLBOMmU%2FThR0HcYknP9PbZal%2Fz%2FL5UXACkNNlryKbdiiRY2AlvgcnUPS3nFs%2BbeP21Tf6rfBeSxXWtX1aBwh84visckIQC%2FBeNZ6DY24yEoRKNegz98zSsUzpDpXlJY%2FSZXUlE3q%2BNIJZDdpSXY1Agez6iJPV5wpCV1%2FYs7OKe5J&X-Amz-Signature=148460ec426ca6a5ab81363815b932701c6f402d78c45bf858eba47499da11d5&X-Amz-SignedHeaders=host&x-amz-checksum-mode=ENABLED&x-id=GetObject"></p><ul><li>代码实现 :</li></ul><h3 id=代码整合scaled-dot-product-attention>代码整合(Scaled Dot-product Attention)<a hidden class=anchor aria-hidden=true href=#代码整合scaled-dot-product-attention>#</a></h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>  <span class=c1># 导入PyTorch库</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn.functional</span> <span class=k>as</span> <span class=nn>F</span>  <span class=c1># 导入PyTorch的nn.functional模块，包含了许多神经网络操作的函数</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>math</span> <span class=kn>import</span> <span class=n>sqrt</span>  <span class=c1># 导入math库中的sqrt函数，用于计算平方根</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 定义Scaled Dot-product Attention函数</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>scaled_dot_product_attention</span><span class=p>(</span><span class=n>query</span><span class=p>,</span> <span class=n>key</span><span class=p>,</span> <span class=n>value</span><span class=p>,</span> <span class=n>query_mask</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>key_mask</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># 获取查询（query）的最后一个维度大小，即键（key）的维度</span>
</span></span><span class=line><span class=cl>    <span class=n>dim_k</span> <span class=o>=</span> <span class=n>query</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># 计算查询和键的点积，并缩放，得到未归一化的注意力分数</span>
</span></span><span class=line><span class=cl>    <span class=n>scores</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>bmm</span><span class=p>(</span><span class=n>query</span><span class=p>,</span> <span class=n>key</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span> <span class=o>/</span> <span class=n>sqrt</span><span class=p>(</span><span class=n>dim_k</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># 如果提供了查询掩码（query_mask）和键掩码（key_mask），则计算掩码矩阵</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>query_mask</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=ow>and</span> <span class=n>key_mask</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>mask</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>bmm</span><span class=p>(</span><span class=n>query_mask</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>),</span> <span class=n>key_mask</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># 如果没有提供掩码，则使用之前传入的掩码（如果有的话）</span>
</span></span><span class=line><span class=cl>        <span class=n>mask</span> <span class=o>=</span> <span class=n>mask</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># 如果存在掩码，则将分数矩阵中与掩码对应位置为0的分数替换为负无穷</span>
</span></span><span class=line><span class=cl>    <span class=c1># 这样在应用softmax时，这些位置的权重会接近于0</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>mask</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>scores</span> <span class=o>=</span> <span class=n>scores</span><span class=o>.</span><span class=n>masked_fill</span><span class=p>(</span><span class=n>mask</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=o>-</span><span class=nb>float</span><span class=p>(</span><span class=s2>&#34;inf&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># 使用softmax函数对分数进行归一化，得到注意力权重</span>
</span></span><span class=line><span class=cl>    <span class=n>weights</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>scores</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># 计算加权后的输出，即将注意力权重与值（value）相乘</span>
</span></span><span class=line><span class=cl>    <span class=c1># 这里的输出是经过注意力加权后的值向量，用于下游任务</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>torch</span><span class=o>.</span><span class=n>bmm</span><span class=p>(</span><span class=n>weights</span><span class=p>,</span> <span class=n>value</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=multi-head-attention>Multi-head Attention<a hidden class=anchor aria-hidden=true href=#multi-head-attention>#</a></h3><ul><li>Multi-head Attention作用 :
首先通过线性映射将 Q,K,V 序列映射到特征空间，每一组线性投影后的向量表示称为一个头 (head)，然后在每组映射后的序列上再应用 Scaled Dot-product Attention, 最后将每个头的向量拼接.
<img loading=lazy src="https://prod-files-secure.s3.us-west-2.amazonaws.com/fc187c04-cf34-444f-b5f2-bdcdfad76660/ab37c920-8ea8-4dd4-b3a6-655932f8727c/image.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466V6O3YB3J%2F20251204%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20251204T024937Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEHIaCXVzLXdlc3QtMiJHMEUCIQCC5AphwKV1HQJLtDr7F8hluRKyjvHYMG%2BkwXTt%2BYH0VAIgBbM%2F0PxyE%2FKy7rSHX5jyT7b8lazGrMHrXJMUQA4bN0kq%2FwMIOxAAGgw2Mzc0MjMxODM4MDUiDMym73mXcm2RNETDtircA1dFFxrThyCSAcWXAZnYT6pR9puHfc2ArjOVukDxoGINEHP3TnAUVyUz5oOpHRnT6o4BOD5dUHWIYbeanJZ%2BlIqt8Cmx%2BLGZNHyENSerO7HSI%2FBtV%2BFr0rrbJ0x5EN%2B%2Byr694VwfDIZDhtrsFs3%2BfX95Rk%2FEDAdlEqpZJhoE9S4rWS0FuZRDQs9%2Fd4FAUFhouSjwAawanxVNnM58i6om%2BGsI0kcgaxAfUaXp%2B4mSGEnHwcK285%2FZhETLjqrDgGCzWKnZzQpiqpouXRRYuoGx9W2F%2FvfBbIq9u%2BP9cyb6bgFJ7%2F8S%2FxMmWRhu5Ag6yUaiY76PfW6xgeIF0HEESuBdip3FQZn8g%2FO4FP3WG%2FEFx80NQvt782WFDchpBKIjFzEwpGthaufFcu%2BHHgDIuqzqcP%2Ff0WkXCY3Wp4UICI5O%2F9moq6g4EPqKwDIUrXH2JlSZ0ky7%2BkJEUVz45j9l55Asi8DbhTtmojXXArvV%2F%2B79AVMvxWfoQIc6SH1TyXTYW%2BBx0HeMGBjJROxmy9PAl4jUpfa%2FHaJceYRqFJ4yYFNE60LoEzoRHkPGkCgdKKvv1FYU8pT1dFYexl6P7Dqypmrcyq0EfCwaq55Q1zh%2BsCFAss3LO15brxClWSdP18zoMPnUw8kGOqUBOtbYKbK7Kku%2BYH4BRwQw0IRKcVA%2FE%2FOuSunP5Nh9P9C9G0RevLBOMmU%2FThR0HcYknP9PbZal%2Fz%2FL5UXACkNNlryKbdiiRY2AlvgcnUPS3nFs%2BbeP21Tf6rfBeSxXWtX1aBwh84visckIQC%2FBeNZ6DY24yEoRKNegz98zSsUzpDpXlJY%2FSZXUlE3q%2BNIJZDdpSXY1Agez6iJPV5wpCV1%2FYs7OKe5J&X-Amz-Signature=c6085e30f303a14644798410ea836e286760d809466fca207f1c0bb75cf50c31&X-Amz-SignedHeaders=host&x-amz-checksum-mode=ENABLED&x-id=GetObject"></li></ul><h3 id=代码整合multi-head-attention>代码整合(Multi-head Attention)<a hidden class=anchor aria-hidden=true href=#代码整合multi-head-attention>#</a></h3><ul><li>代码实现 :</li><li>验证代码</li></ul><hr><h2 id=前馈神经网络>前馈神经网络<a hidden class=anchor aria-hidden=true href=#前馈神经网络>#</a></h2><h3 id=the-feed-forward-layer>The Feed-Forward Layer<a hidden class=anchor aria-hidden=true href=#the-feed-forward-layer>#</a></h3><p>没啥好写的, 就是普通的全连接 + 激活函数</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch</span> <span class=kn>import</span> <span class=n>nn</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 定义FeedForward类，继承自nn.Module</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>FeedForward</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># 初始化函数</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>  <span class=c1># 调用基类的初始化方法</span>
</span></span><span class=line><span class=cl>        <span class=c1># 定义第一个线性层，将输入的隐藏状态映射到中间维度</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>linear_1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>intermediate_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 定义第二个线性层，将中间维度的表示映射回原始的隐藏状态维度</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>linear_2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>intermediate_size</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 定义GELU激活函数</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>gelu</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>GELU</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=c1># 定义Dropout层，用于防止过拟合</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>hidden_dropout_prob</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 前向传播函数</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># 应用第一个线性层</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>linear_1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 应用GELU激活函数</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>gelu</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 应用第二个线性层</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>linear_2</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 应用Dropout</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 返回最终的输出</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span></code></pre></td></tr></table></div></div><p>与上面构建的注意力机制串联测试 :</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>feed_forward</span> <span class=o>=</span> <span class=n>FeedForward</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ff_outputs</span> <span class=o>=</span> <span class=n>feed_forward</span><span class=p>(</span><span class=n>attn_output</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>ff_outputs</span><span class=o>.</span><span class=n>size</span><span class=p>())</span> <span class=c1>#torch.Size([1, 2, 768])</span>
</span></span></code></pre></td></tr></table></div></div><hr><h2 id=层归一化--残差连接>层归一化 & 残差连接<a hidden class=anchor aria-hidden=true href=#层归一化--残差连接>#</a></h2><ul><li>层归一化模块需要包含在残差模块内, 主要作用为 : 将输入的一批向量, 每一个都做标准化处理, 处理为 : 均值为零, 且有单位方差</li><li>残差连接主要作用为 : 是通过直接将输入绕过中间层的计算，帮助模型更容易训练深层网络，避免梯度消失问题并促进信息流动</li></ul><h3 id=transformer-encoder-layer>Transformer Encoder Layer<a hidden class=anchor aria-hidden=true href=#transformer-encoder-layer>#</a></h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch</span> <span class=kn>import</span> <span class=n>nn</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 定义TransformerEncoderLayer类，继承自nn.Module</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>TransformerEncoderLayer</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># 初始化函数</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>  <span class=c1># 调用基类的初始化方法</span>
</span></span><span class=line><span class=cl>        <span class=c1># 定义第一个层归一化，用于注意力机制之前</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>layer_norm_1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 定义第二个层归一化，用于前馈网络之前</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>layer_norm_2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 定义多头注意力机制</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>attention</span> <span class=o>=</span> <span class=n>MultiHeadAttention</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 定义前馈神经网络</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>feed_forward</span> <span class=o>=</span> <span class=n>FeedForward</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 前向传播函数</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># 应用第一个层归一化</span>
</span></span><span class=line><span class=cl>        <span class=n>hidden_state</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>layer_norm_1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 应用注意力机制，并将结果与输入进行残差连接</span>
</span></span><span class=line><span class=cl>        <span class=c1># 注意力机制的输出将与输入x相加，得到更新后的x</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>attention</span><span class=p>(</span><span class=n>hidden_state</span><span class=p>,</span> <span class=n>hidden_state</span><span class=p>,</span> <span class=n>hidden_state</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=n>mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 应用第二个层归一化</span>
</span></span><span class=line><span class=cl>        <span class=c1># 注意这里的self.layer_norm_2(x)实际上是对更新后的x进行归一化</span>
</span></span><span class=line><span class=cl>        <span class=n>hidden_state</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>layer_norm_2</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 应用前馈网络，并将结果与更新后的x进行残差连接</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>feed_forward</span><span class=p>(</span><span class=n>hidden_state</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 返回最终的输出x</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span></code></pre></td></tr></table></div></div><p>代码验证 :</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>encoder_layer</span> <span class=o>=</span> <span class=n>TransformerEncoderLayer</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>inputs_embeds</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>encoder_layer</span><span class=p>(</span><span class=n>inputs_embeds</span><span class=p>)</span><span class=o>.</span><span class=n>size</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=c1>#torch.Size([1, 5, 768])</span>
</span></span><span class=line><span class=cl><span class=c1>#torch.Size([1, 5, 768])</span>
</span></span></code></pre></td></tr></table></div></div><hr><h2 id=绝对位置编码>绝对位置编码<a hidden class=anchor aria-hidden=true href=#绝对位置编码>#</a></h2><p>注意力机制无法捕获词语之间的位置信息，因此 Transformer 模型还使用 Positional Embeddings 添加了词语的位置信息。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch</span> <span class=kn>import</span> <span class=n>nn</span><span class=p>,</span> <span class=n>LongTensor</span><span class=p>,</span> <span class=n>arange</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 定义Embeddings类，继承自nn.Module</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>Embeddings</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># 初始化函数</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>  <span class=c1># 调用基类的初始化方法</span>
</span></span><span class=line><span class=cl>        <span class=c1># 定义词嵌入层，将词ID映射到词向量</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>token_embeddings</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 定义位置嵌入层，为序列中的每个位置生成一个唯一的位置向量</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>position_embeddings</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>max_position_embeddings</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 定义层归一化，用于稳定训练过程</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>layer_norm</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>,</span> <span class=n>eps</span><span class=o>=</span><span class=mf>1e-12</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 定义Dropout层，用于防止过拟合</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>hidden_dropout_prob</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 前向传播函数</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_ids</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># 根据输入序列的长度创建位置ID</span>
</span></span><span class=line><span class=cl>        <span class=n>seq_length</span> <span class=o>=</span> <span class=n>input_ids</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>  <span class=c1># 获取序列长度</span>
</span></span><span class=line><span class=cl>        <span class=n>position_ids</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>seq_length</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>long</span><span class=p>)</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>  <span class=c1># 创建位置ID序列</span>
</span></span><span class=line><span class=cl>        <span class=c1># 创建词嵌入和位置嵌入</span>
</span></span><span class=line><span class=cl>        <span class=n>token_embeddings</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>token_embeddings</span><span class=p>(</span><span class=n>input_ids</span><span class=p>)</span>  <span class=c1># 通过词嵌入层获取词嵌入</span>
</span></span><span class=line><span class=cl>        <span class=n>position_embeddings</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>position_embeddings</span><span class=p>(</span><span class=n>position_ids</span><span class=p>)</span>  <span class=c1># 通过位置嵌入层获取位置嵌入</span>
</span></span><span class=line><span class=cl>        <span class=c1># 将词嵌入和位置嵌入相加，得到最终的嵌入表示</span>
</span></span><span class=line><span class=cl>        <span class=n>embeddings</span> <span class=o>=</span> <span class=n>token_embeddings</span> <span class=o>+</span> <span class=n>position_embeddings</span>
</span></span><span class=line><span class=cl>        <span class=c1># 应用层归一化</span>
</span></span><span class=line><span class=cl>        <span class=n>embeddings</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>layer_norm</span><span class=p>(</span><span class=n>embeddings</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 应用Dropout</span>
</span></span><span class=line><span class=cl>        <span class=n>embeddings</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>embeddings</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 返回最终的嵌入表示</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>embeddings</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 创建Embeddings层的实例，并使用config配置</span>
</span></span><span class=line><span class=cl><span class=n>embedding_layer</span> <span class=o>=</span> <span class=n>Embeddings</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 使用embedding_layer处理输入的词ID，并打印输出的大小</span>
</span></span><span class=line><span class=cl><span class=c1># 这里假设inputs.input_ids是之前通过tokenizer得到的词ID序列</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>embedding_layer</span><span class=p>(</span><span class=n>inputs</span><span class=o>.</span><span class=n>input_ids</span><span class=p>)</span><span class=o>.</span><span class=n>size</span><span class=p>())</span> <span class=c1>#torch.Size([1, 5, 768])</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=transformer-encoder>Transformer Encoder<a hidden class=anchor aria-hidden=true href=#transformer-encoder>#</a></h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch</span> <span class=kn>import</span> <span class=n>nn</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 定义TransformerEncoder类，继承自nn.Module</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>TransformerEncoder</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># 初始化函数</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>  <span class=c1># 调用基类的初始化方法</span>
</span></span><span class=line><span class=cl>        <span class=c1># 创建嵌入层实例，用于将输入的词ID转换为嵌入向量</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>embeddings</span> <span class=o>=</span> <span class=n>Embeddings</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 创建一个包含多个Transformer编码器层的列表</span>
</span></span><span class=line><span class=cl>        <span class=c1># num_hidden_layers表示编码器中隐藏层的数量</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>layers</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>([</span><span class=n>TransformerEncoderLayer</span><span class=p>(</span><span class=n>config</span><span class=p>)</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>num_hidden_layers</span><span class=p>)])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 前向传播函数</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># 首先通过嵌入层处理输入x</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>embeddings</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 然后依次通过每个编码器层</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>layer</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>layers</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># 将当前层的输出作为下一层的输入，并传递掩码（如果有的话）</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=n>layer</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=n>mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 返回最终的编码器输出</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span></code></pre></td></tr></table></div></div><p>测试代码 :</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>encoder</span> <span class=o>=</span> <span class=n>TransformerEncoder</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>encoder</span><span class=p>(</span><span class=n>inputs</span><span class=o>.</span><span class=n>input_ids</span><span class=p>)</span><span class=o>.</span><span class=n>size</span><span class=p>())</span>  <span class=c1>#torch.Size([1, 5, 768])</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=完整代码>完整代码<a hidden class=anchor aria-hidden=true href=#完整代码>#</a></h2><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>  1
</span><span class=lnt>  2
</span><span class=lnt>  3
</span><span class=lnt>  4
</span><span class=lnt>  5
</span><span class=lnt>  6
</span><span class=lnt>  7
</span><span class=lnt>  8
</span><span class=lnt>  9
</span><span class=lnt> 10
</span><span class=lnt> 11
</span><span class=lnt> 12
</span><span class=lnt> 13
</span><span class=lnt> 14
</span><span class=lnt> 15
</span><span class=lnt> 16
</span><span class=lnt> 17
</span><span class=lnt> 18
</span><span class=lnt> 19
</span><span class=lnt> 20
</span><span class=lnt> 21
</span><span class=lnt> 22
</span><span class=lnt> 23
</span><span class=lnt> 24
</span><span class=lnt> 25
</span><span class=lnt> 26
</span><span class=lnt> 27
</span><span class=lnt> 28
</span><span class=lnt> 29
</span><span class=lnt> 30
</span><span class=lnt> 31
</span><span class=lnt> 32
</span><span class=lnt> 33
</span><span class=lnt> 34
</span><span class=lnt> 35
</span><span class=lnt> 36
</span><span class=lnt> 37
</span><span class=lnt> 38
</span><span class=lnt> 39
</span><span class=lnt> 40
</span><span class=lnt> 41
</span><span class=lnt> 42
</span><span class=lnt> 43
</span><span class=lnt> 44
</span><span class=lnt> 45
</span><span class=lnt> 46
</span><span class=lnt> 47
</span><span class=lnt> 48
</span><span class=lnt> 49
</span><span class=lnt> 50
</span><span class=lnt> 51
</span><span class=lnt> 52
</span><span class=lnt> 53
</span><span class=lnt> 54
</span><span class=lnt> 55
</span><span class=lnt> 56
</span><span class=lnt> 57
</span><span class=lnt> 58
</span><span class=lnt> 59
</span><span class=lnt> 60
</span><span class=lnt> 61
</span><span class=lnt> 62
</span><span class=lnt> 63
</span><span class=lnt> 64
</span><span class=lnt> 65
</span><span class=lnt> 66
</span><span class=lnt> 67
</span><span class=lnt> 68
</span><span class=lnt> 69
</span><span class=lnt> 70
</span><span class=lnt> 71
</span><span class=lnt> 72
</span><span class=lnt> 73
</span><span class=lnt> 74
</span><span class=lnt> 75
</span><span class=lnt> 76
</span><span class=lnt> 77
</span><span class=lnt> 78
</span><span class=lnt> 79
</span><span class=lnt> 80
</span><span class=lnt> 81
</span><span class=lnt> 82
</span><span class=lnt> 83
</span><span class=lnt> 84
</span><span class=lnt> 85
</span><span class=lnt> 86
</span><span class=lnt> 87
</span><span class=lnt> 88
</span><span class=lnt> 89
</span><span class=lnt> 90
</span><span class=lnt> 91
</span><span class=lnt> 92
</span><span class=lnt> 93
</span><span class=lnt> 94
</span><span class=lnt> 95
</span><span class=lnt> 96
</span><span class=lnt> 97
</span><span class=lnt> 98
</span><span class=lnt> 99
</span><span class=lnt>100
</span><span class=lnt>101
</span><span class=lnt>102
</span><span class=lnt>103
</span><span class=lnt>104
</span><span class=lnt>105
</span><span class=lnt>106
</span><span class=lnt>107
</span><span class=lnt>108
</span><span class=lnt>109
</span><span class=lnt>110
</span><span class=lnt>111
</span><span class=lnt>112
</span><span class=lnt>113
</span><span class=lnt>114
</span><span class=lnt>115
</span><span class=lnt>116
</span><span class=lnt>117
</span><span class=lnt>118
</span><span class=lnt>119
</span><span class=lnt>120
</span><span class=lnt>121
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch</span> <span class=kn>import</span> <span class=n>nn</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn.functional</span> <span class=k>as</span> <span class=nn>F</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>math</span> <span class=kn>import</span> <span class=n>sqrt</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>AttentionHead</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>embed_dim</span><span class=p>,</span> <span class=n>head_dim</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>q</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>embed_dim</span><span class=p>,</span> <span class=n>head_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>k</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>embed_dim</span><span class=p>,</span> <span class=n>head_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>v</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>embed_dim</span><span class=p>,</span> <span class=n>head_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>query</span><span class=p>,</span> <span class=n>key</span><span class=p>,</span> <span class=n>value</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>query</span><span class=p>,</span> <span class=n>key</span><span class=p>,</span> <span class=n>value</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>q</span><span class=p>(</span><span class=n>query</span><span class=p>),</span> <span class=bp>self</span><span class=o>.</span><span class=n>k</span><span class=p>(</span><span class=n>key</span><span class=p>),</span> <span class=bp>self</span><span class=o>.</span><span class=n>v</span><span class=p>(</span><span class=n>value</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>scores</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>bmm</span><span class=p>(</span><span class=n>query</span><span class=p>,</span> <span class=n>key</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span> <span class=o>/</span> <span class=n>sqrt</span><span class=p>(</span><span class=n>query</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>mask</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>scores</span> <span class=o>=</span> <span class=n>scores</span><span class=o>.</span><span class=n>masked_fill</span><span class=p>(</span><span class=n>mask</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=o>-</span><span class=nb>float</span><span class=p>(</span><span class=s2>&#34;inf&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>weights</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>scores</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>torch</span><span class=o>.</span><span class=n>bmm</span><span class=p>(</span><span class=n>weights</span><span class=p>,</span> <span class=n>value</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>MultiHeadAttention</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>embed_dim</span> <span class=o>=</span> <span class=n>config</span><span class=o>.</span><span class=n>hidden_size</span>
</span></span><span class=line><span class=cl>        <span class=n>num_heads</span> <span class=o>=</span> <span class=n>config</span><span class=o>.</span><span class=n>num_attention_heads</span>
</span></span><span class=line><span class=cl>        <span class=n>head_dim</span> <span class=o>=</span> <span class=n>embed_dim</span> <span class=o>//</span> <span class=n>num_heads</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>heads</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=p>[</span><span class=n>AttentionHead</span><span class=p>(</span><span class=n>embed_dim</span><span class=p>,</span> <span class=n>head_dim</span><span class=p>)</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_heads</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>output_linear</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>embed_dim</span><span class=p>,</span> <span class=n>embed_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>query</span><span class=p>,</span> <span class=n>key</span><span class=p>,</span> <span class=n>value</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>query_mask</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>key_mask</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>query_mask</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=ow>and</span> <span class=n>key_mask</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>mask</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>bmm</span><span class=p>(</span><span class=n>query_mask</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>),</span> <span class=n>key_mask</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>([</span><span class=n>h</span><span class=p>(</span><span class=n>query</span><span class=p>,</span> <span class=n>key</span><span class=p>,</span> <span class=n>value</span><span class=p>,</span> <span class=n>mask</span><span class=p>)</span> <span class=k>for</span> <span class=n>h</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>heads</span><span class=p>],</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>output_linear</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>FeedForward</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>linear_1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>intermediate_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>linear_2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>intermediate_size</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>gelu</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>GELU</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>hidden_dropout_prob</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>linear_1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>gelu</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>linear_2</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>TransformerEncoderLayer</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>layer_norm_1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>layer_norm_2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>attention</span> <span class=o>=</span> <span class=n>MultiHeadAttention</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>feed_forward</span> <span class=o>=</span> <span class=n>FeedForward</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># Apply layer normalization and then copy input into query, key, value</span>
</span></span><span class=line><span class=cl>        <span class=n>hidden_state</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>layer_norm_1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># Apply attention with a skip connection</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>attention</span><span class=p>(</span><span class=n>hidden_state</span><span class=p>,</span> <span class=n>hidden_state</span><span class=p>,</span> <span class=n>hidden_state</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=n>mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># Apply feed-forward layer with a skip connection</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>feed_forward</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>layer_norm_2</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>Embeddings</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>token_embeddings</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>vocab_size</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                             <span class=n>config</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>position_embeddings</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>max_position_embeddings</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                                <span class=n>config</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>layer_norm</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>,</span> <span class=n>eps</span><span class=o>=</span><span class=mf>1e-12</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_ids</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># Create position IDs for input sequence</span>
</span></span><span class=line><span class=cl>        <span class=n>seq_length</span> <span class=o>=</span> <span class=n>input_ids</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>position_ids</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>seq_length</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>long</span><span class=p>)</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># Create token and position embeddings</span>
</span></span><span class=line><span class=cl>        <span class=n>token_embeddings</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>token_embeddings</span><span class=p>(</span><span class=n>input_ids</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>position_embeddings</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>position_embeddings</span><span class=p>(</span><span class=n>position_ids</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># Combine token and position embeddings</span>
</span></span><span class=line><span class=cl>        <span class=n>embeddings</span> <span class=o>=</span> <span class=n>token_embeddings</span> <span class=o>+</span> <span class=n>position_embeddings</span>
</span></span><span class=line><span class=cl>        <span class=n>embeddings</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>layer_norm</span><span class=p>(</span><span class=n>embeddings</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>embeddings</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>embeddings</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>embeddings</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>TransformerEncoder</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>embeddings</span> <span class=o>=</span> <span class=n>Embeddings</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>layers</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=p>[</span><span class=n>TransformerEncoderLayer</span><span class=p>(</span><span class=n>config</span><span class=p>)</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>num_hidden_layers</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>embeddings</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>layer</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>layers</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=n>layer</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s1>&#39;__main__&#39;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoConfig</span>
</span></span><span class=line><span class=cl>    <span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoTokenizer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>model_ckpt</span> <span class=o>=</span> <span class=s2>&#34;bert-base-uncased&#34;</span>\
</span></span><span class=line><span class=cl>    <span class=n>cache_dir</span> <span class=o>=</span> <span class=s1>&#39;./pretrained_model&#39;</span>\
</span></span><span class=line><span class=cl>    <span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_ckpt</span><span class=p>,</span> <span class=n>cache_dir</span><span class=o>=</span><span class=n>cache_dir</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>config</span> <span class=o>=</span> <span class=n>AutoConfig</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_ckpt</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>text</span> <span class=o>=</span> <span class=s2>&#34;hello world&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>inputs</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>text</span><span class=p>,</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>,</span> <span class=n>add_special_tokens</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>encoder</span> <span class=o>=</span> <span class=n>TransformerEncoder</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>encoder</span><span class=p>(</span><span class=n>inputs</span><span class=o>.</span><span class=n>input_ids</span><span class=p>)</span><span class=o>.</span><span class=n>size</span><span class=p>())</span>
</span></span></code></pre></td></tr></table></div></div><hr><blockquote><p>[!IMPORTANT]</p></blockquote><h2 id=绝对位置编码-1>绝对位置编码<a hidden class=anchor aria-hidden=true href=#绝对位置编码-1>#</a></h2><p>使用Pytorch实现Transformer中的绝对位置编码底层代码</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span><span class=lnt>61
</span><span class=lnt>62
</span><span class=lnt>63
</span><span class=lnt>64
</span><span class=lnt>65
</span><span class=lnt>66
</span><span class=lnt>67
</span><span class=lnt>68
</span><span class=lnt>69
</span><span class=lnt>70
</span><span class=lnt>71
</span><span class=lnt>72
</span><span class=lnt>73
</span><span class=lnt>74
</span><span class=lnt>75
</span><span class=lnt>76
</span><span class=lnt>77
</span><span class=lnt>78
</span><span class=lnt>79
</span><span class=lnt>80
</span><span class=lnt>81
</span><span class=lnt>82
</span><span class=lnt>83
</span><span class=lnt>84
</span><span class=lnt>85
</span><span class=lnt>86
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=ch>#!/usr/bin/env python</span>
</span></span><span class=line><span class=cl><span class=c1># -*- encoding: utf-8 -*-</span>
</span></span><span class=line><span class=cl><span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>@File    :   Positional_Encoding.py
</span></span></span><span class=line><span class=cl><span class=s1>@Time    :   2024/09/26 11:23:36
</span></span></span><span class=line><span class=cl><span class=s1>@Author  :   pan binghong 
</span></span></span><span class=line><span class=cl><span class=s1>@Email   :   19909442097@163.com
</span></span></span><span class=line><span class=cl><span class=s1>@description   :   Transformer中的绝对位置编码底层代码实现
</span></span></span><span class=line><span class=cl><span class=s1>&#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>math</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>BertTokenizer</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>os</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>file_path</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>abspath</span><span class=p>(</span><span class=vm>__file__</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>dir</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>dirname</span><span class=p>(</span><span class=n>file_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>os</span><span class=o>.</span><span class=n>chdir</span><span class=p>(</span><span class=nb>dir</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>PositionalEncoding</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>max_len</span><span class=o>=</span><span class=mi>5000</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>        :param d_model: 模型的维度
</span></span></span><span class=line><span class=cl><span class=s1>        :param max_len: 序列的最大长度
</span></span></span><span class=line><span class=cl><span class=s1>        &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>PositionalEncoding</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 创建一个形状为 (max_len, d_model) 的矩阵, 用于存储位置编码</span>
</span></span><span class=line><span class=cl>        <span class=n>pe</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>max_len</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 创建一个形状为 (max_len, 1) 的矩阵, 用于存储位置信息, 保存索引值 e.g.[0, 1, 2, ... , max_len-1]</span>
</span></span><span class=line><span class=cl>        <span class=n>position</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>max_len</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float</span><span class=p>)</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 这段代码计算位置编码中的两个频率。具体来说，它生成一个从0到d_model（不包括d_model）的偶数序列，</span>
</span></span><span class=line><span class=cl>        <span class=c1># 然后将这些偶数转换为浮点数，并乘以一个常数因子 (-math.log(10000.0) / d_model)。</span>
</span></span><span class=line><span class=cl>        <span class=c1># 这个常数因子是通过对10000取自然对数并除以d_model得到的。</span>
</span></span><span class=line><span class=cl>        <span class=c1># 最后，通过torch.exp函数计算这些值的指数，得到最终的div_term。</span>
</span></span><span class=line><span class=cl>        <span class=n>div_term</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>float</span><span class=p>()</span> <span class=o>*</span> <span class=p>(</span><span class=o>-</span><span class=n>math</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=mf>10000.0</span><span class=p>)</span> <span class=o>/</span> <span class=n>d_model</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 应用正弦函数 得到偶数索引位置编码</span>
</span></span><span class=line><span class=cl>        <span class=n>pe</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>::</span><span class=mi>2</span><span class=p>]</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>sin</span><span class=p>(</span><span class=n>position</span> <span class=o>*</span> <span class=n>div_term</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 应用余弦函数 得到奇数索引位置编码</span>
</span></span><span class=line><span class=cl>        <span class=n>pe</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>::</span><span class=mi>2</span><span class=p>]</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cos</span><span class=p>(</span><span class=n>position</span> <span class=o>*</span> <span class=n>div_term</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 增加一个 batch 维度, 使其能够与输入一起使用</span>
</span></span><span class=line><span class=cl>        <span class=n>pe</span> <span class=o>=</span> <span class=n>pe</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 将位置编码矩阵注册为一个参数, 并将其添加到模型参数列表中</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>register_buffer</span><span class=p>(</span><span class=s1>&#39;pe&#39;</span><span class=p>,</span> <span class=n>pe</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>        :param x: 输入的序列张量, shape为: &lt;batch_size, seq_len, d_model&gt;
</span></span></span><span class=line><span class=cl><span class=s1>        :return: 输出的序列张量, shape为: &lt;batch_size, seq_len, d_model&gt;
</span></span></span><span class=line><span class=cl><span class=s1>        &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>pe</span><span class=p>[:,</span> <span class=p>:</span><span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>1</span><span class=p>),</span> <span class=p>:]</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s1>&#39;__main__&#39;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=c1># 初始化参数</span>
</span></span><span class=line><span class=cl>    <span class=n>d_model</span> <span class=o>=</span> <span class=mi>512</span>
</span></span><span class=line><span class=cl>    <span class=n>max_len</span> <span class=o>=</span> <span class=mi>2048</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 初始化位置编码</span>
</span></span><span class=line><span class=cl>    <span class=n>pos_encoder</span> <span class=o>=</span> <span class=n>PositionalEncoding</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>max_len</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 初始化 tokenizer (这里以 Bert 为例)</span>
</span></span><span class=line><span class=cl>    <span class=n>tokenizer</span> <span class=o>=</span> <span class=n>BertTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s1>&#39;bert-base-uncased&#39;</span><span class=p>,</span> <span class=n>cache_dir</span><span class=o>=</span><span class=s1>&#39;./cache&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 使用输入&#34;hello world&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>input_text</span> <span class=o>=</span> <span class=s2>&#34;hello world&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>input_ids</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>input_text</span><span class=p>,</span> <span class=n>add_special_tokens</span><span class=o>=</span><span class=kc>True</span><span class=p>)])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 创建一个形状为 (1, seq_len, d_model) 的零矩阵</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>input_ids</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>1</span><span class=p>),</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 应用位置编码</span>
</span></span><span class=line><span class=cl>    <span class=n>output</span> <span class=o>=</span> <span class=n>pos_encoder</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Input Text:&#34;</span><span class=p>,</span> <span class=n>input_text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Input IDs Shape:&#34;</span><span class=p>,</span> <span class=n>input_ids</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Output Shape:&#34;</span><span class=p>,</span> <span class=n>output</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Output:&#34;</span><span class=p>,</span> <span class=n>output</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>以上案例为 : 当输入hello world, 经过编码后输出其对应的信息</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>Input Text: hello world
</span></span><span class=line><span class=cl>Input IDs Shape: torch.Size<span class=o>([</span>1, 4<span class=o>])</span>
</span></span><span class=line><span class=cl>Output Shape: torch.Size<span class=o>([</span>1, 4, 512<span class=o>])</span>
</span></span><span class=line><span class=cl>Output: tensor<span class=o>([[[</span> 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,
</span></span><span class=line><span class=cl>           0.0000e+00,  1.0000e+00<span class=o>]</span>,
</span></span><span class=line><span class=cl>         <span class=o>[</span> 8.4147e-01,  5.4030e-01,  8.2186e-01,  ...,  1.0000e+00,
</span></span><span class=line><span class=cl>           1.0366e-04,  1.0000e+00<span class=o>]</span>,
</span></span><span class=line><span class=cl>         <span class=o>[</span> 9.0930e-01, -4.1615e-01,  9.3641e-01,  ...,  1.0000e+00,
</span></span><span class=line><span class=cl>           2.0733e-04,  1.0000e+00<span class=o>]</span>,
</span></span><span class=line><span class=cl>         <span class=o>[</span> 1.4112e-01, -9.8999e-01,  2.4509e-01,  ...,  1.0000e+00,
</span></span><span class=line><span class=cl>           3.1099e-04,  1.0000e+00<span class=o>]]])</span>
</span></span></code></pre></td></tr></table></div></div><hr><h2 id=多头注意力机制>多头注意力机制<a hidden class=anchor aria-hidden=true href=#多头注意力机制>#</a></h2><p>使用Pytorch实现多头注意力机制的底层代码, 含例子.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>  1
</span><span class=lnt>  2
</span><span class=lnt>  3
</span><span class=lnt>  4
</span><span class=lnt>  5
</span><span class=lnt>  6
</span><span class=lnt>  7
</span><span class=lnt>  8
</span><span class=lnt>  9
</span><span class=lnt> 10
</span><span class=lnt> 11
</span><span class=lnt> 12
</span><span class=lnt> 13
</span><span class=lnt> 14
</span><span class=lnt> 15
</span><span class=lnt> 16
</span><span class=lnt> 17
</span><span class=lnt> 18
</span><span class=lnt> 19
</span><span class=lnt> 20
</span><span class=lnt> 21
</span><span class=lnt> 22
</span><span class=lnt> 23
</span><span class=lnt> 24
</span><span class=lnt> 25
</span><span class=lnt> 26
</span><span class=lnt> 27
</span><span class=lnt> 28
</span><span class=lnt> 29
</span><span class=lnt> 30
</span><span class=lnt> 31
</span><span class=lnt> 32
</span><span class=lnt> 33
</span><span class=lnt> 34
</span><span class=lnt> 35
</span><span class=lnt> 36
</span><span class=lnt> 37
</span><span class=lnt> 38
</span><span class=lnt> 39
</span><span class=lnt> 40
</span><span class=lnt> 41
</span><span class=lnt> 42
</span><span class=lnt> 43
</span><span class=lnt> 44
</span><span class=lnt> 45
</span><span class=lnt> 46
</span><span class=lnt> 47
</span><span class=lnt> 48
</span><span class=lnt> 49
</span><span class=lnt> 50
</span><span class=lnt> 51
</span><span class=lnt> 52
</span><span class=lnt> 53
</span><span class=lnt> 54
</span><span class=lnt> 55
</span><span class=lnt> 56
</span><span class=lnt> 57
</span><span class=lnt> 58
</span><span class=lnt> 59
</span><span class=lnt> 60
</span><span class=lnt> 61
</span><span class=lnt> 62
</span><span class=lnt> 63
</span><span class=lnt> 64
</span><span class=lnt> 65
</span><span class=lnt> 66
</span><span class=lnt> 67
</span><span class=lnt> 68
</span><span class=lnt> 69
</span><span class=lnt> 70
</span><span class=lnt> 71
</span><span class=lnt> 72
</span><span class=lnt> 73
</span><span class=lnt> 74
</span><span class=lnt> 75
</span><span class=lnt> 76
</span><span class=lnt> 77
</span><span class=lnt> 78
</span><span class=lnt> 79
</span><span class=lnt> 80
</span><span class=lnt> 81
</span><span class=lnt> 82
</span><span class=lnt> 83
</span><span class=lnt> 84
</span><span class=lnt> 85
</span><span class=lnt> 86
</span><span class=lnt> 87
</span><span class=lnt> 88
</span><span class=lnt> 89
</span><span class=lnt> 90
</span><span class=lnt> 91
</span><span class=lnt> 92
</span><span class=lnt> 93
</span><span class=lnt> 94
</span><span class=lnt> 95
</span><span class=lnt> 96
</span><span class=lnt> 97
</span><span class=lnt> 98
</span><span class=lnt> 99
</span><span class=lnt>100
</span><span class=lnt>101
</span><span class=lnt>102
</span><span class=lnt>103
</span><span class=lnt>104
</span><span class=lnt>105
</span><span class=lnt>106
</span><span class=lnt>107
</span><span class=lnt>108
</span><span class=lnt>109
</span><span class=lnt>110
</span><span class=lnt>111
</span><span class=lnt>112
</span><span class=lnt>113
</span><span class=lnt>114
</span><span class=lnt>115
</span><span class=lnt>116
</span><span class=lnt>117
</span><span class=lnt>118
</span><span class=lnt>119
</span><span class=lnt>120
</span><span class=lnt>121
</span><span class=lnt>122
</span><span class=lnt>123
</span><span class=lnt>124
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=ch>#!/usr/bin/env python</span>
</span></span><span class=line><span class=cl><span class=c1># -*- encoding: utf-8 -*-</span>
</span></span><span class=line><span class=cl><span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>@File    :   MultiHeadAttention.py
</span></span></span><span class=line><span class=cl><span class=s1>@Time    :   2024/09/27 09:50:43
</span></span></span><span class=line><span class=cl><span class=s1>@Author  :   pan binghong 
</span></span></span><span class=line><span class=cl><span class=s1>@Email   :   19909442097@163.com
</span></span></span><span class=line><span class=cl><span class=s1>@description   :   
</span></span></span><span class=line><span class=cl><span class=s1>&#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>os</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>BertTokenizer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 获取当前文件的绝对路径</span>
</span></span><span class=line><span class=cl><span class=n>file_path</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>abspath</span><span class=p>(</span><span class=vm>__file__</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 获取当前文件所在的目录路径</span>
</span></span><span class=line><span class=cl><span class=nb>dir</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>dirname</span><span class=p>(</span><span class=n>file_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 将当前工作目录更改为当前文件所在的目录</span>
</span></span><span class=line><span class=cl><span class=n>os</span><span class=o>.</span><span class=n>chdir</span><span class=p>(</span><span class=nb>dir</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>MultiHeadAttention</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>embed_size</span><span class=p>,</span> <span class=n>heads</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>        初始化多头注意力机制
</span></span></span><span class=line><span class=cl><span class=s1>        :param embed_size: 输入嵌入向量的维度
</span></span></span><span class=line><span class=cl><span class=s1>        :param num_heads: 多头注意力机制的头数
</span></span></span><span class=line><span class=cl><span class=s1>        &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>MultiHeadAttention</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>assert</span> <span class=n>embed_size</span> <span class=o>%</span> <span class=n>heads</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=s2>&#34;嵌入维度必须能被头数整除&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>embed_size</span> <span class=o>=</span> <span class=n>embed_size</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>heads</span> <span class=o>=</span> <span class=n>heads</span>
</span></span><span class=line><span class=cl>        <span class=c1># 每个头的维度</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>heads_dim</span> <span class=o>=</span> <span class=n>embed_size</span> <span class=o>//</span> <span class=n>heads</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>value</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>heads_dim</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>heads_dim</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>key</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>heads_dim</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>heads_dim</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>query</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>heads_dim</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>heads_dim</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>fc_out</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>heads</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>heads_dim</span><span class=p>,</span> <span class=n>embed_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>query</span><span class=p>,</span> <span class=n>keys</span><span class=p>,</span> <span class=n>values</span><span class=p>,</span> <span class=n>mask</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>N</span> <span class=o>=</span> <span class=n>query</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>value_len</span><span class=p>,</span> <span class=n>key_len</span><span class=p>,</span> <span class=n>query_len</span> <span class=o>=</span> <span class=n>values</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>keys</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>query</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 将输入的数据划分为多个头, 先调整shape为 (N, value_len, heads, heads_dim)</span>
</span></span><span class=line><span class=cl>        <span class=n>values</span> <span class=o>=</span> <span class=n>values</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>N</span><span class=p>,</span> <span class=n>value_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>heads_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>keys</span> <span class=o>=</span> <span class=n>keys</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>N</span><span class=p>,</span> <span class=n>key_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>heads_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>query</span> <span class=o>=</span> <span class=n>query</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>N</span><span class=p>,</span> <span class=n>query_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>heads_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 初始化Q, K, V矩阵</span>
</span></span><span class=line><span class=cl>        <span class=n>values</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>value</span><span class=p>(</span><span class=n>values</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>keys</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>key</span><span class=p>(</span><span class=n>keys</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>query</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>query</span><span class=p>(</span><span class=n>query</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 计算注意力分数</span>
</span></span><span class=line><span class=cl>        <span class=c1># 这行代码使用爱因斯坦求和约定（Einstein Summation Convention）来计算注意力分数。</span>
</span></span><span class=line><span class=cl>        <span class=c1># 具体来说，它通过矩阵乘法计算query和keys之间的点积，然后将其重塑为所需的形状。</span>
</span></span><span class=line><span class=cl>        <span class=c1># </span>
</span></span><span class=line><span class=cl>        <span class=c1># 参数解释：</span>
</span></span><span class=line><span class=cl>        <span class=c1># - &#34;nqhd,nkhd-&gt;nhqk&#34; 是爱因斯坦求和约定的字符串表示。</span>
</span></span><span class=line><span class=cl>        <span class=c1>#   - &#34;n&#34; 表示批次大小（batch size）。</span>
</span></span><span class=line><span class=cl>        <span class=c1>#   - &#34;q&#34; 表示查询序列的长度（query length）。</span>
</span></span><span class=line><span class=cl>        <span class=c1>#   - &#34;h&#34; 表示注意力头数（number of heads）。</span>
</span></span><span class=line><span class=cl>        <span class=c1>#   - &#34;d&#34; 表示每个头的维度（dimension per head）。</span>
</span></span><span class=line><span class=cl>        <span class=c1>#   - &#34;k&#34; 表示键序列的长度（key length）。</span>
</span></span><span class=line><span class=cl>        <span class=c1># </span>
</span></span><span class=line><span class=cl>        <span class=c1># 具体操作：</span>
</span></span><span class=line><span class=cl>        <span class=c1># - query 的形状为 (N, query_len, heads, heads_dim)。</span>
</span></span><span class=line><span class=cl>        <span class=c1># - keys 的形状为 (N, key_len, heads, heads_dim)。</span>
</span></span><span class=line><span class=cl>        <span class=c1># - 通过 torch.einsum 计算 query 和 keys 的点积，结果的形状为 (N, heads, query_len, key_len)。</span>
</span></span><span class=line><span class=cl>        <span class=c1># </span>
</span></span><span class=line><span class=cl>        <span class=c1># 计算过程：</span>
</span></span><span class=line><span class=cl>        <span class=c1># - 对于每个批次（n），每个头（h），计算 query 和 keys 的点积，得到一个形状为 (query_len, key_len) 的矩阵。</span>
</span></span><span class=line><span class=cl>        <span class=c1># - 最终结果是一个形状为 (N, heads, query_len, key_len) 的张量，表示每个查询和每个键之间的注意力分数。</span>
</span></span><span class=line><span class=cl>        <span class=n>energy</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>einsum</span><span class=p>(</span><span class=s2>&#34;nqhd,nkhd-&gt;nhqk&#34;</span><span class=p>,</span> <span class=p>[</span><span class=n>query</span><span class=p>,</span> <span class=n>keys</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 应用注意力机制</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>mask</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>energy</span> <span class=o>=</span> <span class=n>energy</span><span class=o>.</span><span class=n>masked_fill</span><span class=p>(</span><span class=n>mask</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=nb>float</span><span class=p>(</span><span class=s1>&#39;-1e20&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>attention</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>energy</span> <span class=o>/</span> <span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>embed_size</span> <span class=o>**</span> <span class=p>(</span><span class=mi>1</span><span class=o>/</span><span class=mi>2</span><span class=p>)),</span> <span class=n>dim</span><span class=o>=</span><span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>einsum</span><span class=p>(</span><span class=s2>&#34;nhql,nlhd-&gt;nqhd&#34;</span><span class=p>,</span> <span class=p>[</span><span class=n>attention</span><span class=p>,</span> <span class=n>values</span><span class=p>])</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>N</span><span class=p>,</span> <span class=n>query_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>heads</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>heads_dim</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc_out</span><span class=p>(</span><span class=n>out</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>out</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s1>&#39;__main__&#39;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>tokenizer</span> <span class=o>=</span> <span class=n>BertTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s1>&#39;bert-base-uncased&#39;</span><span class=p>,</span> <span class=n>cache_dir</span><span class=o>=</span><span class=s1>&#39;./cache&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>input_text</span> <span class=o>=</span> <span class=s2>&#34;Hello world!&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 初始化随机keys,values, query</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>tokens</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>tokenize</span><span class=p>(</span><span class=n>input_text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>token_ids</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>convert_tokens_to_ids</span><span class=p>(</span><span class=n>tokens</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>token_ids</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=n>token_ids</span><span class=p>])</span><span class=o>.</span><span class=n>long</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>batch_size</span><span class=p>,</span><span class=n>seq_length</span> <span class=o>=</span> <span class=n>token_ids</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl>    <span class=n>embed_size</span> <span class=o>=</span><span class=mi>512</span>
</span></span><span class=line><span class=cl>    <span class=n>heads</span> <span class=o>=</span> <span class=mi>8</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>values</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>rand</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_length</span><span class=p>,</span> <span class=n>embed_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>keys</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>rand</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_length</span><span class=p>,</span> <span class=n>embed_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>query</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>rand</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_length</span><span class=p>,</span> <span class=n>embed_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>mask</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>attention</span> <span class=o>=</span> <span class=n>MultiHeadAttention</span><span class=p>(</span><span class=n>embed_size</span><span class=p>,</span> <span class=n>heads</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>out</span> <span class=o>=</span> <span class=n>attention</span><span class=p>(</span><span class=n>query</span><span class=p>,</span> <span class=n>keys</span><span class=p>,</span> <span class=n>values</span><span class=p>,</span> <span class=n>mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Tokens: </span><span class=si>{</span><span class=n>tokens</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Token IDs: </span><span class=si>{</span><span class=n>token_ids</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>out</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Multi-head Attention Output: </span><span class=se>\n</span><span class=si>{</span><span class=n>out</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>运行以上代码后输出 :</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>Tokens</span><span class=p>:</span> <span class=p>[</span><span class=s1>&#39;hello&#39;</span><span class=p>,</span> <span class=s1>&#39;world&#39;</span><span class=p>,</span> <span class=s1>&#39;!&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>Token</span> <span class=n>IDs</span><span class=p>:</span> <span class=n>tensor</span><span class=p>([[</span><span class=mi>7592</span><span class=p>,</span> <span class=mi>2088</span><span class=p>,</span>  <span class=mi>999</span><span class=p>]])</span>
</span></span><span class=line><span class=cl><span class=n>torch</span><span class=o>.</span><span class=n>Size</span><span class=p>([</span><span class=mi>1</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>512</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>Multi</span><span class=o>-</span><span class=n>head</span> <span class=n>Attention</span> <span class=n>Output</span><span class=p>:</span>
</span></span><span class=line><span class=cl><span class=n>tensor</span><span class=p>([[[</span><span class=o>-</span><span class=mf>0.0855</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2076</span><span class=p>,</span>  <span class=mf>0.0354</span><span class=p>,</span>  <span class=o>...</span><span class=p>,</span>  <span class=mf>0.1631</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0024</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1372</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         <span class=p>[</span><span class=o>-</span><span class=mf>0.0852</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2079</span><span class=p>,</span>  <span class=mf>0.0366</span><span class=p>,</span>  <span class=o>...</span><span class=p>,</span>  <span class=mf>0.1636</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0025</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1374</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         <span class=p>[</span><span class=o>-</span><span class=mf>0.0860</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2076</span><span class=p>,</span>  <span class=mf>0.0364</span><span class=p>,</span>  <span class=o>...</span><span class=p>,</span>  <span class=mf>0.1644</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0018</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1361</span><span class=p>]]],</span>
</span></span><span class=line><span class=cl>       <span class=n>grad_fn</span><span class=o>=&lt;</span><span class=n>ViewBackward0</span><span class=o>&gt;</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><hr><h2 id=前馈神经网络-1>前馈神经网络<a hidden class=anchor aria-hidden=true href=#前馈神经网络-1>#</a></h2><p>就是一个简单的全连接层…没啥好说的, 看代码 :</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=ch>#!/usr/bin/env python</span>
</span></span><span class=line><span class=cl><span class=c1># -*- encoding: utf-8 -*-</span>
</span></span><span class=line><span class=cl><span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>@File    :   FeedForwardNetwork.py
</span></span></span><span class=line><span class=cl><span class=s1>@Time    :   2024/09/27 14:46:48
</span></span></span><span class=line><span class=cl><span class=s1>@Author  :   pan binghong 
</span></span></span><span class=line><span class=cl><span class=s1>@Email   :   19909442097@163.com
</span></span></span><span class=line><span class=cl><span class=s1>@description   :   
</span></span></span><span class=line><span class=cl><span class=s1>&#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>os</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>BertTokenizer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>file_path</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>abspath</span><span class=p>(</span><span class=vm>__file__</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>dir</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>dirname</span><span class=p>(</span><span class=n>file_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>os</span><span class=o>.</span><span class=n>chdir</span><span class=p>(</span><span class=nb>dir</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>FeedForwardNetwork</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>,</span> <span class=n>dropout</span><span class=o>=</span><span class=mf>0.1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>        :param d_model:输入的特征维度大小
</span></span></span><span class=line><span class=cl><span class=s1>        :param hidden_size:隐藏层大小
</span></span></span><span class=line><span class=cl><span class=s1>        :param dropout:dropout概率
</span></span></span><span class=line><span class=cl><span class=s1>        &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>FeedForwardNetwork</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>liner1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>relu</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>liner2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_size</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>        :param x:输入的特征
</span></span></span><span class=line><span class=cl><span class=s1>        :return:输出的特征
</span></span></span><span class=line><span class=cl><span class=s1>        &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>liner1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>liner2</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s1>&#39;__main__&#39;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>tokenizer</span> <span class=o>=</span> <span class=n>BertTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s1>&#39;bert-base-uncased&#39;</span><span class=p>,</span><span class=n>cache_dir</span><span class=o>=</span><span class=s1>&#39;./cache&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>input_text</span> <span class=o>=</span> <span class=s2>&#34;Hello world&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>tokens</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>tokenize</span><span class=p>(</span><span class=n>input_text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>token_ids</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>convert_tokens_to_ids</span><span class=p>(</span><span class=n>tokens</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>token_ids</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=n>token_ids</span><span class=p>])</span><span class=o>.</span><span class=n>long</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>d_model</span> <span class=o>=</span> <span class=n>token_ids</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>hidden_size</span> <span class=o>=</span> <span class=mi>256</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>ff_netword</span> <span class=o>=</span> <span class=n>FeedForwardNetwork</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>output</span> <span class=o>=</span> <span class=n>ff_netword</span><span class=p>(</span><span class=n>token_ids</span><span class=o>.</span><span class=n>float</span><span class=p>())</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;Input tokens: </span><span class=se>\n</span><span class=si>{</span><span class=n>tokens</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;Input token_ids: </span><span class=se>\n</span><span class=si>{</span><span class=n>token_ids</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;Output from FeedForwardNetwork: </span><span class=se>\n</span><span class=si>{</span><span class=n>output</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;Output shape: </span><span class=se>\n</span><span class=si>{</span><span class=n>output</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>输出结果 :</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>Input</span> <span class=n>tokens</span><span class=p>:</span>
</span></span><span class=line><span class=cl><span class=p>[</span><span class=s1>&#39;hello&#39;</span><span class=p>,</span> <span class=s1>&#39;world&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>Input</span> <span class=n>token_ids</span><span class=p>:</span>
</span></span><span class=line><span class=cl><span class=n>tensor</span><span class=p>([[</span><span class=mi>7592</span><span class=p>,</span> <span class=mi>2088</span><span class=p>]])</span>
</span></span><span class=line><span class=cl><span class=n>Output</span> <span class=kn>from</span> <span class=nn>FeedForwardNetwork</span><span class=p>:</span>
</span></span><span class=line><span class=cl><span class=n>tensor</span><span class=p>([[</span><span class=mf>2332.0767</span><span class=p>,</span> <span class=mf>1194.7576</span><span class=p>]],</span> <span class=n>grad_fn</span><span class=o>=&lt;</span><span class=n>MulBackward0</span><span class=o>&gt;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>Output</span> <span class=n>shape</span><span class=p>:</span>
</span></span><span class=line><span class=cl><span class=n>torch</span><span class=o>.</span><span class=n>Size</span><span class=p>([</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>])</span>
</span></span></code></pre></td></tr></table></div></div><h1 id=层归一化--残差连接-1>层归一化 & 残差连接<a hidden class=anchor aria-hidden=true href=#层归一化--残差连接-1>#</a></h1><hr><p>Layer Normalization</p><hr><p>代码如下 :</p><hr><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-javascript data-lang=javascript><span class=line><span class=cl><span class=ch>#!/usr/bin/env python
</span></span></span><span class=line><span class=cl><span class=ch></span><span class=err>#</span> <span class=o>-*-</span> <span class=nx>encoding</span><span class=o>:</span> <span class=nx>utf</span><span class=o>-</span><span class=mi>8</span> <span class=o>-*-</span>
</span></span><span class=line><span class=cl><span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>@File    :   LayerNormalization.py
</span></span></span><span class=line><span class=cl><span class=s1>@Time    :   2024/09/27 15:17:48
</span></span></span><span class=line><span class=cl><span class=s1>@Author  :   pan binghong 
</span></span></span><span class=line><span class=cl><span class=s1>@Email   :   19909442097@163.com
</span></span></span><span class=line><span class=cl><span class=s1>@description   :   
</span></span></span><span class=line><span class=cl><span class=s1>&#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl><span class=kr>import</span> <span class=nx>torch</span>
</span></span><span class=line><span class=cl><span class=kr>import</span> <span class=nx>torch</span><span class=p>.</span><span class=nx>nn</span> <span class=nx>as</span> <span class=nx>nn</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kr>class</span> <span class=nx>LayerNormalization</span><span class=p>(</span><span class=nx>nn</span><span class=p>.</span><span class=nx>Module</span><span class=p>)</span><span class=o>:</span>
</span></span><span class=line><span class=cl>    <span class=nx>def</span> <span class=nx>__init__</span><span class=p>(</span><span class=nx>self</span><span class=p>,</span> <span class=nx>features</span><span class=p>,</span> <span class=nx>eps</span><span class=o>=</span><span class=mf>1e-6</span><span class=p>)</span><span class=o>:</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>        初始化层归一化模块
</span></span></span><span class=line><span class=cl><span class=s1>        :param features: 特征维度大小
</span></span></span><span class=line><span class=cl><span class=s1>        :param eps: 防止除零的小常数
</span></span></span><span class=line><span class=cl><span class=s1>        &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>        <span class=kr>super</span><span class=p>(</span><span class=nx>LayerNormalization</span><span class=p>,</span> <span class=nx>self</span><span class=p>).</span><span class=nx>__init__</span><span class=p>()</span>  <span class=err>#</span> <span class=nx>调用父类nn</span><span class=p>.</span><span class=nx>Module的初始化方法</span>
</span></span><span class=line><span class=cl>        <span class=nx>self</span><span class=p>.</span><span class=nx>eps</span> <span class=o>=</span> <span class=nx>eps</span>  <span class=err>#</span> <span class=nx>设置防止除零的小常数</span>
</span></span><span class=line><span class=cl>        <span class=nx>self</span><span class=p>.</span><span class=nx>gain</span> <span class=o>=</span> <span class=nx>nn</span><span class=p>.</span><span class=nx>Parameter</span><span class=p>(</span><span class=nx>torch</span><span class=p>.</span><span class=nx>ones</span><span class=p>(</span><span class=nx>features</span><span class=p>))</span>  <span class=err>#</span> <span class=nx>初始化增益参数</span><span class=err>，</span><span class=nx>形状为</span><span class=p>(</span><span class=nx>features</span><span class=p>,)</span><span class=err>，</span><span class=nx>初始值为1</span>
</span></span><span class=line><span class=cl>        <span class=nx>self</span><span class=p>.</span><span class=nx>bias</span> <span class=o>=</span> <span class=nx>nn</span><span class=p>.</span><span class=nx>Parameter</span><span class=p>(</span><span class=nx>torch</span><span class=p>.</span><span class=nx>zeros</span><span class=p>(</span><span class=nx>features</span><span class=p>))</span>  <span class=err>#</span> <span class=nx>初始化偏置参数</span><span class=err>，</span><span class=nx>形状为</span><span class=p>(</span><span class=nx>features</span><span class=p>,)</span><span class=err>，</span><span class=nx>初始值为0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nx>def</span> <span class=nx>forward</span><span class=p>(</span><span class=nx>self</span><span class=p>,</span> <span class=nx>x</span><span class=p>)</span><span class=o>:</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>        前向传播函数
</span></span></span><span class=line><span class=cl><span class=s1>        :param x: 输入张量，形状为(batch_size, seq_len, features)
</span></span></span><span class=line><span class=cl><span class=s1>        :return: 归一化后的输出张量
</span></span></span><span class=line><span class=cl><span class=s1>        &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>        <span class=nx>mean</span> <span class=o>=</span> <span class=nx>x</span><span class=p>.</span><span class=nx>mean</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=nx>keepdim</span><span class=o>=</span><span class=nx>True</span><span class=p>)</span>  <span class=err>#</span> <span class=nx>计算输入张量在最后一个维度上的均值</span><span class=err>，</span><span class=nx>保持维度</span>
</span></span><span class=line><span class=cl>        <span class=nx>std</span> <span class=o>=</span> <span class=nx>x</span><span class=p>.</span><span class=nx>std</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=nx>keepdim</span><span class=o>=</span><span class=nx>True</span><span class=p>)</span>  <span class=err>#</span> <span class=nx>计算输入张量在最后一个维度上的标准差</span><span class=err>，</span><span class=nx>保持维度</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=nx>self</span><span class=p>.</span><span class=nx>gain</span> <span class=o>*</span> <span class=p>(</span><span class=nx>x</span> <span class=o>-</span> <span class=nx>mean</span><span class=p>)</span> <span class=o>/</span> <span class=p>(</span><span class=nx>std</span> <span class=o>+</span> <span class=nx>self</span><span class=p>.</span><span class=nx>eps</span><span class=p>)</span> <span class=o>+</span> <span class=nx>self</span><span class=p>.</span><span class=nx>bias</span>  <span class=err>#</span> <span class=nx>应用层归一化公式并返回结果</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=nx>__name__</span> <span class=o>==</span> <span class=s2>&#34;__main__&#34;</span><span class=o>:</span>
</span></span><span class=line><span class=cl>    <span class=nx>batch_size</span> <span class=o>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>    <span class=nx>seq_len</span> <span class=o>=</span> <span class=mi>2048</span>
</span></span><span class=line><span class=cl>    <span class=nx>features</span> <span class=o>=</span> <span class=mi>4096</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=err>#</span> <span class=nx>创建一个简单的输入张量</span>
</span></span><span class=line><span class=cl>    <span class=nx>x</span> <span class=o>=</span> <span class=nx>torch</span><span class=p>.</span><span class=nx>randn</span><span class=p>(</span><span class=nx>batch_size</span><span class=p>,</span> <span class=nx>seq_len</span><span class=p>,</span> <span class=nx>features</span><span class=p>)</span>  <span class=err>#</span> <span class=nx>随机初始化输入张量</span>
</span></span><span class=line><span class=cl>    <span class=err>#</span> <span class=nx>初始化层归一化层</span>
</span></span><span class=line><span class=cl>    <span class=nx>ln</span> <span class=o>=</span> <span class=nx>LayerNormalization</span><span class=p>(</span><span class=nx>features</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=err>#</span> <span class=nx>应用层归一化</span>
</span></span><span class=line><span class=cl>    <span class=nx>normalized_x</span> <span class=o>=</span> <span class=nx>ln</span><span class=p>(</span><span class=nx>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=err>#</span> <span class=nx>打印原始和归一化后的张量</span>
</span></span><span class=line><span class=cl>    <span class=nx>print</span><span class=p>(</span><span class=s2>&#34;原始输入张量:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nx>print</span><span class=p>(</span><span class=nx>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nx>print</span><span class=p>(</span><span class=s2>&#34;\n归一化后的输出张量:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nx>print</span><span class=p>(</span><span class=nx>normalized_x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nx>print</span><span class=p>(</span><span class=s2>&#34;\n归一化后的维度:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nx>print</span><span class=p>(</span><span class=nx>normalized_x</span><span class=p>.</span><span class=nx>shape</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><hr><p>输出 :</p><hr><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-javascript data-lang=javascript><span class=line><span class=cl><span class=nx>原始输入张量</span><span class=o>:</span>
</span></span><span class=line><span class=cl><span class=nx>tensor</span><span class=p>([[[</span> <span class=mf>0.7926</span><span class=p>,</span>  <span class=mf>0.9379</span><span class=p>,</span>  <span class=mf>0.9247</span><span class=p>,</span>  <span class=p>...,</span> <span class=o>-</span><span class=mf>0.8849</span><span class=p>,</span>  <span class=mf>0.4262</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1126</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         <span class=p>[</span><span class=o>-</span><span class=mf>1.0093</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.3612</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.7135</span><span class=p>,</span>  <span class=p>...,</span>  <span class=mf>1.7116</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1164</span><span class=p>,</span>  <span class=mf>0.8453</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         <span class=p>[</span> <span class=mf>1.1906</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.3223</span><span class=p>,</span>  <span class=mf>0.3553</span><span class=p>,</span>  <span class=p>...,</span> <span class=o>-</span><span class=mf>0.0293</span><span class=p>,</span>  <span class=mf>0.6014</span><span class=p>,</span>  <span class=mf>0.3705</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         <span class=p>...,</span>
</span></span><span class=line><span class=cl>         <span class=p>[</span><span class=o>-</span><span class=mf>0.3714</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1437</span><span class=p>,</span>  <span class=mf>1.2062</span><span class=p>,</span>  <span class=p>...,</span> <span class=o>-</span><span class=mf>0.7603</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.2689</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.4045</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         <span class=p>[</span><span class=o>-</span><span class=mf>0.6838</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.5469</span><span class=p>,</span>  <span class=mf>0.2328</span><span class=p>,</span>  <span class=p>...,</span> <span class=o>-</span><span class=mf>0.4500</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.1035</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2005</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         <span class=p>[</span> <span class=mf>0.4267</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.5163</span><span class=p>,</span>  <span class=mf>1.1316</span><span class=p>,</span>  <span class=p>...,</span> <span class=o>-</span><span class=mf>0.1339</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.4004</span><span class=p>,</span>  <span class=mf>0.2841</span><span class=p>]]])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nx>归一化后的输出张量</span><span class=o>:</span>
</span></span><span class=line><span class=cl><span class=nx>tensor</span><span class=p>([[[</span> <span class=mf>0.7959</span><span class=p>,</span>  <span class=mf>0.9418</span><span class=p>,</span>  <span class=mf>0.9285</span><span class=p>,</span>  <span class=p>...,</span> <span class=o>-</span><span class=mf>0.8873</span><span class=p>,</span>  <span class=mf>0.4283</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1123</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         <span class=p>[</span><span class=o>-</span><span class=mf>0.9919</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.3548</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.7011</span><span class=p>,</span>  <span class=p>...,</span>  <span class=mf>1.6830</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1141</span><span class=p>,</span>  <span class=mf>0.8314</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         <span class=p>[</span> <span class=mf>1.2045</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.3012</span><span class=p>,</span>  <span class=mf>0.3732</span><span class=p>,</span>  <span class=p>...,</span> <span class=o>-</span><span class=mf>0.0096</span><span class=p>,</span>  <span class=mf>0.6181</span><span class=p>,</span>  <span class=mf>0.3883</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         <span class=p>...,</span>
</span></span><span class=line><span class=cl>         <span class=p>[</span><span class=o>-</span><span class=mf>0.3827</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1589</span><span class=p>,</span>  <span class=mf>1.1681</span><span class=p>,</span>  <span class=p>...,</span> <span class=o>-</span><span class=mf>0.7650</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.2649</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.4152</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         <span class=p>[</span><span class=o>-</span><span class=mf>0.7084</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.5709</span><span class=p>,</span>  <span class=mf>0.2130</span><span class=p>,</span>  <span class=p>...,</span> <span class=o>-</span><span class=mf>0.4735</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.1304</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2226</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         <span class=p>[</span> <span class=mf>0.3982</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.5383</span><span class=p>,</span>  <span class=mf>1.0983</span><span class=p>,</span>  <span class=p>...,</span> <span class=o>-</span><span class=mf>0.1585</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.4232</span><span class=p>,</span>  <span class=mf>0.2567</span><span class=p>]]],</span>
</span></span><span class=line><span class=cl>       <span class=nx>grad_fn</span><span class=o>=&lt;</span><span class=nx>AddBackward0</span><span class=o>&gt;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nx>归一化后的维度</span><span class=o>:</span>
</span></span><span class=line><span class=cl><span class=nx>torch</span><span class=p>.</span><span class=nx>Size</span><span class=p>([</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2048</span><span class=p>,</span> <span class=mi>4096</span><span class=p>])</span>
</span></span></code></pre></td></tr></table></div></div><hr><h3 id=residual-connections>Residual Connections<a hidden class=anchor aria-hidden=true href=#residual-connections>#</a></h3><p>残差连接代码实现 :</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=ch>#!/usr/bin/env python</span>
</span></span><span class=line><span class=cl><span class=c1># -*- encoding: utf-8 -*-</span>
</span></span><span class=line><span class=cl><span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>@File    :   ResidualConnection.py
</span></span></span><span class=line><span class=cl><span class=s1>@Time    :   2024/09/27 15:33:49
</span></span></span><span class=line><span class=cl><span class=s1>@Author  :   pan binghong 
</span></span></span><span class=line><span class=cl><span class=s1>@Email   :   19909442097@163.com
</span></span></span><span class=line><span class=cl><span class=s1>@description   :   
</span></span></span><span class=line><span class=cl><span class=s1>&#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>LayerNormalization</span> <span class=kn>import</span> <span class=n>LayerNormalization</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>ResidualConnection</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>size</span><span class=p>,</span> <span class=n>dropout</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>ResidualConnection</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>  <span class=c1># 正确调用父类构造函数</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>norm</span> <span class=o>=</span> <span class=n>LayerNormalization</span><span class=p>(</span><span class=n>size</span><span class=p>)</span>  <span class=c1># 在父类构造函数之后设置属性</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>sublayer</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>sublayer</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>x</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s1>&#39;__main__&#39;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>size</span> <span class=o>=</span> <span class=mi>512</span>
</span></span><span class=line><span class=cl>    <span class=n>dropout</span> <span class=o>=</span> <span class=mf>0.1</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>residual_module</span> <span class=o>=</span> <span class=n>ResidualConnection</span><span class=p>(</span><span class=n>size</span><span class=p>,</span> <span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>rand</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=n>size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>sublayer</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>size</span><span class=p>,</span> <span class=n>size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>output</span> <span class=o>=</span> <span class=n>residual_module</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>sublayer</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;output shape: </span><span class=se>\n</span><span class=si>{</span><span class=n>output</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;out: </span><span class=se>\n</span><span class=si>{</span><span class=n>output</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>输出内容 :</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>output</span> <span class=n>shape</span><span class=p>:</span>
</span></span><span class=line><span class=cl><span class=n>torch</span><span class=o>.</span><span class=n>Size</span><span class=p>([</span><span class=mi>32</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=mi>512</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>out</span><span class=p>:</span>
</span></span><span class=line><span class=cl><span class=n>tensor</span><span class=p>([[[</span> <span class=mf>0.3520</span><span class=p>,</span>  <span class=mf>0.6030</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2354</span><span class=p>,</span>  <span class=o>...</span><span class=p>,</span>  <span class=mf>0.8682</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.3730</span><span class=p>,</span>  <span class=mf>0.1524</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         <span class=p>[</span> <span class=mf>1.4186</span><span class=p>,</span>  <span class=mf>0.5724</span><span class=p>,</span>  <span class=mf>0.2079</span><span class=p>,</span>  <span class=o>...</span><span class=p>,</span>  <span class=mf>0.4792</span><span class=p>,</span>  <span class=mf>1.2186</span><span class=p>,</span>  <span class=mf>0.6546</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         <span class=p>[</span> <span class=mf>1.0631</span><span class=p>,</span>  <span class=mf>1.0479</span><span class=p>,</span>  <span class=mf>0.4523</span><span class=p>,</span>  <span class=o>...</span><span class=p>,</span>  <span class=mf>1.4115</span><span class=p>,</span>  <span class=mf>1.5870</span><span class=p>,</span>  <span class=mf>0.7165</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         <span class=o>...</span><span class=p>,</span>
</span></span><span class=line><span class=cl>         <span class=p>[</span> <span class=mf>0.0567</span><span class=p>,</span>  <span class=mf>2.4177</span><span class=p>,</span>  <span class=mf>1.6159</span><span class=p>,</span>  <span class=o>...</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.5791</span><span class=p>,</span>  <span class=mf>0.4186</span><span class=p>,</span>  <span class=mf>0.6306</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         <span class=p>[</span> <span class=mf>1.9228</span><span class=p>,</span>  <span class=mf>0.8467</span><span class=p>,</span>  <span class=mf>0.7399</span><span class=p>,</span>  <span class=o>...</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.5388</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1463</span><span class=p>,</span>  <span class=mf>1.1880</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         <span class=p>[</span> <span class=mf>0.1197</span><span class=p>,</span>  <span class=mf>0.0182</span><span class=p>,</span>  <span class=mf>0.2941</span><span class=p>,</span>  <span class=o>...</span><span class=p>,</span>  <span class=mf>0.5807</span><span class=p>,</span>  <span class=mf>0.3925</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.4700</span><span class=p>]],</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=p>[[</span> <span class=mf>0.3333</span><span class=p>,</span>  <span class=mf>0.8267</span><span class=p>,</span>  <span class=mf>0.3082</span><span class=p>,</span>  <span class=o>...</span><span class=p>,</span>  <span class=mf>1.3075</span><span class=p>,</span>  <span class=mf>0.2646</span><span class=p>,</span>  <span class=mf>0.4092</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         <span class=p>[</span> <span class=mf>0.7507</span><span class=p>,</span>  <span class=mf>0.9554</span><span class=p>,</span>  <span class=mf>0.2910</span><span class=p>,</span>  <span class=o>...</span><span class=p>,</span>  <span class=mf>1.1357</span><span class=p>,</span>  <span class=mf>0.4684</span><span class=p>,</span>  <span class=mf>0.4244</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         <span class=p>[</span> <span class=mf>0.6514</span><span class=p>,</span>  <span class=mf>0.2003</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.3597</span><span class=p>,</span>  <span class=o>...</span><span class=p>,</span>  <span class=mf>0.3598</span><span class=p>,</span>  <span class=mf>0.4869</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1992</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         <span class=o>...</span><span class=p>,</span>
</span></span><span class=line><span class=cl>         <span class=p>[</span><span class=o>-</span><span class=mf>0.0030</span><span class=p>,</span>  <span class=mf>1.0668</span><span class=p>,</span>  <span class=mf>0.3075</span><span class=p>,</span>  <span class=o>...</span><span class=p>,</span>  <span class=mf>0.5691</span><span class=p>,</span>  <span class=mf>0.2380</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1247</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         <span class=p>[</span> <span class=mf>0.9135</span><span class=p>,</span>  <span class=mf>0.4341</span><span class=p>,</span>  <span class=mf>0.0337</span><span class=p>,</span>  <span class=o>...</span><span class=p>,</span>  <span class=mf>0.3597</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.7784</span><span class=p>,</span>  <span class=mf>0.8458</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         <span class=p>[</span><span class=o>-</span><span class=mf>0.1205</span><span class=p>,</span>  <span class=mf>0.6370</span><span class=p>,</span>  <span class=mf>1.0110</span><span class=p>,</span>  <span class=o>...</span><span class=p>,</span>  <span class=mf>0.0136</span><span class=p>,</span>  <span class=mf>0.6965</span><span class=p>,</span>  <span class=mf>0.2374</span><span class=p>]],</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=p>[[</span> <span class=mf>1.7201</span><span class=p>,</span>  <span class=mf>0.9184</span><span class=p>,</span>  <span class=mf>1.4459</span><span class=p>,</span>  <span class=o>...</span><span class=p>,</span>  <span class=mf>0.6506</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.4328</span><span class=p>,</span>  <span class=mf>0.4222</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         <span class=p>[</span> <span class=mf>0.1091</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.7816</span><span class=p>,</span>  <span class=mf>0.0389</span><span class=p>,</span>  <span class=o>...</span><span class=p>,</span>  <span class=mf>0.4128</span><span class=p>,</span>  <span class=mf>0.5077</span><span class=p>,</span>  <span class=mf>1.2345</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         <span class=p>[</span> <span class=mf>0.3344</span><span class=p>,</span>  <span class=mf>0.5098</span><span class=p>,</span>  <span class=mf>0.1903</span><span class=p>,</span>  <span class=o>...</span><span class=p>,</span>  <span class=mf>0.4348</span><span class=p>,</span>  <span class=mf>0.1655</span><span class=p>,</span>  <span class=mf>1.0356</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         <span class=o>...</span><span class=p>,</span>
</span></span><span class=line><span class=cl>         <span class=p>[</span> <span class=mf>0.6577</span><span class=p>,</span>  <span class=mf>1.0894</span><span class=p>,</span>  <span class=mf>0.3509</span><span class=p>,</span>  <span class=o>...</span><span class=p>,</span>  <span class=mf>0.9495</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.3284</span><span class=p>,</span>  <span class=mf>0.8220</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         <span class=p>[</span><span class=o>-</span><span class=mf>0.3985</span><span class=p>,</span>  <span class=mf>1.0728</span><span class=p>,</span>  <span class=mf>1.2246</span><span class=p>,</span>  <span class=o>...</span><span class=p>,</span>  <span class=mf>0.6893</span><span class=p>,</span>  <span class=mf>0.3443</span><span class=p>,</span>  <span class=mf>0.4279</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         <span class=p>[</span> <span class=mf>0.2423</span><span class=p>,</span>  <span class=mf>0.0113</span><span class=p>,</span>  <span class=mf>1.3485</span><span class=p>,</span>  <span class=o>...</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2677</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0577</span><span class=p>,</span>  <span class=mf>0.4010</span><span class=p>]],</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=o>...</span><span class=p>,</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=p>[[</span> <span class=mf>1.4007</span><span class=p>,</span>  <span class=mf>1.3420</span><span class=p>,</span>  <span class=mf>0.1977</span><span class=p>,</span>  <span class=o>...</span><span class=p>,</span>  <span class=mf>0.8533</span><span class=p>,</span>  <span class=mf>0.1814</span><span class=p>,</span>  <span class=mf>0.2697</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         <span class=p>[</span> <span class=mf>0.6700</span><span class=p>,</span>  <span class=mf>0.2914</span><span class=p>,</span>  <span class=mf>0.7087</span><span class=p>,</span>  <span class=o>...</span><span class=p>,</span>  <span class=mf>0.4371</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.6651</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.3476</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         <span class=p>[</span> <span class=mf>1.7371</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.3646</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.7164</span><span class=p>,</span>  <span class=o>...</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2941</span><span class=p>,</span>  <span class=mf>1.5312</span><span class=p>,</span>  <span class=mf>0.1496</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         <span class=o>...</span><span class=p>,</span>
</span></span><span class=line><span class=cl>         <span class=p>[</span> <span class=mf>1.4492</span><span class=p>,</span>  <span class=mf>1.6843</span><span class=p>,</span>  <span class=mf>0.4061</span><span class=p>,</span>  <span class=o>...</span><span class=p>,</span>  <span class=mf>0.2602</span><span class=p>,</span>  <span class=mf>0.3412</span><span class=p>,</span>  <span class=mf>0.9145</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         <span class=p>[</span> <span class=mf>0.5208</span><span class=p>,</span>  <span class=mf>0.7262</span><span class=p>,</span>  <span class=mf>1.1507</span><span class=p>,</span>  <span class=o>...</span><span class=p>,</span>  <span class=mf>1.6124</span><span class=p>,</span>  <span class=mf>0.1670</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.7637</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         <span class=p>[</span> <span class=mf>0.6195</span><span class=p>,</span>  <span class=mf>1.8701</span><span class=p>,</span>  <span class=mf>0.6011</span><span class=p>,</span>  <span class=o>...</span><span class=p>,</span>  <span class=mf>0.7136</span><span class=p>,</span>  <span class=mf>0.1405</span><span class=p>,</span>  <span class=mf>0.7195</span><span class=p>]],</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=p>[[</span> <span class=mf>1.2362</span><span class=p>,</span>  <span class=mf>0.3252</span><span class=p>,</span>  <span class=mf>1.5944</span><span class=p>,</span>  <span class=o>...</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.6239</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.5983</span><span class=p>,</span>  <span class=mf>0.0794</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         <span class=p>[</span> <span class=mf>1.1429</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.8601</span><span class=p>,</span>  <span class=mf>0.6993</span><span class=p>,</span>  <span class=o>...</span><span class=p>,</span>  <span class=mf>0.1767</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.7440</span><span class=p>,</span>  <span class=mf>0.6210</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         <span class=p>[</span> <span class=mf>0.1337</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.8456</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.4567</span><span class=p>,</span>  <span class=o>...</span><span class=p>,</span>  <span class=mf>1.0074</span><span class=p>,</span>  <span class=mf>0.6997</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.7483</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         <span class=o>...</span><span class=p>,</span>
</span></span><span class=line><span class=cl>         <span class=p>[</span><span class=o>-</span><span class=mf>0.2421</span><span class=p>,</span>  <span class=mf>0.8441</span><span class=p>,</span>  <span class=mf>1.1355</span><span class=p>,</span>  <span class=o>...</span><span class=p>,</span>  <span class=mf>1.2241</span><span class=p>,</span>  <span class=mf>0.0689</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.8084</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         <span class=p>[</span> <span class=mf>0.1203</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1496</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.3044</span><span class=p>,</span>  <span class=o>...</span><span class=p>,</span>  <span class=mf>0.2365</span><span class=p>,</span>  <span class=mf>1.0541</span><span class=p>,</span>  <span class=mf>0.5421</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         <span class=p>[</span> <span class=mf>0.7855</span><span class=p>,</span>  <span class=mf>0.0565</span><span class=p>,</span>  <span class=mf>0.9192</span><span class=p>,</span>  <span class=o>...</span><span class=p>,</span>  <span class=mf>0.8071</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.9707</span><span class=p>,</span>  <span class=mf>1.3335</span><span class=p>]],</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=p>[[</span> <span class=mf>0.6022</span><span class=p>,</span>  <span class=mf>1.4715</span><span class=p>,</span>  <span class=mf>0.2470</span><span class=p>,</span>  <span class=o>...</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0782</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.6734</span><span class=p>,</span>  <span class=mf>0.8383</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         <span class=p>[</span> <span class=mf>0.8088</span><span class=p>,</span>  <span class=mf>0.3382</span><span class=p>,</span>  <span class=mf>0.6812</span><span class=p>,</span>  <span class=o>...</span><span class=p>,</span>  <span class=mf>1.1501</span><span class=p>,</span>  <span class=mf>1.0537</span><span class=p>,</span>  <span class=mf>0.5442</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         <span class=p>[</span><span class=o>-</span><span class=mf>0.0593</span><span class=p>,</span>  <span class=mf>1.7771</span><span class=p>,</span>  <span class=mf>0.0580</span><span class=p>,</span>  <span class=o>...</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0578</span><span class=p>,</span>  <span class=mf>0.7382</span><span class=p>,</span>  <span class=mf>1.2158</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         <span class=o>...</span><span class=p>,</span>
</span></span><span class=line><span class=cl>         <span class=p>[</span> <span class=mf>1.1114</span><span class=p>,</span>  <span class=mf>0.3564</span><span class=p>,</span>  <span class=mf>0.8435</span><span class=p>,</span>  <span class=o>...</span><span class=p>,</span>  <span class=mf>0.1796</span><span class=p>,</span>  <span class=mf>1.2682</span><span class=p>,</span>  <span class=mf>0.5146</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         <span class=p>[</span> <span class=mf>1.0304</span><span class=p>,</span>  <span class=mf>1.2170</span><span class=p>,</span>  <span class=mf>0.8374</span><span class=p>,</span>  <span class=o>...</span><span class=p>,</span>  <span class=mf>2.2357</span><span class=p>,</span>  <span class=mf>0.2286</span><span class=p>,</span>  <span class=mf>0.3899</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         <span class=p>[</span> <span class=mf>0.5022</span><span class=p>,</span>  <span class=mf>0.3711</span><span class=p>,</span>  <span class=mf>0.2397</span><span class=p>,</span>  <span class=o>...</span><span class=p>,</span>  <span class=mf>0.9505</span><span class=p>,</span>  <span class=mf>0.3877</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2048</span><span class=p>]]],</span>
</span></span><span class=line><span class=cl>       <span class=n>grad_fn</span><span class=o>=&lt;</span><span class=n>AddBackward0</span><span class=o>&gt;</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=编码器>编码器<a hidden class=anchor aria-hidden=true href=#编码器>#</a></h2><h3 id=encoder-layer>Encoder Layer<a hidden class=anchor aria-hidden=true href=#encoder-layer>#</a></h3><p>编码器层由之前构建的多头注意力机制, 前馈神经网络, 残差模块, 层归一化组合构成</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=ch>#!/usr/bin/env python</span>
</span></span><span class=line><span class=cl><span class=c1># -*- encoding: utf-8 -*-</span>
</span></span><span class=line><span class=cl><span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>@File    :   EncoderLayer.py
</span></span></span><span class=line><span class=cl><span class=s1>@Time    :   2024/09/27 16:51:01
</span></span></span><span class=line><span class=cl><span class=s1>@Author  :   pan binghong 
</span></span></span><span class=line><span class=cl><span class=s1>@Email   :   19909442097@163.com
</span></span></span><span class=line><span class=cl><span class=s1>@description   :   多头注意力机制、前馈神经网络、位置编码、残差连接和层归一化结合起来，</span><span class=se>\n</span><span class=s1>
</span></span></span><span class=line><span class=cl><span class=s1>                   构建一个 Encoder Layer。Encoder Layer 是 Transformer 编码器的基本组成单位。
</span></span></span><span class=line><span class=cl><span class=s1>&#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>MultiHeadAttention</span> <span class=kn>import</span> <span class=n>MultiHeadAttention</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>FeedForwardNetwork</span> <span class=kn>import</span> <span class=n>FeedForwardNetwork</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>LayerNormalization</span> <span class=kn>import</span> <span class=n>LayerNormalization</span> <span class=c1>#在残差连接模块中完成</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>ResidualConnection</span> <span class=kn>import</span> <span class=n>ResidualConnection</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>EncoderLayer</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>,</span> <span class=n>dropout</span><span class=o>=</span><span class=mf>0.1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>EncoderLayer</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>d_model</span> <span class=o>=</span> <span class=n>d_model</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>self_attn</span> <span class=o>=</span> <span class=n>MultiHeadAttention</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>pos_ffn</span> <span class=o>=</span> <span class=n>FeedForwardNetwork</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>,</span> <span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>residual</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>([</span>
</span></span><span class=line><span class=cl>            <span class=n>ResidualConnection</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>dropout</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>ResidualConnection</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=p>])</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>residual</span><span class=p>[</span><span class=mi>0</span><span class=p>](</span><span class=n>x</span><span class=p>,</span> <span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>self_attn</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>mask</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>residual</span><span class=p>[</span><span class=mi>1</span><span class=p>](</span><span class=n>x</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>pos_ffn</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl><span class=c1># 示例</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s2>&#34;__main__&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=c1># 示例输入</span>
</span></span><span class=line><span class=cl>    <span class=n>d_model</span> <span class=o>=</span> <span class=mi>512</span>
</span></span><span class=line><span class=cl>    <span class=n>num_heads</span> <span class=o>=</span> <span class=mi>8</span>
</span></span><span class=line><span class=cl>    <span class=n>hidden_size</span> <span class=o>=</span> <span class=mi>2048</span>
</span></span><span class=line><span class=cl>    <span class=n>dropout</span> <span class=o>=</span> <span class=mf>0.1</span>
</span></span><span class=line><span class=cl>    <span class=n>batch_size</span> <span class=o>=</span> <span class=mi>32</span>
</span></span><span class=line><span class=cl>    <span class=n>seq_len</span> <span class=o>=</span> <span class=mi>10</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>encoder_layer</span> <span class=o>=</span> <span class=n>EncoderLayer</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>,</span> <span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>input_tensor</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>mask</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span><span class=n>seq_len</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 前向传播</span>
</span></span><span class=line><span class=cl>    <span class=n>output_tensor</span> <span class=o>=</span> <span class=n>encoder_layer</span><span class=p>(</span><span class=n>input_tensor</span><span class=p>,</span> <span class=n>mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 输出结果</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Input shape:&#34;</span><span class=p>,</span> <span class=n>input_tensor</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Output shape:&#34;</span><span class=p>,</span> <span class=n>output_tensor</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>输出结果为 :</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>Input</span> <span class=n>shape</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Size</span><span class=p>([</span><span class=mi>32</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=mi>512</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>Output</span> <span class=n>shape</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Size</span><span class=p>([</span><span class=mi>32</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=mi>512</span><span class=p>])</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=encoder>Encoder<a hidden class=anchor aria-hidden=true href=#encoder>#</a></h3><p>Encoder由n个Encoder_Lyaer组成, 详细代码如下 :</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=ch>#!/usr/bin/env python</span>
</span></span><span class=line><span class=cl><span class=c1># -*- encoding: utf-8 -*-</span>
</span></span><span class=line><span class=cl><span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>@File    :   Encoder.py
</span></span></span><span class=line><span class=cl><span class=s1>@Time    :   2024/09/29 08:39:15
</span></span></span><span class=line><span class=cl><span class=s1>@Author  :   pan binghong 
</span></span></span><span class=line><span class=cl><span class=s1>@Email   :   19909442097@163.com
</span></span></span><span class=line><span class=cl><span class=s1>@description   :   
</span></span></span><span class=line><span class=cl><span class=s1>&#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>os</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>sys</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>EncoderLayer</span> <span class=kn>import</span> <span class=n>EncoderLayer</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>LayerNormalization</span> <span class=kn>import</span> <span class=n>LayerNormalization</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>Encoder</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>encoder_layer</span><span class=p>,</span> <span class=n>num_layers</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>Encoder</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>encoder_layer</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>([</span>
</span></span><span class=line><span class=cl>            <span class=n>encoder_layer</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_layers</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>norm</span> <span class=o>=</span> <span class=n>LayerNormalization</span><span class=p>(</span><span class=n>encoder_layer</span><span class=o>.</span><span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>src</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>layer</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>encoder_layer</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>src</span> <span class=o>=</span> <span class=n>layer</span><span class=p>(</span><span class=n>src</span><span class=p>,</span> <span class=n>mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>src</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s1>&#39;__main__&#39;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>d_model</span> <span class=o>=</span> <span class=mi>512</span>
</span></span><span class=line><span class=cl>    <span class=n>num_heades</span> <span class=o>=</span> <span class=mi>8</span>
</span></span><span class=line><span class=cl>    <span class=n>hidden_size</span> <span class=o>=</span> <span class=mi>2048</span>
</span></span><span class=line><span class=cl>    <span class=n>droupout</span> <span class=o>=</span> <span class=mf>0.1</span>
</span></span><span class=line><span class=cl>    <span class=n>num_layers</span> <span class=o>=</span> <span class=mi>6</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>encoder_layer</span> <span class=o>=</span> <span class=n>EncoderLayer</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>num_heades</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>,</span> <span class=n>droupout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>encoder</span> <span class=o>=</span> <span class=n>Encoder</span><span class=p>(</span><span class=n>encoder_layer</span><span class=p>,</span> <span class=n>num_layers</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>src</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>rand</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=mi>10</span> <span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>output</span> <span class=o>=</span> <span class=n>encoder</span><span class=p>(</span><span class=n>src</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>output</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>输出结果为 :</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>Input shape: torch.Size<span class=o>([</span>32, 10, 512<span class=o>])</span>
</span></span><span class=line><span class=cl>Output shape: torch.Size<span class=o>([</span>32, 10, 512<span class=o>])</span>
</span></span></code></pre></td></tr></table></div></div><ul><li>32个样本</li><li>每个样本有10个时间步</li><li>每个时间步的特征向量有512个维度</li></ul><hr><h2 id=解码器>解码器<a hidden class=anchor aria-hidden=true href=#解码器>#</a></h2><h3 id=decoder_layer>Decoder_Layer<a hidden class=anchor aria-hidden=true href=#decoder_layer>#</a></h3><p>编码器与解码器最大的区别就是使用了Mask Multi-Head Attention, 用于防止模型训练过程中”看到”后续的目标词, 由多个解码器层构成, 代码如下 :</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=ch>#!/usr/bin/env python</span>
</span></span><span class=line><span class=cl><span class=c1># -*- encoding: utf-8 -*-</span>
</span></span><span class=line><span class=cl><span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>@File    :   DecoderLayer.py
</span></span></span><span class=line><span class=cl><span class=s1>@Time    :   2024/09/29 09:34:04
</span></span></span><span class=line><span class=cl><span class=s1>@Author  :   pan binghong 
</span></span></span><span class=line><span class=cl><span class=s1>@Email   :   19909442097@163.com
</span></span></span><span class=line><span class=cl><span class=s1>@description   :   Transformer 解码器与编码器类似，</span><span class=se>\n</span><span class=s1>
</span></span></span><span class=line><span class=cl><span class=s1>主要区别在于解码器使用了 Masked Multi-Head Attention，</span><span class=se>\n</span><span class=s1>
</span></span></span><span class=line><span class=cl><span class=s1>用于防止模型在训练过程中“看到”后续的目标词。</span><span class=se>\n</span><span class=s1>
</span></span></span><span class=line><span class=cl><span class=s1>解码器也是由多个 Decoder Layer 堆叠组成。
</span></span></span><span class=line><span class=cl><span class=s1>&#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>MultiHeadAttention</span> <span class=kn>import</span> <span class=n>MultiHeadAttention</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>FeedForwardNetwork</span> <span class=kn>import</span> <span class=n>FeedForwardNetwork</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>ResidualConnection</span> <span class=kn>import</span> <span class=n>ResidualConnection</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>DecoderLayer</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>heads</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>,</span> <span class=n>dropout</span><span class=o>=</span><span class=mf>0.1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>DecoderLayer</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>self_attn</span> <span class=o>=</span> <span class=n>MultiHeadAttention</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>heads</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>src_attn</span> <span class=o>=</span> <span class=n>MultiHeadAttention</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>heads</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>feed_forward</span> <span class=o>=</span> <span class=n>FeedForwardNetwork</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>,</span> <span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>residuals</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>([</span>
</span></span><span class=line><span class=cl>            <span class=n>ResidualConnection</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>dropout</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>ResidualConnection</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>dropout</span><span class=p>),</span>            
</span></span><span class=line><span class=cl>            <span class=n>ResidualConnection</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>memory</span><span class=p>,</span> <span class=n>src_mask</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>trg_mask</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># 自注意力机制</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>residuals</span><span class=p>[</span><span class=mi>0</span><span class=p>](</span><span class=n>x</span><span class=p>,</span> <span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>self_attn</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>trg_mask</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=c1># 编码器-解码器注意力机制</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>residuals</span><span class=p>[</span><span class=mi>1</span><span class=p>](</span><span class=n>x</span><span class=p>,</span> <span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>src_attn</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>memory</span><span class=p>,</span> <span class=n>memory</span><span class=p>,</span> <span class=n>src_mask</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=c1># 前馈网络</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>residuals</span><span class=p>[</span><span class=mi>2</span><span class=p>](</span><span class=n>x</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>feed_forward</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s1>&#39;__main__&#39;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>embedding_size</span> <span class=o>=</span> <span class=mi>512</span>
</span></span><span class=line><span class=cl>    <span class=n>heads</span> <span class=o>=</span> <span class=mi>8</span>
</span></span><span class=line><span class=cl>    <span class=n>hidden_size</span> <span class=o>=</span> <span class=mi>2048</span>
</span></span><span class=line><span class=cl>    <span class=n>batch_size</span> <span class=o>=</span> <span class=mi>8</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>decoder_layer</span> <span class=o>=</span> <span class=n>DecoderLayer</span><span class=p>(</span><span class=n>embedding_size</span><span class=p>,</span> <span class=n>heads</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>decoder_layer</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>decoder_layer</span><span class=o>.</span><span class=n>parameters</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>rand</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=mi>16</span><span class=p>,</span> <span class=n>embedding_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>memory</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>rand</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=mi>16</span><span class=p>,</span> <span class=n>embedding_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>src_mask</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>rand</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>16</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>trg_mask</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>rand</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=mi>16</span><span class=p>,</span> <span class=mi>16</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>output</span> <span class=o>=</span> <span class=n>decoder_layer</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>memory</span><span class=p>,</span> <span class=n>src_mask</span><span class=p>,</span> <span class=n>trg_mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>output</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>输出结果如下 :</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>DecoderLayer</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=p>(</span><span class=n>self_attn</span><span class=p>):</span> <span class=n>MultiHeadAttention</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=n>value</span><span class=p>):</span> <span class=n>Linear</span><span class=p>(</span><span class=n>in_features</span><span class=o>=</span><span class=mi>64</span><span class=p>,</span> <span class=n>out_features</span><span class=o>=</span><span class=mi>64</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=n>key</span><span class=p>):</span> <span class=n>Linear</span><span class=p>(</span><span class=n>in_features</span><span class=o>=</span><span class=mi>64</span><span class=p>,</span> <span class=n>out_features</span><span class=o>=</span><span class=mi>64</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=n>query</span><span class=p>):</span> <span class=n>Linear</span><span class=p>(</span><span class=n>in_features</span><span class=o>=</span><span class=mi>64</span><span class=p>,</span> <span class=n>out_features</span><span class=o>=</span><span class=mi>64</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=n>fc_out</span><span class=p>):</span> <span class=n>Linear</span><span class=p>(</span><span class=n>in_features</span><span class=o>=</span><span class=mi>512</span><span class=p>,</span> <span class=n>out_features</span><span class=o>=</span><span class=mi>512</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=p>(</span><span class=n>src_attn</span><span class=p>):</span> <span class=n>MultiHeadAttention</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=n>value</span><span class=p>):</span> <span class=n>Linear</span><span class=p>(</span><span class=n>in_features</span><span class=o>=</span><span class=mi>64</span><span class=p>,</span> <span class=n>out_features</span><span class=o>=</span><span class=mi>64</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=n>key</span><span class=p>):</span> <span class=n>Linear</span><span class=p>(</span><span class=n>in_features</span><span class=o>=</span><span class=mi>64</span><span class=p>,</span> <span class=n>out_features</span><span class=o>=</span><span class=mi>64</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=n>query</span><span class=p>):</span> <span class=n>Linear</span><span class=p>(</span><span class=n>in_features</span><span class=o>=</span><span class=mi>64</span><span class=p>,</span> <span class=n>out_features</span><span class=o>=</span><span class=mi>64</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=n>fc_out</span><span class=p>):</span> <span class=n>Linear</span><span class=p>(</span><span class=n>in_features</span><span class=o>=</span><span class=mi>512</span><span class=p>,</span> <span class=n>out_features</span><span class=o>=</span><span class=mi>512</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=p>(</span><span class=n>feed_forward</span><span class=p>):</span> <span class=n>FeedForwardNetwork</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=n>liner1</span><span class=p>):</span> <span class=n>Linear</span><span class=p>(</span><span class=n>in_features</span><span class=o>=</span><span class=mi>512</span><span class=p>,</span> <span class=n>out_features</span><span class=o>=</span><span class=mi>2048</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=n>relu</span><span class=p>):</span> <span class=n>ReLU</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=n>liner2</span><span class=p>):</span> <span class=n>Linear</span><span class=p>(</span><span class=n>in_features</span><span class=o>=</span><span class=mi>2048</span><span class=p>,</span> <span class=n>out_features</span><span class=o>=</span><span class=mi>512</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=n>dropout</span><span class=p>):</span> <span class=n>Dropout</span><span class=p>(</span><span class=n>p</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span> <span class=n>inplace</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=p>(</span><span class=n>residuals</span><span class=p>):</span> <span class=n>ModuleList</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=mi>0</span><span class=o>-</span><span class=mi>2</span><span class=p>):</span> <span class=mi>3</span> <span class=n>x</span> <span class=n>ResidualConnection</span><span class=p>(</span>
</span></span><span class=line><span class=cl>      <span class=p>(</span><span class=n>norm</span><span class=p>):</span> <span class=n>LayerNormalization</span><span class=p>()</span>
</span></span><span class=line><span class=cl>      <span class=p>(</span><span class=n>dropout</span><span class=p>):</span> <span class=n>Dropout</span><span class=p>(</span><span class=n>p</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span> <span class=n>inplace</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>&lt;</span><span class=n>bound</span> <span class=n>method</span> <span class=n>Module</span><span class=o>.</span><span class=n>parameters</span> <span class=n>of</span> <span class=n>DecoderLayer</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=p>(</span><span class=n>self_attn</span><span class=p>):</span> <span class=n>MultiHeadAttention</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=n>value</span><span class=p>):</span> <span class=n>Linear</span><span class=p>(</span><span class=n>in_features</span><span class=o>=</span><span class=mi>64</span><span class=p>,</span> <span class=n>out_features</span><span class=o>=</span><span class=mi>64</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=n>key</span><span class=p>):</span> <span class=n>Linear</span><span class=p>(</span><span class=n>in_features</span><span class=o>=</span><span class=mi>64</span><span class=p>,</span> <span class=n>out_features</span><span class=o>=</span><span class=mi>64</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=n>query</span><span class=p>):</span> <span class=n>Linear</span><span class=p>(</span><span class=n>in_features</span><span class=o>=</span><span class=mi>64</span><span class=p>,</span> <span class=n>out_features</span><span class=o>=</span><span class=mi>64</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=n>fc_out</span><span class=p>):</span> <span class=n>Linear</span><span class=p>(</span><span class=n>in_features</span><span class=o>=</span><span class=mi>512</span><span class=p>,</span> <span class=n>out_features</span><span class=o>=</span><span class=mi>512</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=p>(</span><span class=n>src_attn</span><span class=p>):</span> <span class=n>MultiHeadAttention</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=n>value</span><span class=p>):</span> <span class=n>Linear</span><span class=p>(</span><span class=n>in_features</span><span class=o>=</span><span class=mi>64</span><span class=p>,</span> <span class=n>out_features</span><span class=o>=</span><span class=mi>64</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=n>key</span><span class=p>):</span> <span class=n>Linear</span><span class=p>(</span><span class=n>in_features</span><span class=o>=</span><span class=mi>64</span><span class=p>,</span> <span class=n>out_features</span><span class=o>=</span><span class=mi>64</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=n>query</span><span class=p>):</span> <span class=n>Linear</span><span class=p>(</span><span class=n>in_features</span><span class=o>=</span><span class=mi>64</span><span class=p>,</span> <span class=n>out_features</span><span class=o>=</span><span class=mi>64</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=n>fc_out</span><span class=p>):</span> <span class=n>Linear</span><span class=p>(</span><span class=n>in_features</span><span class=o>=</span><span class=mi>512</span><span class=p>,</span> <span class=n>out_features</span><span class=o>=</span><span class=mi>512</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=p>(</span><span class=n>feed_forward</span><span class=p>):</span> <span class=n>FeedForwardNetwork</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=n>liner1</span><span class=p>):</span> <span class=n>Linear</span><span class=p>(</span><span class=n>in_features</span><span class=o>=</span><span class=mi>512</span><span class=p>,</span> <span class=n>out_features</span><span class=o>=</span><span class=mi>2048</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=n>relu</span><span class=p>):</span> <span class=n>ReLU</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=n>liner2</span><span class=p>):</span> <span class=n>Linear</span><span class=p>(</span><span class=n>in_features</span><span class=o>=</span><span class=mi>2048</span><span class=p>,</span> <span class=n>out_features</span><span class=o>=</span><span class=mi>512</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=n>dropout</span><span class=p>):</span> <span class=n>Dropout</span><span class=p>(</span><span class=n>p</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span> <span class=n>inplace</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=p>(</span><span class=n>residuals</span><span class=p>):</span> <span class=n>ModuleList</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=mi>0</span><span class=o>-</span><span class=mi>2</span><span class=p>):</span> <span class=mi>3</span> <span class=n>x</span> <span class=n>ResidualConnection</span><span class=p>(</span>
</span></span><span class=line><span class=cl>      <span class=p>(</span><span class=n>norm</span><span class=p>):</span> <span class=n>LayerNormalization</span><span class=p>()</span>
</span></span><span class=line><span class=cl>      <span class=p>(</span><span class=n>dropout</span><span class=p>):</span> <span class=n>Dropout</span><span class=p>(</span><span class=n>p</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span> <span class=n>inplace</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>)</span><span class=o>&gt;</span>
</span></span><span class=line><span class=cl><span class=n>torch</span><span class=o>.</span><span class=n>Size</span><span class=p>([</span><span class=mi>8</span><span class=p>,</span> <span class=mi>16</span><span class=p>,</span> <span class=mi>512</span><span class=p>])</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=decoder>Decoder<a hidden class=anchor aria-hidden=true href=#decoder>#</a></h3><p>代码报错解决中…</p><hr><blockquote><p>References</p></blockquote></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://Pan-Binghong.github.io/daily-learning/ai/%E5%9F%BA%E4%BA%8Emodelzoonpu%E5%BE%AE%E8%B0%83chatglm3%E5%85%A8%E6%B5%81%E7%A8%8B/><span class=title>« Prev</span><br><span>基于ModelZOO(NPU)微调ChatGLM3全流程</span>
</a><a class=next href=https://Pan-Binghong.github.io/daily-learning/ai/%E5%9F%BA%E4%BA%8Eswift%E5%BE%AE%E8%B0%83minicpm-v2.6/><span class=title>Next »</span><br><span>基于Swift微调MiniCPM-v2.6</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share 基于Pytorch架构手撕Transformer on x" href="https://x.com/intent/tweet/?text=%e5%9f%ba%e4%ba%8ePytorch%e6%9e%b6%e6%9e%84%e6%89%8b%e6%92%95Transformer&amp;url=https%3a%2f%2fPan-Binghong.github.io%2fdaily-learning%2fai%2f%25E5%259F%25BA%25E4%25BA%258Epytorch%25E6%259E%25B6%25E6%259E%2584%25E6%2589%258B%25E6%2592%2595transformer%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 基于Pytorch架构手撕Transformer on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fPan-Binghong.github.io%2fdaily-learning%2fai%2f%25E5%259F%25BA%25E4%25BA%258Epytorch%25E6%259E%25B6%25E6%259E%2584%25E6%2589%258B%25E6%2592%2595transformer%2f&amp;title=%e5%9f%ba%e4%ba%8ePytorch%e6%9e%b6%e6%9e%84%e6%89%8b%e6%92%95Transformer&amp;summary=%e5%9f%ba%e4%ba%8ePytorch%e6%9e%b6%e6%9e%84%e6%89%8b%e6%92%95Transformer&amp;source=https%3a%2f%2fPan-Binghong.github.io%2fdaily-learning%2fai%2f%25E5%259F%25BA%25E4%25BA%258Epytorch%25E6%259E%25B6%25E6%259E%2584%25E6%2589%258B%25E6%2592%2595transformer%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 基于Pytorch架构手撕Transformer on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fPan-Binghong.github.io%2fdaily-learning%2fai%2f%25E5%259F%25BA%25E4%25BA%258Epytorch%25E6%259E%25B6%25E6%259E%2584%25E6%2589%258B%25E6%2592%2595transformer%2f&title=%e5%9f%ba%e4%ba%8ePytorch%e6%9e%b6%e6%9e%84%e6%89%8b%e6%92%95Transformer"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 基于Pytorch架构手撕Transformer on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fPan-Binghong.github.io%2fdaily-learning%2fai%2f%25E5%259F%25BA%25E4%25BA%258Epytorch%25E6%259E%25B6%25E6%259E%2584%25E6%2589%258B%25E6%2592%2595transformer%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 基于Pytorch架构手撕Transformer on whatsapp" href="https://api.whatsapp.com/send?text=%e5%9f%ba%e4%ba%8ePytorch%e6%9e%b6%e6%9e%84%e6%89%8b%e6%92%95Transformer%20-%20https%3a%2f%2fPan-Binghong.github.io%2fdaily-learning%2fai%2f%25E5%259F%25BA%25E4%25BA%258Epytorch%25E6%259E%25B6%25E6%259E%2584%25E6%2589%258B%25E6%2592%2595transformer%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 基于Pytorch架构手撕Transformer on telegram" href="https://telegram.me/share/url?text=%e5%9f%ba%e4%ba%8ePytorch%e6%9e%b6%e6%9e%84%e6%89%8b%e6%92%95Transformer&amp;url=https%3a%2f%2fPan-Binghong.github.io%2fdaily-learning%2fai%2f%25E5%259F%25BA%25E4%25BA%258Epytorch%25E6%259E%25B6%25E6%259E%2584%25E6%2589%258B%25E6%2592%2595transformer%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 基于Pytorch架构手撕Transformer on ycombinator" href="https://news.ycombinator.com/submitlink?t=%e5%9f%ba%e4%ba%8ePytorch%e6%9e%b6%e6%9e%84%e6%89%8b%e6%92%95Transformer&u=https%3a%2f%2fPan-Binghong.github.io%2fdaily-learning%2fai%2f%25E5%259F%25BA%25E4%25BA%258Epytorch%25E6%259E%25B6%25E6%259E%2584%25E6%2589%258B%25E6%2592%2595transformer%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer><div class=comments-section><hr class=comments-divider><div class=giscus-comments><h3 class=comments-title>💬 评论讨论</h3><p class=comments-hint>使用 GitHub 账号登录即可参与讨论。评论将同步到
<a href=https://github.com/Pan-Binghong/daily-learning/discussions target=_blank rel=noopener>GitHub Discussions</a></p><script src=https://giscus.app/client.js data-repo=Pan-Binghong/daily-learning data-repo-id=R_kgDONYfgVw data-category=Announcements data-category-id=DIC_kwDONYfgV84CkvE7 data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script></div></div><style>.comments-section{margin-top:3rem}.comments-divider{margin:2rem 0;border:none;border-top:2px solid var(--border)}.comments-title{font-size:1.5rem;margin-bottom:1rem;color:var(--primary)}.comments-hint{font-size:.9rem;color:var(--secondary);margin-bottom:1.5rem;padding:1rem;background:var(--code-bg);border-radius:8px;border-left:4px solid var(--primary)}.comments-hint a{color:var(--primary);text-decoration:underline}.giscus-comments{margin-top:2rem}</style></article></main><footer class=footer><span>&copy; 2025 <a href=https://Pan-Binghong.github.io/daily-learning/>Pan Binghong's Tech Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg></a><div class=reading-progress-bar id=reading-progress></div><script>window.addEventListener("scroll",function(){const e=document.getElementById("reading-progress");if(!e)return;const t=window.innerHeight,n=document.documentElement.scrollHeight-t,s=window.scrollY,o=s/n*100;e.style.width=o+"%"}),document.addEventListener("DOMContentLoaded",function(){const e=document.querySelectorAll(".copy-text");e.forEach(e=>{e.addEventListener("click",function(){this.textContent="✓ 已复制",setTimeout(()=>{this.textContent="复制"},2e3)})})}),document.addEventListener("DOMContentLoaded",function(){const e=document.querySelectorAll('a[href^="http"]');e.forEach(e=>{e.hostname!==window.location.hostname&&(e.setAttribute("target","_blank"),e.setAttribute("rel","noopener noreferrer"))})}),document.addEventListener("DOMContentLoaded",function(){const e=document.querySelectorAll(".post-content img");e.forEach(e=>{e.style.cursor="pointer",e.addEventListener("click",function(){window.open(this.src,"_blank")})})})</script><style>#top-link{transition:all .3s ease}#top-link:hover{transform:translateY(-5px);box-shadow:0 5px 15px rgba(0,0,0,.2)}</style><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>